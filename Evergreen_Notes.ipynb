{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanpereramerry/evergreen-notes-public/blob/main/Evergreen_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "ayMMiRfC2tZm",
        "outputId": "b0f64d9f-eafc-4810-efc2-277b5414cc1c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-67be1021f33a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import PyPDF2\n",
        "import openai\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set your OpenAI API key (in production, use environment variables)\n",
        "# openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "class EvergreenNotesGenerator:\n",
        "    def __init__(self, api_key=None, model=\"gpt-4o\", notes_per_chapter=5):\n",
        "        \"\"\"\n",
        "        Initialize the Evergreen Notes Generator\n",
        "\n",
        "        Args:\n",
        "            api_key (str): OpenAI API key\n",
        "            model (str): OpenAI model to use\n",
        "            notes_per_chapter (int): Number of notes to generate per chapter\n",
        "        \"\"\"\n",
        "        if api_key:\n",
        "            self.client = openai.OpenAI(api_key=api_key)\n",
        "        else:\n",
        "            self.client = openai.OpenAI()\n",
        "\n",
        "        self.model = model\n",
        "        self.notes_per_chapter = notes_per_chapter\n",
        "\n",
        "        # GPT-4o can handle up to 128k tokens\n",
        "        # Setting a conservative limit of ~90k tokens (~360,000 chars)\n",
        "        self.max_chunk_chars = 360000\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        \"\"\"Extract full text from PDF file\"\"\"\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def extract_chapters(self, full_text):\n",
        "        \"\"\"\n",
        "        Advanced chapter extraction from text content\n",
        "        Uses multiple methods to identify chapter boundaries\n",
        "        \"\"\"\n",
        "        # Method 1: Extract based on Table of Contents if available\n",
        "        toc_pattern = r'(?:TABLE\\s+OF\\s+CONTENTS|CONTENTS).*?\\n(.*?)(?:\\n\\s*\\n|\\n(?:CHAPTER|INTRODUCTION))'\n",
        "        toc_match = re.search(toc_pattern, full_text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        chapter_patterns = [\n",
        "            # Common chapter heading patterns\n",
        "            r'(?:^|\\n)(?:CHAPTER|PART|SECTION)\\s+[\\dIVXLC]+(?:[:\\.\\s]+[^\\n]+)?(?=\\n)',\n",
        "            r'(?:^|\\n)(?:\\d+\\.?\\s+)(?:[A-Z][^\\n]+)(?=\\n)',\n",
        "            r'(?:^|\\n)(?:[IVX]+\\.?\\s+)(?:[A-Z][^\\n]+)(?=\\n)'\n",
        "        ]\n",
        "\n",
        "        # If TOC found, extract chapter titles from it\n",
        "        if toc_match:\n",
        "            toc_text = toc_match.group(1)\n",
        "            toc_entries = re.findall(r'((?:chapter|part|section)\\s+[\\dIVXLC]+[:\\.\\s]+[^\\n]+)',\n",
        "                                    toc_text, re.IGNORECASE)\n",
        "\n",
        "            # Add TOC entries to patterns\n",
        "            for entry in toc_entries:\n",
        "                clean_entry = entry.strip()\n",
        "                chapter_patterns.insert(0, f'(?:^|\\n)(?:{re.escape(clean_entry)})(?=\\n)')\n",
        "\n",
        "        # Find all potential chapter headings\n",
        "        all_headings = []\n",
        "        for pattern in chapter_patterns:\n",
        "            headings = re.finditer(pattern, full_text, re.MULTILINE | re.IGNORECASE)\n",
        "            all_headings.extend([(match.start(), match.group().strip()) for match in headings])\n",
        "\n",
        "        # Sort headings by position in text\n",
        "        all_headings.sort()\n",
        "\n",
        "        # Extract chapter content\n",
        "        chapters = {}\n",
        "        for i, (pos, heading) in enumerate(all_headings):\n",
        "            start_pos = pos + len(heading)\n",
        "\n",
        "            # If not the last chapter, end at the next chapter\n",
        "            if i < len(all_headings) - 1:\n",
        "                end_pos = all_headings[i + 1][0]\n",
        "            else:\n",
        "                end_pos = len(full_text)\n",
        "\n",
        "            chapter_content = full_text[start_pos:end_pos].strip()\n",
        "\n",
        "            # Skip extremely short \"chapters\" (likely false positives)\n",
        "            if len(chapter_content) < 1000:\n",
        "                continue\n",
        "\n",
        "            chapters[heading] = chapter_content\n",
        "\n",
        "        # Fallback method: If no chapters found, try simple numeric chapter detection\n",
        "        if not chapters:\n",
        "            simple_chapters = re.split(r'\\n(?:CHAPTER|PART|SECTION)\\s+[\\dIVXLC]+', full_text)\n",
        "            if len(simple_chapters) > 1:\n",
        "                for i, chapter in enumerate(simple_chapters[1:], 1):\n",
        "                    if len(chapter.strip()) > 1000:  # Skip short sections\n",
        "                        chapters[f\"Chapter {i}\"] = chapter.strip()\n",
        "\n",
        "        # Last resort: split by length into even sections\n",
        "        if not chapters:\n",
        "            # Split into roughly equal sections, aiming for 10-15 chapters\n",
        "            text_length = len(full_text)\n",
        "            section_length = max(10000, text_length // 12)  # Min 10,000 chars per section\n",
        "\n",
        "            for i in range(0, text_length, section_length):\n",
        "                section_text = full_text[i:i + section_length].strip()\n",
        "                if len(section_text) > 1000:  # Skip short sections\n",
        "                    chapters[f\"Section {i//section_length + 1}\"] = section_text\n",
        "\n",
        "        return chapters\n",
        "\n",
        "    def generate_titles(self, chapter_text, chapter_title):\n",
        "        \"\"\"Generate potential evergreen note titles for a chapter\"\"\"\n",
        "        # Prepare system prompt with detailed guidance\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to generate potential titles for evergreen notes based on the chapter provided. \"\n",
        "            \"Evergreen note titles should: \"\n",
        "            \"1. Be complete sentences that express a clear, specific claim or insight. \"\n",
        "            \"2. Convey enough detail to understand the core idea without additional context. \"\n",
        "            \"3. Use clear, precise language and active verbs. \"\n",
        "            \"4. Focus on the most important and insightful ideas from the text. \"\n",
        "            \"5. Be framed positively and constructively. \"\n",
        "            \"6. Avoid vague, general statements - be specific and concrete. \"\n",
        "            \"Generate titles that capture the most significant concepts, arguments, frameworks, and insights from the chapter.\"\n",
        "        )\n",
        "\n",
        "        # Prepare user prompt\n",
        "        user_prompt = (\n",
        "            f\"Generate {self.notes_per_chapter * 2} potential evergreen note titles based on this chapter: \"\n",
        "            f\"CHAPTER TITLE: {chapter_title} \"\n",
        "            f\"CHAPTER CONTENT: {chapter_text} \"\n",
        "            f\"Return ONLY the titles, one per line, numbered.\"\n",
        "        )\n",
        "\n",
        "        # Make API call\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract titles from response\n",
        "        content = response.choices[0].message.content\n",
        "        titles = []\n",
        "\n",
        "        # Parse titles, handling both numbered and unnumbered formats\n",
        "        for line in content.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Remove numbering if present\n",
        "            cleaned_line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
        "            if cleaned_line:\n",
        "                titles.append(cleaned_line)\n",
        "\n",
        "        return titles\n",
        "\n",
        "    def select_best_titles(self, chapter_text, titles, chapter_title):\n",
        "        \"\"\"Select the best titles based on relevance and quality\"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to select the most valuable and insightful evergreen note titles from a list of candidates. \"\n",
        "            \"The best evergreen note titles should: \"\n",
        "            \"1. Capture significant insights or claims from the text. \"\n",
        "            \"2. Be specific, clear, and well-formulated. \"\n",
        "            \"3. Focus on the most important concepts in the chapter. \"\n",
        "            \"4. Cover diverse aspects of the chapter content (avoid redundancy). \"\n",
        "            \"5. Prioritize non-obvious, thought-provoking ideas.\"\n",
        "        )\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"From the following list of potential evergreen note titles for the chapter '{chapter_title}', \"\n",
        "            f\"select exactly {self.notes_per_chapter} titles that best meet the criteria in my instructions. \"\n",
        "            f\"Candidate titles: {' '.join([f'{i+1}. {title}' for i, title in enumerate(titles)])} \"\n",
        "            f\"Return ONLY the numbers of the selected titles in the format: 1, 5, 8, 10, 12 \"\n",
        "            f\"Do not include any explanations or additional text.\"\n",
        "        )\n",
        "\n",
        "        # Make API call\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Parse selected title numbers\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Extract numbers, handling various formats\n",
        "        selected_indices = []\n",
        "        for num in re.findall(r'\\d+', content):\n",
        "            idx = int(num) - 1  # Convert to 0-based index\n",
        "            if 0 <= idx < len(titles):\n",
        "                selected_indices.append(idx)\n",
        "\n",
        "        # Ensure we have the correct number of titles\n",
        "        selected_indices = selected_indices[:self.notes_per_chapter]\n",
        "\n",
        "        # Get the selected titles\n",
        "        selected_titles = [titles[idx] for idx in selected_indices]\n",
        "\n",
        "        # If we don't have enough titles, add more from the original list\n",
        "        if len(selected_titles) < self.notes_per_chapter:\n",
        "            for title in titles:\n",
        "                if title not in selected_titles:\n",
        "                    selected_titles.append(title)\n",
        "                    if len(selected_titles) >= self.notes_per_chapter:\n",
        "                        break\n",
        "\n",
        "        return selected_titles\n",
        "\n",
        "    def generate_note_content(self, chapter_text, title, chapter_title):\n",
        "        \"\"\"Generate content for a single evergreen note\"\"\"\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to write the content for an evergreen note based on the provided title and chapter. \"\n",
        "            \"Your evergreen note should: \"\n",
        "            \"1. Thoroughly explore the specific idea in the title (200-300 words). \"\n",
        "            \"2. Include interpretations, critical thinking, and broader implications. \"\n",
        "            \"3. Be structured clearly with a logical flow of ideas. \"\n",
        "            \"4. Use precise language and concrete examples. \"\n",
        "            \"5. Go beyond summarizing - add insight and connections. \"\n",
        "            \"6. Maintain academic rigor while being accessible. \"\n",
        "            \"7. Look for opportunities to reference related concepts (for interlinking). \"\n",
        "            \"The note should function as a standalone 'API' for this knowledge that can be understood without the original text.\"\n",
        "        )\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Write content for an evergreen note with the following title, based on this chapter: \"\n",
        "            f\"CHAPTER: {chapter_title} \"\n",
        "            f\"NOTE TITLE: {title} \"\n",
        "            f\"CHAPTER CONTENT: {chapter_text} \"\n",
        "            f\"Generate a 200-300 word evergreen note that thoroughly explores this idea.\"\n",
        "        )\n",
        "\n",
        "        # Make API call\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Extract content\n",
        "        content = response.choices[0].message.content\n",
        "        return content\n",
        "\n",
        "    def process_chapter(self, chapter_title, chapter_text):\n",
        "        \"\"\"Process a single chapter to generate evergreen notes\"\"\"\n",
        "        print(f\"Processing: {chapter_title}\")\n",
        "\n",
        "        # Handle large chapters by chunking if necessary\n",
        "        if len(chapter_text) > self.max_chunk_chars:\n",
        "            # For title generation, use the first 1/3 and last 1/3 of the chapter\n",
        "            third = len(chapter_text) // 3\n",
        "            title_text = chapter_text[:third] + \"\\n...\\n\" + chapter_text[-third:]\n",
        "        else:\n",
        "            title_text = chapter_text\n",
        "\n",
        "        # Generate potential titles\n",
        "        titles = self.generate_titles(title_text, chapter_title)\n",
        "\n",
        "        # Select best titles\n",
        "        selected_titles = self.select_best_titles(title_text, titles, chapter_title)\n",
        "\n",
        "        # Generate content for each selected title\n",
        "        notes = []\n",
        "\n",
        "        for title in tqdm(selected_titles, desc=\"Generating notes\", leave=False):\n",
        "            # For content generation, extract relevant parts of the chapter if it's too large\n",
        "            if len(chapter_text) > self.max_chunk_chars:\n",
        "                relevant_text = self.extract_relevant_sections(chapter_text, title)\n",
        "            else:\n",
        "                relevant_text = chapter_text\n",
        "\n",
        "            content = self.generate_note_content(relevant_text, title, chapter_title)\n",
        "\n",
        "            notes.append({\n",
        "                \"title\": title,\n",
        "                \"content\": content\n",
        "            })\n",
        "\n",
        "        return notes\n",
        "\n",
        "    def extract_relevant_sections(self, chapter_text, title):\n",
        "        \"\"\"Extract the most relevant parts of a long chapter for a specific title\"\"\"\n",
        "        # Split chapter into paragraphs\n",
        "        paragraphs = [p for p in chapter_text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        # Conservative max size (GPT-4o can handle ~100k tokens, but we'll use less)\n",
        "        max_chars = self.max_chunk_chars // 2\n",
        "\n",
        "        if len(chapter_text) <= max_chars:\n",
        "            return chapter_text\n",
        "\n",
        "        # Extract important keywords from title\n",
        "        title_words = set(re.findall(r'\\b\\w{4,}\\b', title.lower()))\n",
        "        # Remove common stop words\n",
        "        stop_words = {'this', 'that', 'with', 'from', 'have', 'they', 'what', 'when', 'where', 'which', 'their'}\n",
        "        title_words = {w for w in title_words if w not in stop_words}\n",
        "\n",
        "        # Score paragraphs based on relevance to title\n",
        "        scored_paragraphs = []\n",
        "\n",
        "        for i, para in enumerate(paragraphs):\n",
        "            score = 0\n",
        "            para_lower = para.lower()\n",
        "\n",
        "            # Score based on keyword matches\n",
        "            for word in title_words:\n",
        "                # Full word matches\n",
        "                score += para_lower.count(f' {word} ') * 3\n",
        "                # Partial matches\n",
        "                score += para_lower.count(word) * 2\n",
        "\n",
        "            # Boost score for position in chapter\n",
        "            # First few paragraphs get a boost for context\n",
        "            if i < 3:\n",
        "                score += 5\n",
        "            # Introduction paragraphs get a boost\n",
        "            elif i < len(paragraphs) * 0.1:\n",
        "                score += 3\n",
        "            # Conclusion paragraphs get a boost\n",
        "            elif i > len(paragraphs) * 0.9:\n",
        "                score += 2\n",
        "\n",
        "            # Length factor (prefer substantial paragraphs)\n",
        "            if 100 <= len(para) <= 1000:\n",
        "                score += 1\n",
        "\n",
        "            scored_paragraphs.append((score, i, para))\n",
        "\n",
        "        # Sort by score descending\n",
        "        scored_paragraphs.sort(reverse=True)\n",
        "\n",
        "        # Get introduction (first few paragraphs)\n",
        "        intro_size = min(3, len(paragraphs))\n",
        "        intro = '\\n\\n'.join(paragraphs[:intro_size])\n",
        "\n",
        "        # Get top-scoring paragraphs\n",
        "        selected = []\n",
        "        current_size = len(intro)\n",
        "\n",
        "        for score, i, para in scored_paragraphs:\n",
        "            # Skip paragraphs already in intro\n",
        "            if i < intro_size:\n",
        "                continue\n",
        "\n",
        "            # Stop if we've reached the size limit\n",
        "            if current_size + len(para) > max_chars:\n",
        "                break\n",
        "\n",
        "            # Add paragraph\n",
        "            selected.append((i, para))\n",
        "            current_size += len(para)\n",
        "\n",
        "        # Sort selected paragraphs by original position\n",
        "        selected.sort()\n",
        "\n",
        "        # Combine intro with selected paragraphs\n",
        "        result = intro + '\\n\\n' + '\\n\\n'.join(para for _, para in selected)\n",
        "\n",
        "        # If we have space, add a few more paragraphs for context\n",
        "        remaining = max_chars - len(result)\n",
        "        if remaining > 5000:\n",
        "            additional = []\n",
        "            for i, para in enumerate(paragraphs):\n",
        "                # Skip paragraphs we already have\n",
        "                if i < intro_size or any(idx == i for idx, _ in selected):\n",
        "                    continue\n",
        "\n",
        "                if len(para) < remaining:\n",
        "                    additional.append((i, para))\n",
        "                    remaining -= len(para)\n",
        "\n",
        "                    # Stop if we've added enough\n",
        "                    if remaining < 5000:\n",
        "                        break\n",
        "\n",
        "            # Add additional paragraphs in order\n",
        "            additional.sort()\n",
        "            for _, para in additional:\n",
        "                result += '\\n\\n' + para\n",
        "\n",
        "        return result\n",
        "\n",
        "    def save_notes_as_text(self, notes, chapter_title, output_dir):\n",
        "        \"\"\"Save notes in plain text format\"\"\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "        filename = os.path.join(output_dir, f\"{safe_chapter_title}_notes.txt\")\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Evergreen Notes for: {chapter_title}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "            for i, note in enumerate(notes, 1):\n",
        "                f.write(f\"Note {i}: {note['title']}\\n\")\n",
        "                f.write(\"-\" * 80 + \"\\n\")\n",
        "                f.write(note['content'] + \"\\n\\n\")\n",
        "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def save_notes_as_json(self, notes, chapter_title, output_dir):\n",
        "        \"\"\"Save notes in JSON format\"\"\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "        filename = os.path.join(output_dir, f\"{safe_chapter_title}_notes.json\")\n",
        "\n",
        "        # Add IDs to notes for reference\n",
        "        numbered_notes = []\n",
        "        for i, note in enumerate(notes, 1):\n",
        "            note_with_id = {\n",
        "                \"id\": f\"{safe_chapter_title}_note_{i}\",\n",
        "                \"title\": note['title'],\n",
        "                \"content\": note['content']\n",
        "            }\n",
        "            numbered_notes.append(note_with_id)\n",
        "\n",
        "        data = {\n",
        "            \"chapter\": chapter_title,\n",
        "            \"notes\": numbered_notes\n",
        "        }\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def process_book(self, pdf_path, output_dir, parallel=True):\n",
        "        \"\"\"Process the entire book to generate evergreen notes\"\"\"\n",
        "        print(f\"Extracting text from PDF: {pdf_path}\")\n",
        "        full_text = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        print(\"Identifying chapters...\")\n",
        "        chapters = self.extract_chapters(full_text)\n",
        "        print(f\"Found {len(chapters)} chapters.\")\n",
        "\n",
        "        if len(chapters) == 0:\n",
        "            print(\"No chapters identified. Please check the PDF format.\")\n",
        "            return None\n",
        "\n",
        "        # Process each chapter\n",
        "        all_notes = {}\n",
        "\n",
        "        if parallel and len(chapters) > 1:\n",
        "            # Process chapters in parallel\n",
        "            with ThreadPoolExecutor(max_workers=min(5, len(chapters))) as executor:\n",
        "                future_to_chapter = {\n",
        "                    executor.submit(self.process_chapter, title, text): title\n",
        "                    for title, text in chapters.items()\n",
        "                }\n",
        "\n",
        "                for future in tqdm(future_to_chapter, desc=\"Processing chapters\"):\n",
        "                    chapter_title = future_to_chapter[future]\n",
        "                    try:\n",
        "                        chapter_notes = future.result()\n",
        "\n",
        "                        # Save chapter notes\n",
        "                        txt_file = self.save_notes_as_text(chapter_notes, chapter_title, output_dir)\n",
        "                        json_file = self.save_notes_as_json(chapter_notes, chapter_title, output_dir)\n",
        "\n",
        "                        print(f\"✓ Saved notes for '{chapter_title}' to {txt_file} and {json_file}\")\n",
        "                        all_notes[chapter_title] = chapter_notes\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing chapter '{chapter_title}': {e}\")\n",
        "        else:\n",
        "            # Process chapters sequentially\n",
        "            for chapter_title, chapter_text in tqdm(chapters.items(), desc=\"Processing chapters\"):\n",
        "                try:\n",
        "                    chapter_notes = self.process_chapter(chapter_title, chapter_text)\n",
        "\n",
        "                    # Save chapter notes\n",
        "                    txt_file = self.save_notes_as_text(chapter_notes, chapter_title, output_dir)\n",
        "                    json_file = self.save_notes_as_json(chapter_notes, chapter_title, output_dir)\n",
        "\n",
        "                    print(f\"✓ Saved notes for '{chapter_title}' to {txt_file} and {json_file}\")\n",
        "                    all_notes[chapter_title] = chapter_notes\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chapter '{chapter_title}': {e}\")\n",
        "\n",
        "        # Save combined JSON with all notes\n",
        "        combined_json = os.path.join(output_dir, \"all_notes.json\")\n",
        "\n",
        "        # Create a structured representation of all notes\n",
        "        book_notes = {\n",
        "            \"book\": os.path.basename(pdf_path),\n",
        "            \"chapters\": []\n",
        "        }\n",
        "\n",
        "        for chapter_title, chapter_notes in all_notes.items():\n",
        "            safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "            chapter_data = {\n",
        "                \"title\": chapter_title,\n",
        "                \"id\": safe_chapter_title,\n",
        "                \"notes\": []\n",
        "            }\n",
        "\n",
        "            for i, note in enumerate(chapter_notes, 1):\n",
        "                note_id = f\"{safe_chapter_title}_note_{i}\"\n",
        "                note_with_id = {\n",
        "                    \"id\": note_id,\n",
        "                    \"title\": note['title'],\n",
        "                    \"content\": note['content']\n",
        "                }\n",
        "                chapter_data[\"notes\"].append(note_with_id)\n",
        "\n",
        "            book_notes[\"chapters\"].append(chapter_data)\n",
        "\n",
        "        with open(combined_json, 'w', encoding='utf-8') as f:\n",
        "            json.dump(book_notes, f, indent=2)\n",
        "\n",
        "        print(f\"✓ Combined notes saved to {combined_json}\")\n",
        "\n",
        "        return book_notes\n",
        "\n",
        "\n",
        "# Example usage in Google Colab\n",
        "def run_in_colab(pdf_file_path, api_key, notes_per_chapter=5):\n",
        "    \"\"\"\n",
        "    Run the evergreen notes generator in Google Colab\n",
        "\n",
        "    Args:\n",
        "        pdf_file_path (str): Path to the PDF file\n",
        "        api_key (str): OpenAI API key\n",
        "        notes_per_chapter (int): Number of notes to generate per chapter\n",
        "    \"\"\"\n",
        "    output_dir = \"./evergreen_notes_output\"\n",
        "\n",
        "    # Initialize the generator\n",
        "    generator = EvergreenNotesGenerator(\n",
        "        api_key=api_key,\n",
        "        model=\"gpt-4o\",\n",
        "        notes_per_chapter=notes_per_chapter\n",
        "    )\n",
        "\n",
        "    # Process the book\n",
        "    generator.process_book(pdf_file_path, output_dir)\n",
        "\n",
        "    # Zip the output directory for easy download\n",
        "    os.system(f\"zip -r evergreen_notes.zip {output_dir}\")\n",
        "\n",
        "    print(\"Processing complete! You can now download the evergreen_notes.zip file.\")\n",
        "\n",
        "    # For Google Colab, you would use:\n",
        "    # from google.colab import files\n",
        "    # files.download(\"evergreen_notes.zip\")\n",
        "\n",
        "# INSTEAD, add this code at the end:\n",
        "from google.colab import files\n",
        "\n",
        "# Upload PDF file\n",
        "print(\"Please upload your PDF book:\")\n",
        "uploaded = files.upload()\n",
        "pdf_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Get OpenAI API key\n",
        "api_key = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "# Run the generator\n",
        "run_in_colab(pdf_filename, api_key, notes_per_chapter=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1CCarFt7lXP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import PyPDF2\n",
        "import openai\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "class EvergreenNotesGenerator:\n",
        "    def __init__(self, api_key=None, model_config=None, notes_per_chapter=5):\n",
        "        \"\"\"\n",
        "        Initialize the Evergreen Notes Generator\n",
        "\n",
        "        Args:\n",
        "            api_key (str): OpenAI API key\n",
        "            model_config (dict): Model configuration with different models for different tasks\n",
        "            notes_per_chapter (int): Number of notes to generate per chapter\n",
        "        \"\"\"\n",
        "        if api_key:\n",
        "            self.client = openai.OpenAI(api_key=api_key)\n",
        "        else:\n",
        "            self.client = openai.OpenAI()\n",
        "\n",
        "        # Default model configuration (using GPT-4o for everything)\n",
        "        self.model_config = {\n",
        "            'title_generation': 'gpt-4o-mini',  # Titles can use the mini model\n",
        "            'title_selection': 'gpt-4o-mini',   # Selection can use the mini model\n",
        "            'content_generation': 'gpt-4o'      # Content needs the full model\n",
        "        }\n",
        "\n",
        "        # Override with custom config if provided\n",
        "        if model_config:\n",
        "            self.model_config.update(model_config)\n",
        "\n",
        "        self.notes_per_chapter = notes_per_chapter\n",
        "\n",
        "        # Limiting context sizes based on models\n",
        "        self.max_chunk_chars = 300000  # For GPT-4o (~75k tokens)\n",
        "        self.mini_max_chunk_chars = 200000  # For GPT-4o-mini (smaller to be safe)\n",
        "\n",
        "    def rate_limited_api_call(self, messages, model, max_retries=8, initial_wait=2):\n",
        "        \"\"\"Make an API call with automatic retry on rate limit errors\"\"\"\n",
        "        retry_count = 0\n",
        "        wait_time = initial_wait\n",
        "\n",
        "        while retry_count < max_retries:\n",
        "            try:\n",
        "                return self.client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=messages\n",
        "                )\n",
        "            except Exception as e:\n",
        "                error_str = str(e)\n",
        "                if \"rate_limit_exceeded\" in error_str:\n",
        "                    # Extract wait time from error message if possible\n",
        "                    wait_match = re.search(r'try again in (\\d+\\.\\d+)s', error_str)\n",
        "                    if wait_match:\n",
        "                        wait_time = float(wait_match.group(1)) * 1.5  # Add a 50% buffer\n",
        "\n",
        "                    # Add some jitter to avoid thundering herd problem\n",
        "                    jitter = random.uniform(0.5, 1.5)\n",
        "                    adjusted_wait = wait_time * jitter\n",
        "\n",
        "                    print(f\"Rate limit hit for {model}. Waiting {adjusted_wait:.2f}s before retry {retry_count+1}/{max_retries}\")\n",
        "                    time.sleep(adjusted_wait)\n",
        "                    wait_time *= 1.5  # Increase wait time for next attempt\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    # Non-rate-limit error, re-raise\n",
        "                    raise\n",
        "\n",
        "        # If we get here, we've exhausted our retries\n",
        "        raise Exception(f\"Failed after {max_retries} attempts due to rate limiting\")\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        \"\"\"Extract full text from PDF file\"\"\"\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                try:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not extract text from a page: {e}\")\n",
        "        return text\n",
        "\n",
        "    def extract_chapters(self, full_text):\n",
        "        \"\"\"\n",
        "        Advanced chapter extraction from text content\n",
        "        Uses multiple methods to identify chapter boundaries\n",
        "        \"\"\"\n",
        "        # Keep original text for better chapter detection\n",
        "        original_text = full_text\n",
        "\n",
        "        # Clean up text version for pattern matching\n",
        "        clean_text = re.sub(r'\\s+', ' ', full_text)\n",
        "        clean_text = re.sub(r'(\\w)- (\\w)', r'\\1\\2', clean_text)  # Fix hyphenated words\n",
        "\n",
        "        # Try to find a table of contents\n",
        "        print(\"Looking for table of contents...\")\n",
        "        toc_patterns = [\n",
        "            r'(?:TABLE\\s+OF\\s+CONTENTS|CONTENTS).*?\\n(.*?)(?:\\n\\s*\\n|\\n(?:CHAPTER|INTRODUCTION))',\n",
        "            r'(?:CONTENTS|INDEX).*?\\n(.*?)(?:\\n\\s*\\n)'\n",
        "        ]\n",
        "\n",
        "        toc_entries = []\n",
        "        for pattern in toc_patterns:\n",
        "            try:\n",
        "                toc_match = re.search(pattern, original_text, re.DOTALL | re.IGNORECASE)\n",
        "                if toc_match:\n",
        "                    toc_text = toc_match.group(1)\n",
        "                    # Extract chapter titles and page numbers from TOC\n",
        "                    toc_entries = re.findall(r'((?:chapter|part|section)\\s+[\\dIVXLC]+[:\\.\\s]+[^0-9\\n]+)',\n",
        "                                           toc_text, re.IGNORECASE)\n",
        "                    if toc_entries:\n",
        "                        print(f\"Found {len(toc_entries)} entries in table of contents\")\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing TOC pattern: {e}\")\n",
        "\n",
        "        # For your book (religious text), look for specific patterns\n",
        "        print(\"Searching for chapter patterns...\")\n",
        "        chapter_patterns = [\n",
        "            # Standard chapter headings\n",
        "            r'(?:^|\\n)(?:CHAPTER|CH\\.|PART|SECTION)\\s+[\\dIVXLC]+(?:[:\\.\\s]+[^\\n]+)?(?=\\n)',\n",
        "            # Numbered sections (like \"1. Introduction\")\n",
        "            r'(?:^|\\n)(?:\\d+\\.?\\s+)(?:[A-Z][^\\n]{5,})(?=\\n)',\n",
        "            # Roman numeral sections\n",
        "            r'(?:^|\\n)(?:[IVX]+\\.?\\s+)(?:[A-Z][^\\n]{5,})(?=\\n)',\n",
        "            # Try patterns specific to religious/spiritual books\n",
        "            r'(?:^|\\n)(?:Prayer|Meditation|Reflection)(?:\\s+[\\dIVXLC]+)?(?:[:\\.\\s]+[^\\n]+)?(?=\\n)',\n",
        "            # Try bold/emphasized headings (hard to detect without formatting info)\n",
        "            r'(?:^|\\n)([A-Z][A-Z\\s]{3,}[A-Z])(?=\\n)',\n",
        "            # Try detecting based on line breaks and capitalization\n",
        "            r'(?:\\n\\n)([A-Z][^a-z\\n]{0,10}[A-Za-z][^\\n]{3,60})(?:\\n\\n)'\n",
        "        ]\n",
        "\n",
        "        # Add patterns from TOC entries\n",
        "        if toc_entries:\n",
        "            for entry in toc_entries:\n",
        "                entry_clean = re.sub(r'\\s+', ' ', entry.strip())\n",
        "                # Convert to regex pattern, escaping special characters\n",
        "                entry_pattern = re.escape(entry_clean)\n",
        "                # Make it more flexible with whitespace\n",
        "                entry_pattern = re.sub(r'\\\\\\s+', r'\\\\s+', entry_pattern)\n",
        "                chapter_patterns.insert(0, f'(?:^|\\n)(?:{entry_pattern})(?=\\n)')\n",
        "\n",
        "        # For your specific book, add custom pattern\n",
        "        # This seems to be working from your previous run\n",
        "        chapter_patterns.insert(0, r'(?:^|\\n)(\\d+\\.\\s+[A-Z][^\\n]{5,})(?=\\n)')\n",
        "\n",
        "        # Find all potential chapter headings\n",
        "        all_headings = []\n",
        "        for pattern in chapter_patterns:\n",
        "            try:\n",
        "                headings = re.finditer(pattern, original_text, re.MULTILINE | re.IGNORECASE)\n",
        "                matches = [(match.start(), match.group().strip()) for match in headings]\n",
        "                if matches:\n",
        "                    print(f\"Pattern '{pattern}' found {len(matches)} matches\")\n",
        "                all_headings.extend(matches)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error with pattern {pattern}: {e}\")\n",
        "\n",
        "        # Sort headings by position in text and remove duplicates\n",
        "        all_headings.sort()\n",
        "        unique_headings = []\n",
        "        for pos, heading in all_headings:\n",
        "            if not unique_headings or pos > unique_headings[-1][0] + 50:  # Allow closer headings\n",
        "                unique_headings.append((pos, heading))\n",
        "\n",
        "        print(f\"Found {len(unique_headings)} potential chapter headings\")\n",
        "\n",
        "        # Extract chapter content\n",
        "        chapters = {}\n",
        "\n",
        "        # REMOVE the check for too many chapters - some books have many small chapters\n",
        "\n",
        "        for i, (pos, heading) in enumerate(unique_headings):\n",
        "            start_pos = pos + len(heading)\n",
        "\n",
        "            # If not the last chapter, end at the next chapter\n",
        "            if i < len(unique_headings) - 1:\n",
        "                end_pos = unique_headings[i + 1][0]\n",
        "            else:\n",
        "                end_pos = len(original_text)\n",
        "\n",
        "            chapter_content = original_text[start_pos:end_pos].strip()\n",
        "\n",
        "            # Reduced minimum size to catch smaller chapters\n",
        "            if len(chapter_content) < 500:\n",
        "                continue\n",
        "\n",
        "            # Clean up chapter title\n",
        "            clean_heading = re.sub(r'\\s+', ' ', heading).strip()\n",
        "            # Limit chapter title length\n",
        "            if len(clean_heading) > 100:\n",
        "                clean_heading = clean_heading[:97] + \"...\"\n",
        "\n",
        "            chapters[clean_heading] = chapter_content\n",
        "\n",
        "        # Fallback 1: Try detecting chapters based on page numbers or markers\n",
        "        if not chapters:\n",
        "            print(\"Trying to detect chapters based on page breaks...\")\n",
        "            page_break_patterns = [\n",
        "                r'\\n\\s*\\d+\\s*\\n',  # Page numbers\n",
        "                r'\\n\\s*\\*\\s*\\*\\s*\\*\\s*\\n',  # Asterisk dividers\n",
        "                r'\\n\\s*-{3,}\\s*\\n'  # Dash dividers\n",
        "            ]\n",
        "\n",
        "            for pattern in page_break_patterns:\n",
        "                sections = re.split(pattern, original_text)\n",
        "                if len(sections) > 5:  # If we found reasonable number of sections\n",
        "                    print(f\"Found {len(sections)} sections using page break pattern\")\n",
        "                    for i, section in enumerate(sections, 1):\n",
        "                        if len(section.strip()) > 500:\n",
        "                            chapters[f\"Section {i}\"] = section.strip()\n",
        "                    break\n",
        "\n",
        "        # Fallback 2: If still no chapters, try simpler patterns\n",
        "        if not chapters:\n",
        "            print(\"Using simplified chapter detection method...\")\n",
        "            simple_chapters = re.split(r'\\n(?:CHAPTER|PART|SECTION)\\s+[\\dIVXLC]+\\s*[:\\.]?', original_text)\n",
        "            if len(simple_chapters) > 1:\n",
        "                for i, chapter in enumerate(simple_chapters[1:], 1):\n",
        "                    if len(chapter.strip()) > 500:  # Lower minimum size\n",
        "                        chapters[f\"Chapter {i}\"] = chapter.strip()\n",
        "\n",
        "        # Fallback 3: Split by double line breaks to find potential sections\n",
        "        if not chapters:\n",
        "            print(\"Trying to split by paragraph breaks...\")\n",
        "            sections = re.split(r'\\n\\s*\\n\\s*\\n', original_text)  # Try to find major paragraph breaks\n",
        "            if len(sections) >= 5:  # If we have at least 5 sections\n",
        "                # Group sections into chapters (approximately 5-7 sections per chapter)\n",
        "                section_per_chapter = max(3, len(sections) // 10)\n",
        "                for i in range(0, len(sections), section_per_chapter):\n",
        "                    end_idx = min(i + section_per_chapter, len(sections))\n",
        "                    chapter_text = \"\\n\\n\".join(sections[i:end_idx])\n",
        "                    if len(chapter_text.strip()) > 500:\n",
        "                        chapters[f\"Part {i//section_per_chapter + 1}\"] = chapter_text\n",
        "\n",
        "        # Last resort: split by length into even sections\n",
        "        if not chapters:\n",
        "            print(\"No chapters detected. Creating equal-length sections...\")\n",
        "            # Split into roughly equal sections\n",
        "            text_length = len(original_text)\n",
        "            # Adjusted to create more sections for a spiritual/religious book\n",
        "            section_length = max(5000, text_length // 20)\n",
        "\n",
        "            for i in range(0, text_length, section_length):\n",
        "                section_text = original_text[i:i + section_length].strip()\n",
        "                if len(section_text) > 500:  # Lower minimum size\n",
        "                    chapters[f\"Section {i//section_length + 1}\"] = section_text\n",
        "\n",
        "        print(f\"Successfully extracted {len(chapters)} chapters/sections\")\n",
        "        return chapters\n",
        "\n",
        "    def generate_titles(self, chapter_text, chapter_title):\n",
        "        \"\"\"Generate potential evergreen note titles for a chapter\"\"\"\n",
        "        # Use the mini model for title generation\n",
        "        model = self.model_config['title_generation']\n",
        "\n",
        "        # Prepare system prompt with detailed guidance\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to generate potential titles for evergreen notes based on the chapter provided. \"\n",
        "            \"Evergreen note titles should: \"\n",
        "            \"1. Be complete sentences that express a clear, specific claim or insight. \"\n",
        "            \"2. Convey enough detail to understand the core idea without additional context. \"\n",
        "            \"3. Use clear, precise language and active verbs. \"\n",
        "            \"4. Focus on the most important and insightful ideas from the text. \"\n",
        "            \"5. Be framed positively and constructively. \"\n",
        "            \"6. Avoid vague, general statements - be specific and concrete. \"\n",
        "            \"Generate titles that capture the most significant concepts, arguments, frameworks, and insights from the chapter.\"\n",
        "        )\n",
        "\n",
        "        # Limit text size based on mini model\n",
        "        max_size = self.mini_max_chunk_chars\n",
        "        if len(chapter_text) > max_size:\n",
        "            # Use first and last portions with an indication of truncation\n",
        "            portion_size = max_size // 2 - 50\n",
        "            chapter_text = chapter_text[:portion_size] + \"\\n[...TEXT TRUNCATED...]\\n\" + chapter_text[-portion_size:]\n",
        "\n",
        "        # Prepare user prompt\n",
        "        user_prompt = (\n",
        "            f\"Generate {self.notes_per_chapter * 2} potential evergreen note titles based on this chapter: \"\n",
        "            f\"CHAPTER TITLE: {chapter_title} \"\n",
        "            f\"CHAPTER CONTENT: {chapter_text} \"\n",
        "            f\"Return ONLY the titles, one per line, numbered.\"\n",
        "        )\n",
        "\n",
        "        # Make API call with rate limiting\n",
        "        response = self.rate_limited_api_call(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        # Extract titles from response\n",
        "        content = response.choices[0].message.content\n",
        "        titles = []\n",
        "\n",
        "        # Parse titles, handling both numbered and unnumbered formats\n",
        "        for line in content.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Remove numbering if present\n",
        "            cleaned_line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
        "            if cleaned_line:\n",
        "                titles.append(cleaned_line)\n",
        "\n",
        "        return titles\n",
        "\n",
        "    def select_best_titles(self, chapter_text, titles, chapter_title):\n",
        "        \"\"\"Select the best titles based on relevance and quality\"\"\"\n",
        "        # Use the mini model for title selection\n",
        "        model = self.model_config['title_selection']\n",
        "\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to select the most valuable and insightful evergreen note titles from a list of candidates. \"\n",
        "            \"The best evergreen note titles should: \"\n",
        "            \"1. Capture significant insights or claims from the text. \"\n",
        "            \"2. Be specific, clear, and well-formulated. \"\n",
        "            \"3. Focus on the most important concepts in the chapter. \"\n",
        "            \"4. Cover diverse aspects of the chapter content (avoid redundancy). \"\n",
        "            \"5. Prioritize non-obvious, thought-provoking ideas.\"\n",
        "        )\n",
        "\n",
        "        # Format the titles with numbers\n",
        "        formatted_titles = \"\\n\".join([f\"{i+1}. {title}\" for i, title in enumerate(titles)])\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"From the following list of potential evergreen note titles for the chapter '{chapter_title}', \"\n",
        "            f\"select exactly {self.notes_per_chapter} titles that best meet the criteria in my instructions. \"\n",
        "            f\"Candidate titles:\\n{formatted_titles}\\n\\n\"\n",
        "            f\"Return ONLY the numbers of the selected titles in the format: 1, 5, 8, 10, 12 \"\n",
        "            f\"Do not include any explanations or additional text.\"\n",
        "        )\n",
        "\n",
        "        # Make API call with rate limiting\n",
        "        response = self.rate_limited_api_call(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        # Parse selected title numbers\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Extract numbers, handling various formats\n",
        "        selected_indices = []\n",
        "        for num in re.findall(r'\\d+', content):\n",
        "            idx = int(num) - 1  # Convert to 0-based index\n",
        "            if 0 <= idx < len(titles):\n",
        "                selected_indices.append(idx)\n",
        "\n",
        "        # Ensure we have the correct number of titles\n",
        "        selected_indices = selected_indices[:self.notes_per_chapter]\n",
        "\n",
        "        # Get the selected titles\n",
        "        selected_titles = [titles[idx] for idx in selected_indices]\n",
        "\n",
        "        # If we don't have enough titles, add more from the original list\n",
        "        if len(selected_titles) < self.notes_per_chapter:\n",
        "            for title in titles:\n",
        "                if title not in selected_titles:\n",
        "                    selected_titles.append(title)\n",
        "                    if len(selected_titles) >= self.notes_per_chapter:\n",
        "                        break\n",
        "\n",
        "        return selected_titles\n",
        "\n",
        "    def generate_note_content(self, chapter_text, title, chapter_title):\n",
        "        \"\"\"Generate content for a single evergreen note\"\"\"\n",
        "        # Use the full model for content generation\n",
        "        model = self.model_config['content_generation']\n",
        "\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to write the content for an evergreen note based on the provided title and chapter. \"\n",
        "            \"Your evergreen note should: \"\n",
        "            \"1. Thoroughly explore the specific idea in the title (200-300 words). \"\n",
        "            \"2. Include interpretations, critical thinking, and broader implications. \"\n",
        "            \"3. Be structured clearly with a logical flow of ideas. \"\n",
        "            \"4. Use precise language and concrete examples. \"\n",
        "            \"5. Go beyond summarizing - add insight and connections. \"\n",
        "            \"6. Maintain academic rigor while being accessible. \"\n",
        "            \"7. Look for opportunities to reference related concepts (for interlinking). \"\n",
        "            \"The note should function as a standalone 'API' for this knowledge that can be understood without the original text.\"\n",
        "        )\n",
        "\n",
        "        # Extract relevant parts if chapter is too large\n",
        "        if len(chapter_text) > self.max_chunk_chars:\n",
        "            relevant_text = self.extract_relevant_sections(chapter_text, title)\n",
        "        else:\n",
        "            relevant_text = chapter_text\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Write content for an evergreen note with the following title, based on this chapter: \"\n",
        "            f\"CHAPTER: {chapter_title} \"\n",
        "            f\"NOTE TITLE: {title} \"\n",
        "            f\"CHAPTER CONTENT: {relevant_text} \"\n",
        "            f\"Generate a 200-300 word evergreen note that thoroughly explores this idea.\"\n",
        "        )\n",
        "\n",
        "        # Make API call with rate limiting\n",
        "        response = self.rate_limited_api_call(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        # Extract content\n",
        "        content = response.choices[0].message.content\n",
        "        return content\n",
        "\n",
        "    def process_chapter(self, chapter_title, chapter_text):\n",
        "        \"\"\"Process a single chapter to generate evergreen notes\"\"\"\n",
        "        try:\n",
        "            print(f\"Processing: {chapter_title}\")\n",
        "\n",
        "            # Handle large chapters by chunking if necessary\n",
        "            if len(chapter_text) > self.mini_max_chunk_chars:\n",
        "                # For title generation, use the first 1/3 and last 1/3 of the chapter\n",
        "                third = len(chapter_text) // 3\n",
        "                title_text = chapter_text[:third] + \"\\n...\\n\" + chapter_text[-third:]\n",
        "            else:\n",
        "                title_text = chapter_text\n",
        "\n",
        "            # Generate potential titles\n",
        "            titles = self.generate_titles(title_text, chapter_title)\n",
        "\n",
        "            # Add delay after title generation to avoid rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "            # Select best titles\n",
        "            selected_titles = self.select_best_titles(title_text, titles, chapter_title)\n",
        "\n",
        "            # Add delay after title selection to avoid rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "            # Generate content for each selected title\n",
        "            notes = []\n",
        "\n",
        "            for title in tqdm(selected_titles, desc=\"Generating notes\", leave=False):\n",
        "                # For content generation, extract relevant parts of the chapter if it's too large\n",
        "                if len(chapter_text) > self.max_chunk_chars:\n",
        "                    relevant_text = self.extract_relevant_sections(chapter_text, title)\n",
        "                else:\n",
        "                    relevant_text = chapter_text\n",
        "\n",
        "                content = self.generate_note_content(relevant_text, title, chapter_title)\n",
        "\n",
        "                notes.append({\n",
        "                    \"title\": title,\n",
        "                    \"content\": content\n",
        "                })\n",
        "\n",
        "                # Add delay between note generation to avoid rate limits\n",
        "                time.sleep(2)\n",
        "\n",
        "            return notes\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in process_chapter for '{chapter_title}': {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_relevant_sections(self, chapter_text, title):\n",
        "        \"\"\"Extract the most relevant parts of a long chapter for a specific title\"\"\"\n",
        "        # Split chapter into paragraphs\n",
        "        paragraphs = [p for p in chapter_text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        # Conservative max size\n",
        "        max_chars = self.max_chunk_chars // 2\n",
        "\n",
        "        if len(chapter_text) <= max_chars:\n",
        "            return chapter_text\n",
        "\n",
        "        # Extract important keywords from title\n",
        "        title_words = set(re.findall(r'\\b\\w{4,}\\b', title.lower()))\n",
        "        # Remove common stop words\n",
        "        stop_words = {'this', 'that', 'with', 'from', 'have', 'they', 'what', 'when', 'where', 'which', 'their'}\n",
        "        title_words = {w for w in title_words if w not in stop_words}\n",
        "\n",
        "        # Score paragraphs based on relevance to title\n",
        "        scored_paragraphs = []\n",
        "\n",
        "        for i, para in enumerate(paragraphs):\n",
        "            score = 0\n",
        "            para_lower = para.lower()\n",
        "\n",
        "            # Score based on keyword matches\n",
        "            for word in title_words:\n",
        "                # Full word matches\n",
        "                score += para_lower.count(f' {word} ') * 3\n",
        "                # Partial matches\n",
        "                score += para_lower.count(word) * 2\n",
        "\n",
        "            # Boost score for position in chapter\n",
        "            # First few paragraphs get a boost for context\n",
        "            if i < 3:\n",
        "                score += 5\n",
        "            # Introduction paragraphs get a boost\n",
        "            elif i < len(paragraphs) * 0.1:\n",
        "                score += 3\n",
        "            # Conclusion paragraphs get a boost\n",
        "            elif i > len(paragraphs) * 0.9:\n",
        "                score += 2\n",
        "\n",
        "            # Length factor (prefer substantial paragraphs)\n",
        "            if 100 <= len(para) <= 1000:\n",
        "                score += 1\n",
        "\n",
        "            scored_paragraphs.append((score, i, para))\n",
        "\n",
        "        # Sort by score descending\n",
        "        scored_paragraphs.sort(reverse=True)\n",
        "\n",
        "        # Get introduction (first few paragraphs)\n",
        "        intro_size = min(3, len(paragraphs))\n",
        "        intro = '\\n\\n'.join(paragraphs[:intro_size])\n",
        "\n",
        "        # Get top-scoring paragraphs\n",
        "        selected = []\n",
        "        current_size = len(intro)\n",
        "\n",
        "        for score, i, para in scored_paragraphs:\n",
        "            # Skip paragraphs already in intro\n",
        "            if i < intro_size:\n",
        "                continue\n",
        "\n",
        "            # Stop if we've reached the size limit\n",
        "            if current_size + len(para) > max_chars:\n",
        "                break\n",
        "\n",
        "            # Add paragraph\n",
        "            selected.append((i, para))\n",
        "            current_size += len(para)\n",
        "\n",
        "        # Sort selected paragraphs by original position\n",
        "        selected.sort()\n",
        "\n",
        "        # Combine intro with selected paragraphs\n",
        "        result = intro + '\\n\\n' + '\\n\\n'.join(para for _, para in selected)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def save_notes_as_text(self, notes, chapter_title, output_dir):\n",
        "        \"\"\"Save notes in plain text format\"\"\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "        filename = os.path.join(output_dir, f\"{safe_chapter_title}_notes.txt\")\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Evergreen Notes for: {chapter_title}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "            for i, note in enumerate(notes, 1):\n",
        "                f.write(f\"Note {i}: {note['title']}\\n\")\n",
        "                f.write(\"-\" * 80 + \"\\n\")\n",
        "                f.write(note['content'] + \"\\n\\n\")\n",
        "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def save_notes_as_json(self, notes, chapter_title, output_dir):\n",
        "        \"\"\"Save notes in JSON format\"\"\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "        filename = os.path.join(output_dir, f\"{safe_chapter_title}_notes.json\")\n",
        "\n",
        "        # Add IDs to notes for reference\n",
        "        numbered_notes = []\n",
        "        for i, note in enumerate(notes, 1):\n",
        "            note_with_id = {\n",
        "                \"id\": f\"{safe_chapter_title}_note_{i}\",\n",
        "                \"title\": note['title'],\n",
        "                \"content\": note['content']\n",
        "            }\n",
        "            numbered_notes.append(note_with_id)\n",
        "\n",
        "        data = {\n",
        "            \"chapter\": chapter_title,\n",
        "            \"notes\": numbered_notes\n",
        "        }\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def process_book(self, pdf_path, output_dir, parallel=False):\n",
        "        \"\"\"Process the entire book to generate evergreen notes\"\"\"\n",
        "        print(f\"Extracting text from PDF: {pdf_path}\")\n",
        "        full_text = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        print(\"Identifying chapters...\")\n",
        "        chapters = self.extract_chapters(full_text)\n",
        "        print(f\"Found {len(chapters)} chapters.\")\n",
        "\n",
        "        if len(chapters) == 0:\n",
        "            print(\"No chapters identified. Please check the PDF format.\")\n",
        "            return None\n",
        "\n",
        "        # Process each chapter\n",
        "        all_notes = {}\n",
        "        failed_chapters = []\n",
        "\n",
        "        if parallel and len(chapters) > 1:\n",
        "            # Process chapters in parallel with fewer workers to avoid rate limits\n",
        "            with ThreadPoolExecutor(max_workers=min(2, len(chapters))) as executor:\n",
        "                future_to_chapter = {\n",
        "                    executor.submit(self.process_chapter, title, text): title\n",
        "                    for title, text in chapters.items()\n",
        "                }\n",
        "\n",
        "                for future in tqdm(future_to_chapter, desc=\"Processing chapters\"):\n",
        "                    chapter_title = future_to_chapter[future]\n",
        "                    try:\n",
        "                        chapter_notes = future.result()\n",
        "\n",
        "                        # Save chapter notes\n",
        "                        txt_file = self.save_notes_as_text(chapter_notes, chapter_title, output_dir)\n",
        "                        json_file = self.save_notes_as_json(chapter_notes, chapter_title, output_dir)\n",
        "\n",
        "                        print(f\"✓ Saved notes for '{chapter_title}' to {txt_file} and {json_file}\")\n",
        "                        all_notes[chapter_title] = chapter_notes\n",
        "\n",
        "                        # Add delay between chapters\n",
        "                        time.sleep(3)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing chapter '{chapter_title}': {e}\")\n",
        "                        failed_chapters.append((chapter_title, chapters[chapter_title]))\n",
        "        else:\n",
        "            # Process chapters sequentially\n",
        "            for chapter_title, chapter_text in tqdm(chapters.items(), desc=\"Processing chapters\"):\n",
        "                try:\n",
        "                    chapter_notes = self.process_chapter(chapter_title, chapter_text)\n",
        "\n",
        "                    # Save chapter notes\n",
        "                    txt_file = self.save_notes_as_text(chapter_notes, chapter_title, output_dir)\n",
        "                    json_file = self.save_notes_as_json(chapter_notes, chapter_title, output_dir)\n",
        "\n",
        "                    print(f\"✓ Saved notes for '{chapter_title}' to {txt_file} and {json_file}\")\n",
        "                    all_notes[chapter_title] = chapter_notes\n",
        "\n",
        "                    # Add delay between chapters to avoid rate limits\n",
        "                    time.sleep(3)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chapter '{chapter_title}': {e}\")\n",
        "                    failed_chapters.append((chapter_title, chapter_text))\n",
        "\n",
        "        # Retry failed chapters after a longer delay\n",
        "        if failed_chapters:\n",
        "            print(f\"\\nRetrying {len(failed_chapters)} failed chapters after a delay...\")\n",
        "            time.sleep(60)  # Wait a full minute before retries\n",
        "\n",
        "            for chapter_title, chapter_text in tqdm(failed_chapters, desc=\"Retrying failed chapters\"):\n",
        "                try:\n",
        "                    print(f\"Retrying: {chapter_title}\")\n",
        "                    chapter_notes = self.process_chapter(chapter_title, chapter_text)\n",
        "\n",
        "                    # Save chapter notes\n",
        "                    txt_file = self.save_notes_as_text(chapter_notes, chapter_title, output_dir)\n",
        "                    json_file = self.save_notes_as_json(chapter_notes, chapter_title, output_dir)\n",
        "\n",
        "                    print(f\"✓ Successfully saved notes for '{chapter_title}' on retry\")\n",
        "                    all_notes[chapter_title] = chapter_notes\n",
        "\n",
        "                    # Longer delay between retries\n",
        "                    time.sleep(10)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Still failed to process chapter '{chapter_title}': {e}\")\n",
        "\n",
        "        # Save combined JSON with all notes\n",
        "        combined_json = os.path.join(output_dir, \"all_notes.json\")\n",
        "\n",
        "        # Create a structured representation of all notes\n",
        "        book_notes = {\n",
        "            \"book\": os.path.basename(pdf_path),\n",
        "            \"chapters\": []\n",
        "        }\n",
        "\n",
        "        for chapter_title, chapter_notes in all_notes.items():\n",
        "            safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "            chapter_data = {\n",
        "                \"title\": chapter_title,\n",
        "                \"id\": safe_chapter_title,\n",
        "                \"notes\": []\n",
        "            }\n",
        "\n",
        "            for i, note in enumerate(chapter_notes, 1):\n",
        "                note_id = f\"{safe_chapter_title}_note_{i}\"\n",
        "                note_with_id = {\n",
        "                    \"id\": note_id,\n",
        "                    \"title\": note['title'],\n",
        "                    \"content\": note['content']\n",
        "                }\n",
        "                chapter_data[\"notes\"].append(note_with_id)\n",
        "\n",
        "            book_notes[\"chapters\"].append(chapter_data)\n",
        "\n",
        "        with open(combined_json, 'w', encoding='utf-8') as f:\n",
        "            json.dump(book_notes, f, indent=2)\n",
        "\n",
        "        print(f\"✓ Combined notes saved to {combined_json}\")\n",
        "\n",
        "        return book_notes\n",
        "\n",
        "\n",
        "# Example usage in Google Colab\n",
        "def run_in_colab(pdf_file_path, api_key, notes_per_chapter=5, use_mixed_models=True):\n",
        "    \"\"\"\n",
        "    Run the evergreen notes generator in Google Colab\n",
        "\n",
        "    Args:\n",
        "        pdf_file_path (str): Path to the PDF file\n",
        "        api_key (str): OpenAI API key\n",
        "        notes_per_chapter (int): Number of notes to generate per chapter\n",
        "        use_mixed_models (bool): Whether to use mixed models (gpt-4o-mini for simpler tasks)\n",
        "    \"\"\"\n",
        "    output_dir = \"./evergreen_notes_output\"\n",
        "\n",
        "    # Model configuration\n",
        "    if use_mixed_models:\n",
        "        model_config = {\n",
        "            'title_generation': 'gpt-4o-mini',  # Titles can use the mini model\n",
        "            'title_selection': 'gpt-4o-mini',   # Selection can use the mini model\n",
        "            'content_generation': 'gpt-4o'      # Content needs the full model\n",
        "        }\n",
        "    else:\n",
        "        model_config = {\n",
        "            'title_generation': 'gpt-4o',\n",
        "            'title_selection': 'gpt-4o',\n",
        "            'content_generation': 'gpt-4o'\n",
        "        }\n",
        "\n",
        "    # Initialize the generator\n",
        "    generator = EvergreenNotesGenerator(\n",
        "        api_key=api_key,\n",
        "        model_config=model_config,\n",
        "        notes_per_chapter=notes_per_chapter\n",
        "    )\n",
        "\n",
        "    # Process the book\n",
        "    generator.process_book(pdf_file_path, output_dir, parallel=False)\n",
        "\n",
        "    # Zip the output directory for easy download\n",
        "    os.system(f\"zip -r evergreen_notes.zip {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "duecVcZF7qJ9",
        "outputId": "82cbabd0-1f7e-4a0c-e755-abb88c725b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your PDF book:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-15250e1f-e019-4e36-b252-846f6b399395\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-15250e1f-e019-4e36-b252-846f6b399395\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008).pdf to Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008) (2).pdf\n",
            "Enter your OpenAI API key: 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\n",
            "How many notes per chapter? (default: 3): 3\n",
            "Use GPT-4o-mini for title generation to save costs? (y/n, default: y): y\n",
            "Starting to process Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008) (2).pdf with 3 notes per chapter...\n",
            "Extracting text from PDF: Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008) (2).pdf\n",
            "Identifying chapters...\n",
            "Looking for table of contents...\n",
            "Searching for chapter patterns...\n",
            "Pattern '(?:^|\\n)(\\d+\\.\\s+[A-Z][^\\n]{5,})(?=\\n)' found 73 matches\n",
            "Pattern '(?:^|\\n)(?:CHAPTER|CH\\.|PART|SECTION)\\s+[\\dIVXLC]+(?:[:\\.\\s]+[^\\n]+)?(?=\\n)' found 4 matches\n",
            "Pattern '(?:^|\\n)(?:\\d+\\.?\\s+)(?:[A-Z][^\\n]{5,})(?=\\n)' found 74 matches\n",
            "Pattern '(?:^|\\n)(?:[IVX]+\\.?\\s+)(?:[A-Z][^\\n]{5,})(?=\\n)' found 16 matches\n",
            "Pattern '(?:^|\\n)(?:Prayer|Meditation|Reflection)(?:\\s+[\\dIVXLC]+)?(?:[:\\.\\s]+[^\\n]+)?(?=\\n)' found 29 matches\n",
            "Pattern '(?:^|\\n)([A-Z][A-Z\\s]{3,}[A-Z])(?=\\n)' found 174 matches\n",
            "Found 254 potential chapter headings\n",
            "Successfully extracted 95 chapters/sections\n",
            "Found 95 chapters.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:   0%|          | 0/95 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Scripture texts from the New and Old Testaments are taken from The Holy Bible Revised Standard\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Processing chapters:   0%|          | 0/95 [00:07<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-93aef51949f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Run the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting to process {pdf_filename} with {notes_per_chapter} notes per chapter...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mrun_in_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotes_per_chapter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotes_per_chapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mixed_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_mixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Create and download the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-45297935c3de>\u001b[0m in \u001b[0;36mrun_in_colab\u001b[0;34m(pdf_file_path, api_key, notes_per_chapter, use_mixed_models)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;31m# Process the book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;31m# Zip the output directory for easy download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-45297935c3de>\u001b[0m in \u001b[0;36mprocess_book\u001b[0;34m(self, pdf_path, output_dir, parallel)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mchapter_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_text\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing chapters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                     \u001b[0mchapter_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_chapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapter_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;31m# Save chapter notes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-45297935c3de>\u001b[0m in \u001b[0;36mprocess_chapter\u001b[0;34m(self, chapter_title, chapter_text)\u001b[0m\n\u001b[1;32m    462\u001b[0m                     \u001b[0mrelevant_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchapter_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_note_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 notes.append({\n",
            "\u001b[0;32m<ipython-input-12-45297935c3de>\u001b[0m in \u001b[0;36mgenerate_note_content\u001b[0;34m(self, chapter_text, title, chapter_title)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;31m# Make API call with rate limiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         response = self.rate_limited_api_call(\n\u001b[0m\u001b[1;32m    418\u001b[0m             messages=[\n\u001b[1;32m    419\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msystem_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-45297935c3de>\u001b[0m in \u001b[0;36mrate_limited_api_call\u001b[0;34m(self, messages, model, max_retries, initial_wait)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mretry_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 return self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    956\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Upload PDF file\n",
        "print(\"Please upload your PDF book:\")\n",
        "uploaded = files.upload()\n",
        "pdf_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Get OpenAI API key\n",
        "api_key = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "# Set number of notes per chapter\n",
        "notes_per_chapter = int(input(\"How many notes per chapter? (default: 3): \") or \"3\")\n",
        "\n",
        "# Ask if user wants to use mixed models\n",
        "use_mixed = input(\"Use GPT-4o-mini for title generation to save costs? (y/n, default: y): \").lower() != 'n'\n",
        "\n",
        "# Run the generator\n",
        "print(f\"Starting to process {pdf_filename} with {notes_per_chapter} notes per chapter...\")\n",
        "run_in_colab(pdf_filename, api_key, notes_per_chapter=notes_per_chapter, use_mixed_models=use_mixed)\n",
        "\n",
        "# Create and download the zip file\n",
        "from google.colab import files\n",
        "print(\"\\nProcessing complete! Preparing download...\")\n",
        "time.sleep(3)\n",
        "files.download(\"evergreen_notes.zip\")\n",
        "\n",
        "# Add a viewer for browsing notes in the notebook\n",
        "def view_notes():\n",
        "    import json\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    if not os.path.exists(\"./evergreen_notes_output/all_notes.json\"):\n",
        "        print(\"Notes haven't been generated yet or couldn't be found.\")\n",
        "        return\n",
        "\n",
        "    with open(\"./evergreen_notes_output/all_notes.json\", 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Create a simple HTML viewer\n",
        "    html = f\"\"\"\n",
        "    <h1>Notes for {data['book']}</h1>\n",
        "    <div style=\"max-width:800px\">\n",
        "    \"\"\"\n",
        "\n",
        "    for chapter in data['chapters']:\n",
        "        html += f\"\"\"\n",
        "        <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
        "            <h2>{chapter['title']}</h2>\n",
        "        \"\"\"\n",
        "\n",
        "        for note in chapter['notes']:\n",
        "            html += f\"\"\"\n",
        "            <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
        "                <h3>{note['title']}</h3>\n",
        "                <div style=\"white-space:pre-wrap\">{note['content']}</div>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        html += \"</div>\"\n",
        "\n",
        "    html += \"</div>\"\n",
        "    display(HTML(html))\n",
        "\n",
        "print(\"\\nWould you like to view the generated notes in the notebook?\")\n",
        "if input(\"View notes now? (y/n): \").lower() == 'y':\n",
        "    view_notes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6IiwUYRrePj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import PyPDF2\n",
        "import openai\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "class EvergreenNotesGenerator:\n",
        "    def __init__(self, api_key=None, model_config=None, notes_per_chapter=5):\n",
        "        \"\"\"\n",
        "        Initialize the Evergreen Notes Generator\n",
        "\n",
        "        Args:\n",
        "            api_key (str): OpenAI API key\n",
        "            model_config (dict): Model configuration with different models for different tasks\n",
        "            notes_per_chapter (int): Number of notes to generate per chapter\n",
        "        \"\"\"\n",
        "        if api_key:\n",
        "            self.client = openai.OpenAI(api_key=api_key)\n",
        "        else:\n",
        "            self.client = openai.OpenAI()\n",
        "\n",
        "        # Default model configuration (using GPT-4o for everything)\n",
        "        self.model_config = {\n",
        "            'title_generation': 'gpt-4o-mini',  # Titles can use the mini model\n",
        "            'title_selection': 'gpt-4o-mini',   # Selection can use the mini model\n",
        "            'content_generation': 'gpt-4o'      # Content needs the full model\n",
        "        }\n",
        "\n",
        "        # Override with custom config if provided\n",
        "        if model_config:\n",
        "            self.model_config.update(model_config)\n",
        "\n",
        "        self.notes_per_chapter = notes_per_chapter\n",
        "\n",
        "        # Limiting context sizes based on models\n",
        "        self.max_chunk_chars = 300000  # For GPT-4o (~75k tokens)\n",
        "        self.mini_max_chunk_chars = 200000  # For GPT-4o-mini (smaller to be safe)\n",
        "\n",
        "    def rate_limited_api_call(self, messages, model, max_retries=8, initial_wait=2):\n",
        "        \"\"\"Make an API call with automatic retry on rate limit errors\"\"\"\n",
        "        retry_count = 0\n",
        "        wait_time = initial_wait\n",
        "\n",
        "        while retry_count < max_retries:\n",
        "            try:\n",
        "                return self.client.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=messages\n",
        "                )\n",
        "            except Exception as e:\n",
        "                error_str = str(e)\n",
        "                if \"rate_limit_exceeded\" in error_str:\n",
        "                    # Extract wait time from error message if possible\n",
        "                    wait_match = re.search(r'try again in (\\d+\\.\\d+)s', error_str)\n",
        "                    if wait_match:\n",
        "                        wait_time = float(wait_match.group(1)) * 1.5  # Add a 50% buffer\n",
        "\n",
        "                    # Add some jitter to avoid thundering herd problem\n",
        "                    jitter = random.uniform(0.5, 1.5)\n",
        "                    adjusted_wait = wait_time * jitter\n",
        "\n",
        "                    print(f\"Rate limit hit for {model}. Waiting {adjusted_wait:.2f}s before retry {retry_count+1}/{max_retries}\")\n",
        "                    time.sleep(adjusted_wait)\n",
        "                    wait_time *= 1.5  # Increase wait time for next attempt\n",
        "                    retry_count += 1\n",
        "                else:\n",
        "                    # Non-rate-limit error, re-raise\n",
        "                    raise\n",
        "\n",
        "        # If we get here, we've exhausted our retries\n",
        "        raise Exception(f\"Failed after {max_retries} attempts due to rate limiting\")\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        \"\"\"Extract full text from PDF file\"\"\"\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                try:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not extract text from a page: {e}\")\n",
        "        return text\n",
        "\n",
        "    def extract_chapters(self, full_text):\n",
        "        \"\"\"Main chapter extraction method - uses TOC method first, then fallback if needed\"\"\"\n",
        "        try:\n",
        "            # Try the TOC-based method first\n",
        "            chapters = self.extract_chapters_by_toc(full_text)\n",
        "\n",
        "            # If we got a reasonable number of chapters, return them\n",
        "            if chapters and len(chapters) >= 3 and len(chapters) <= 40:\n",
        "                return chapters\n",
        "\n",
        "            # Otherwise, use the fallback method\n",
        "            return self.extract_chapters_fallback(full_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chapter extraction: {e}\")\n",
        "            return self.extract_chapters_fallback(full_text)\n",
        "\n",
        "    def extract_chapters_by_toc(self, full_text):\n",
        "        \"\"\"Extract chapters by identifying the table of contents using LLM\"\"\"\n",
        "        # Import here to avoid installation issues\n",
        "        try:\n",
        "            from fuzzywuzzy import fuzz\n",
        "        except ImportError:\n",
        "            print(\"fuzzywuzzy not installed. Installing...\")\n",
        "            os.system(\"pip install fuzzywuzzy python-Levenshtein\")\n",
        "            from fuzzywuzzy import fuzz\n",
        "\n",
        "        # Step 1: Extract initial portion of text (first 15% to capture TOC)\n",
        "        sample_size = min(len(full_text) // 6, 30000)  # First ~15% or max 30k chars\n",
        "        text_sample = full_text[:sample_size]\n",
        "\n",
        "        # Step 2: Use LLM to identify the TOC and extract chapter titles\n",
        "        system_prompt = (\n",
        "            \"You are a specialized AI trained to identify and extract Tables of Contents from books. \"\n",
        "            \"Your only task is to find chapter titles from the text provided.\"\n",
        "        )\n",
        "\n",
        "        user_prompt = (\n",
        "            \"Below is the beginning portion of a book. Your task is to:\\n\"\n",
        "            \"1. Identify if there is a Table of Contents\\n\"\n",
        "            \"2. Extract ONLY the main chapter titles as they appear (not subheadings or sections)\\n\"\n",
        "            \"3. Return them as a numbered list\\n\\n\"\n",
        "            \"If you don't find a clear Table of Contents, look for chapter headings in the text itself.\\n\"\n",
        "            \"Return ONLY the numbered list of chapter titles, nothing else.\\n\\n\"\n",
        "            f\"TEXT:\\n{text_sample}\"\n",
        "        )\n",
        "\n",
        "        # Use a cheaper model for this task\n",
        "        print(\"Identifying table of contents...\")\n",
        "        response = self.rate_limited_api_call(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=\"gpt-3.5-turbo\"  # Cheaper model is sufficient for this task\n",
        "        )\n",
        "\n",
        "        toc_content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Parse the LLM's response into a list of chapter titles\n",
        "        chapter_titles = []\n",
        "        for line in toc_content.splitlines():\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Remove numbering if present\n",
        "            match = re.match(r'^\\d+\\.?\\s*(.*)', line)\n",
        "            if match:\n",
        "                title = match.group(1).strip()\n",
        "                if title and len(title) > 3:  # Avoid empty or very short titles\n",
        "                    chapter_titles.append(title)\n",
        "\n",
        "        print(f\"Found {len(chapter_titles)} potential chapter titles\")\n",
        "\n",
        "        if len(chapter_titles) < 3:\n",
        "            print(\"Too few chapter titles found, trying alternative approach...\")\n",
        "            # If we didn't find enough chapter titles, try a different prompt\n",
        "            user_prompt_alt = (\n",
        "                \"You are examining the beginning of a book. Please identify the main chapter titles or major sections.\\n\"\n",
        "                \"Look carefully for any patterns that indicate chapters, such as 'Chapter 1', numbered sections, or distinct headings.\\n\"\n",
        "                \"Return ONLY a numbered list of the major divisions you find, with no additional text.\\n\\n\"\n",
        "                f\"TEXT:\\n{text_sample}\"\n",
        "            )\n",
        "\n",
        "            response = self.rate_limited_api_call(\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt_alt}\n",
        "                ],\n",
        "                model=\"gpt-3.5-turbo\"\n",
        "            )\n",
        "\n",
        "            toc_content = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Parse the alternative response\n",
        "            chapter_titles = []\n",
        "            for line in toc_content.splitlines():\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                match = re.match(r'^\\d+\\.?\\s*(.*)', line)\n",
        "                if match:\n",
        "                    title = match.group(1).strip()\n",
        "                    if title and len(title) > 3:\n",
        "                        chapter_titles.append(title)\n",
        "\n",
        "            print(f\"Second attempt found {len(chapter_titles)} potential chapter titles\")\n",
        "\n",
        "        # If we still don't have enough chapters, fall back to the original method\n",
        "        if len(chapter_titles) < 3:\n",
        "            print(\"Failed to identify sufficient chapter titles, using fallback method\")\n",
        "            return self.extract_chapters_fallback(full_text)\n",
        "\n",
        "        # Step 3: Use fuzzy matching to find chapter positions in the full text\n",
        "        print(\"Locating chapters in the full text...\")\n",
        "\n",
        "        # Prepare to collect chapter positions\n",
        "        chapter_positions = []\n",
        "\n",
        "        # Try different matching strategies:\n",
        "        # 1. First try exact matches with context\n",
        "        for title in chapter_titles:\n",
        "            # Consider various formatting possibilities\n",
        "            patterns = [\n",
        "                # Exact match with newlines before/after\n",
        "                rf'\\n\\s*{re.escape(title)}\\s*\\n',\n",
        "                # Chapter X: Title format\n",
        "                rf'\\n\\s*(?:Chapter|CHAPTER|Part|PART|Section|SECTION)(?:\\s+[\\dIVXLC]+)?(?:[\\:\\.]?\\s+){re.escape(title)}\\s*\\n',\n",
        "                # X. Title format\n",
        "                rf'\\n\\s*(?:\\d+\\.|\\d+|[IVX]+\\.|\\[.+?\\])\\s+{re.escape(title)}\\s*\\n'\n",
        "            ]\n",
        "\n",
        "            for pattern in patterns:\n",
        "                matches = list(re.finditer(pattern, full_text, re.MULTILINE | re.IGNORECASE))\n",
        "                if matches:\n",
        "                    print(f\"Found '{title}' with pattern: {pattern}\")\n",
        "                    pos = matches[0].start()  # Take the first occurrence\n",
        "                    chapter_positions.append((pos, title))\n",
        "                    break\n",
        "\n",
        "        # 2. If we don't have enough matches, try fuzzy matching\n",
        "        if len(chapter_positions) < len(chapter_titles) // 2:\n",
        "            print(\"Using fuzzy matching for remaining chapters...\")\n",
        "\n",
        "            # For titles we haven't found yet\n",
        "            unfound_titles = [t for t in chapter_titles if not any(t == title for _, title in chapter_positions)]\n",
        "\n",
        "            # Search for each title using fuzzy matching\n",
        "            for title in unfound_titles:\n",
        "                best_match = None\n",
        "                best_score = 0\n",
        "                best_pos = -1\n",
        "\n",
        "                # Look for potential matches in the text\n",
        "                # Simplified approach: check the beginning of paragraphs\n",
        "                paragraphs = re.split(r'\\n\\s*\\n', full_text)\n",
        "                for i, para in enumerate(paragraphs):\n",
        "                    # Check first line of paragraph\n",
        "                    first_line = para.split('\\n')[0] if para else \"\"\n",
        "                    if len(first_line) > 3:\n",
        "                        score = fuzz.ratio(title.lower(), first_line.lower())\n",
        "\n",
        "                        # If the score is good enough and better than previous matches\n",
        "                        if score > 75 and score > best_score:\n",
        "                            best_score = score\n",
        "                            best_match = first_line\n",
        "                            # Calculate approximate position in full text\n",
        "                            best_pos = full_text.find(para)\n",
        "\n",
        "                if best_pos >= 0:\n",
        "                    print(f\"Fuzzy matched '{title}' to '{best_match}' (score: {best_score})\")\n",
        "                    chapter_positions.append((best_pos, title))\n",
        "\n",
        "        # If we still don't have enough matches, fall back\n",
        "        if len(chapter_positions) < 3:\n",
        "            print(f\"Only found {len(chapter_positions)} chapters via matching, using fallback\")\n",
        "            return self.extract_chapters_fallback(full_text)\n",
        "\n",
        "        # Sort chapters by position\n",
        "        chapter_positions.sort()\n",
        "\n",
        "        # Extract chapter content\n",
        "        chapters = {}\n",
        "        for i, (pos, title) in enumerate(chapter_positions):\n",
        "            # Find end position (start of next chapter or end of text)\n",
        "            if i < len(chapter_positions) - 1:\n",
        "                end_pos = chapter_positions[i + 1][0]\n",
        "            else:\n",
        "                end_pos = len(full_text)\n",
        "\n",
        "            # Extract chapter content\n",
        "            content = full_text[pos:end_pos].strip()\n",
        "\n",
        "            # Skip the title in the content\n",
        "            first_newline = content.find('\\n')\n",
        "            if first_newline > 0:\n",
        "                content = content[first_newline:].strip()\n",
        "\n",
        "            # Only add chapters with sufficient content\n",
        "            if len(content) > 500:\n",
        "                chapters[title] = content\n",
        "\n",
        "        print(f\"Successfully extracted {len(chapters)} chapters via TOC method\")\n",
        "\n",
        "        return chapters\n",
        "\n",
        "    def extract_chapters_fallback(self, full_text):\n",
        "        \"\"\"Fallback method to extract chapters when TOC method fails\"\"\"\n",
        "        print(\"Using fallback chapter extraction method...\")\n",
        "\n",
        "        # Create 10-15 roughly equal sections\n",
        "        chapters = {}\n",
        "        target_sections = min(15, max(8, len(full_text) // 15000))\n",
        "        section_length = len(full_text) // target_sections\n",
        "\n",
        "        for i in range(0, len(full_text), section_length):\n",
        "            section_text = full_text[i:i + section_length].strip()\n",
        "            if len(section_text) > 500:  # Only include substantive sections\n",
        "                chapters[f\"Section {i//section_length + 1}\"] = section_text\n",
        "\n",
        "        print(f\"Created {len(chapters)} equal-length sections\")\n",
        "        return chapters\n",
        "\n",
        "    def generate_titles(self, chapter_text, chapter_title):\n",
        "        \"\"\"Generate potential evergreen note titles for a chapter\"\"\"\n",
        "        # Use the mini model for title generation\n",
        "        model = self.model_config['title_generation']\n",
        "\n",
        "        # Prepare system prompt with detailed guidance\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to generate potential titles for evergreen notes based on the chapter provided. \"\n",
        "            \"Evergreen note titles should: \"\n",
        "            \"1. Be complete sentences that express a clear, specific claim or insight. \"\n",
        "            \"2. Convey enough detail to understand the core idea without additional context. \"\n",
        "            \"3. Use clear, precise language and active verbs. \"\n",
        "            \"4. Focus on the most important and insightful ideas from the text. \"\n",
        "            \"5. Be framed positively and constructively. \"\n",
        "            \"6. Avoid vague, general statements - be specific and concrete. \"\n",
        "            \"Generate titles that capture the most significant concepts, arguments, frameworks, and insights from the chapter.\"\n",
        "        )\n",
        "\n",
        "        # Limit text size based on mini model\n",
        "        max_size = self.mini_max_chunk_chars\n",
        "        if len(chapter_text) > max_size:\n",
        "            # Use first and last portions with an indication of truncation\n",
        "            portion_size = max_size // 2 - 50\n",
        "            chapter_text = chapter_text[:portion_size] + \"\\n[...TEXT TRUNCATED...]\\n\" + chapter_text[-portion_size:]\n",
        "\n",
        "        # Prepare user prompt\n",
        "        user_prompt = (\n",
        "            f\"Generate {self.notes_per_chapter * 2} potential evergreen note titles based on this chapter: \"\n",
        "            f\"CHAPTER TITLE: {chapter_title} \"\n",
        "            f\"CHAPTER CONTENT: {chapter_text} \"\n",
        "            f\"Return ONLY the titles, one per line, numbered.\"\n",
        "        )\n",
        "\n",
        "        # Make API call with rate limiting\n",
        "        response = self.rate_limited_api_call(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        # Extract titles from response\n",
        "        content = response.choices[0].message.content\n",
        "        titles = []\n",
        "\n",
        "        # Parse titles, handling both numbered and unnumbered formats\n",
        "        for line in content.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Remove numbering if present\n",
        "            cleaned_line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
        "            if cleaned_line:\n",
        "                titles.append(cleaned_line)\n",
        "\n",
        "        return titles\n",
        "\n",
        "    def select_best_titles(self, chapter_text, titles, chapter_title):\n",
        "        \"\"\"Select the best titles based on relevance and quality\"\"\"\n",
        "        # Use the mini model for title selection\n",
        "        model = self.model_config['title_selection']\n",
        "\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to select the most valuable and insightful evergreen note titles from a list of candidates. \"\n",
        "            \"The best evergreen note titles should: \"\n",
        "            \"1. Capture significant insights or claims from the text. \"\n",
        "            \"2. Be specific, clear, and well-formulated. \"\n",
        "            \"3. Focus on the most important concepts in the chapter. \"\n",
        "            \"4. Cover diverse aspects of the chapter content (avoid redundancy). \"\n",
        "            \"5. Prioritize non-obvious, thought-provoking ideas.\"\n",
        "        )\n",
        "\n",
        "        # Format the titles with numbers\n",
        "        formatted_titles = \"\\n\".join([f\"{i+1}. {title}\" for i, title in enumerate(titles)])\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"From the following list of potential evergreen note titles for the chapter '{chapter_title}', \"\n",
        "            f\"select exactly {self.notes_per_chapter} titles that best meet the criteria in my instructions. \"\n",
        "            f\"Candidate titles:\\n{formatted_titles}\\n\\n\"\n",
        "            f\"Return ONLY the numbers of the selected titles in the format: 1, 5, 8, 10, 12 \"\n",
        "            f\"Do not include any explanations or additional text.\"\n",
        "        )\n",
        "\n",
        "        # Make API call with rate limiting\n",
        "        response = self.rate_limited_api_call(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        # Parse selected title numbers\n",
        "        content = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Extract numbers, handling various formats\n",
        "        selected_indices = []\n",
        "        for num in re.findall(r'\\d+', content):\n",
        "            idx = int(num) - 1  # Convert to 0-based index\n",
        "            if 0 <= idx < len(titles):\n",
        "                selected_indices.append(idx)\n",
        "\n",
        "        # Ensure we have the correct number of titles\n",
        "        selected_indices = selected_indices[:self.notes_per_chapter]\n",
        "\n",
        "        # Get the selected titles\n",
        "        selected_titles = [titles[idx] for idx in selected_indices]\n",
        "\n",
        "        # If we don't have enough titles, add more from the original list\n",
        "        if len(selected_titles) < self.notes_per_chapter:\n",
        "            for title in titles:\n",
        "                if title not in selected_titles:\n",
        "                    selected_titles.append(title)\n",
        "                    if len(selected_titles) >= self.notes_per_chapter:\n",
        "                        break\n",
        "\n",
        "        return selected_titles\n",
        "\n",
        "    def generate_note_content(self, chapter_text, title, chapter_title):\n",
        "        \"\"\"Generate content for a single evergreen note\"\"\"\n",
        "        # Use the full model for content generation\n",
        "        model = self.model_config['content_generation']\n",
        "\n",
        "        system_prompt = (\n",
        "            \"You are an expert at creating insightful evergreen notes from academic and non-fiction texts. \"\n",
        "            \"Your task is to write the content for an evergreen note based on the provided title and chapter. \"\n",
        "            \"Your evergreen note should: \"\n",
        "            \"1. Thoroughly explore the specific idea in the title (200-300 words). \"\n",
        "            \"2. Include interpretations, critical thinking, and broader implications. \"\n",
        "            \"3. Be structured clearly with a logical flow of ideas. \"\n",
        "            \"4. Use precise language and concrete examples. \"\n",
        "            \"5. Go beyond summarizing - add insight and connections. \"\n",
        "            \"6. Maintain academic rigor while being accessible. \"\n",
        "            \"7. Look for opportunities to reference related concepts (for interlinking). \"\n",
        "            \"The note should function as a standalone 'API' for this knowledge that can be understood without the original text.\"\n",
        "        )\n",
        "\n",
        "        # Extract relevant parts if chapter is too large\n",
        "        if len(chapter_text) > self.max_chunk_chars:\n",
        "            relevant_text = self.extract_relevant_sections(chapter_text, title)\n",
        "        else:\n",
        "            relevant_text = chapter_text\n",
        "\n",
        "        user_prompt = (\n",
        "            f\"Write content for an evergreen note with the following title, based on this chapter: \"\n",
        "            f\"CHAPTER: {chapter_title} \"\n",
        "            f\"NOTE TITLE: {title} \"\n",
        "            f\"CHAPTER CONTENT: {relevant_text} \"\n",
        "            f\"Generate a 200-300 word evergreen note that thoroughly explores this idea.\"\n",
        "        )\n",
        "\n",
        "        # Make API call with rate limiting\n",
        "        response = self.rate_limited_api_call(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "        # Extract content\n",
        "        content = response.choices[0].message.content\n",
        "        return content\n",
        "\n",
        "    def process_chapter(self, chapter_title, chapter_text):\n",
        "        \"\"\"Process a single chapter to generate evergreen notes\"\"\"\n",
        "        try:\n",
        "            print(f\"Processing: {chapter_title}\")\n",
        "\n",
        "            # Handle large chapters by chunking if necessary\n",
        "            if len(chapter_text) > self.mini_max_chunk_chars:\n",
        "                # For title generation, use the first 1/3 and last 1/3 of the chapter\n",
        "                third = len(chapter_text) // 3\n",
        "                title_text = chapter_text[:third] + \"\\n...\\n\" + chapter_text[-third:]\n",
        "            else:\n",
        "                title_text = chapter_text\n",
        "\n",
        "            # Generate potential titles\n",
        "            titles = self.generate_titles(title_text, chapter_title)\n",
        "\n",
        "            # Add delay after title generation to avoid rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "            # Select best titles\n",
        "            selected_titles = self.select_best_titles(title_text, titles, chapter_title)\n",
        "\n",
        "            # Add delay after title selection to avoid rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "            # Generate content for each selected title\n",
        "            notes = []\n",
        "\n",
        "            for title in tqdm(selected_titles, desc=\"Generating notes\", leave=False):\n",
        "                # For content generation, extract relevant parts of the chapter if it's too large\n",
        "                if len(chapter_text) > self.max_chunk_chars:\n",
        "                    relevant_text = self.extract_relevant_sections(chapter_text, title)\n",
        "                else:\n",
        "                    relevant_text = chapter_text\n",
        "\n",
        "                content = self.generate_note_content(relevant_text, title, chapter_title)\n",
        "\n",
        "                notes.append({\n",
        "                    \"title\": title,\n",
        "                    \"content\": content\n",
        "                })\n",
        "\n",
        "                # Add delay between note generation to avoid rate limits\n",
        "                time.sleep(2)\n",
        "\n",
        "            return notes\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in process_chapter for '{chapter_title}': {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_relevant_sections(self, chapter_text, title):\n",
        "        \"\"\"Extract the most relevant parts of a long chapter for a specific title\"\"\"\n",
        "        # Split chapter into paragraphs\n",
        "        paragraphs = [p for p in chapter_text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        # Conservative max size\n",
        "        max_chars = self.max_chunk_chars // 2\n",
        "\n",
        "        if len(chapter_text) <= max_chars:\n",
        "            return chapter_text\n",
        "\n",
        "        # Extract important keywords from title\n",
        "        title_words = set(re.findall(r'\\b\\w{4,}\\b', title.lower()))\n",
        "        # Remove common stop words\n",
        "        stop_words = {'this', 'that', 'with', 'from', 'have', 'they', 'what', 'when', 'where', 'which', 'their'}\n",
        "        title_words = {w for w in title_words if w not in stop_words}\n",
        "\n",
        "        # Score paragraphs based on relevance to title\n",
        "        scored_paragraphs = []\n",
        "\n",
        "        for i, para in enumerate(paragraphs):\n",
        "            score = 0\n",
        "            para_lower = para.lower()\n",
        "\n",
        "            # Score based on keyword matches\n",
        "            for word in title_words:\n",
        "                # Full word matches\n",
        "                score += para_lower.count(f' {word} ') * 3\n",
        "                # Partial matches\n",
        "                score += para_lower.count(word) * 2\n",
        "\n",
        "            # Boost score for position in chapter\n",
        "            # First few paragraphs get a boost for context\n",
        "            if i < 3:\n",
        "                score += 5\n",
        "            # Introduction paragraphs get a boost\n",
        "            elif i < len(paragraphs) * 0.1:\n",
        "                score += 3\n",
        "            # Conclusion paragraphs get a boost\n",
        "            elif i > len(paragraphs) * 0.9:\n",
        "                score += 2\n",
        "\n",
        "            # Length factor (prefer substantial paragraphs)\n",
        "            if 100 <= len(para) <= 1000:\n",
        "                score += 1\n",
        "\n",
        "            scored_paragraphs.append((score, i, para))\n",
        "\n",
        "        # Sort by score descending\n",
        "        scored_paragraphs.sort(reverse=True)\n",
        "\n",
        "        # Get introduction (first few paragraphs)\n",
        "        intro_size = min(3, len(paragraphs))\n",
        "        intro = '\\n\\n'.join(paragraphs[:intro_size])\n",
        "\n",
        "        # Get top-scoring paragraphs\n",
        "        selected = []\n",
        "        current_size = len(intro)\n",
        "\n",
        "        for score, i, para in scored_paragraphs:\n",
        "            # Skip paragraphs already in intro\n",
        "            if i < intro_size:\n",
        "                continue\n",
        "\n",
        "            # Stop if we've reached the size limit\n",
        "            if current_size + len(para) > max_chars:\n",
        "                break\n",
        "\n",
        "            # Add paragraph\n",
        "            selected.append((i, para))\n",
        "            current_size += len(para)\n",
        "\n",
        "        # Sort selected paragraphs by original position\n",
        "        selected.sort()\n",
        "\n",
        "        # Combine intro with selected paragraphs\n",
        "        result = intro + '\\n\\n' + '\\n\\n'.join(para for _, para in selected)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def save_notes_as_text(self, notes, chapter_title, output_dir):\n",
        "        \"\"\"Save notes in plain text format\"\"\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "        filename = os.path.join(output_dir, f\"{safe_chapter_title}_notes.txt\")\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"Evergreen Notes for: {chapter_title}\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "            for i, note in enumerate(notes, 1):\n",
        "                f.write(f\"Note {i}: {note['title']}\\n\")\n",
        "                f.write(\"-\" * 80 + \"\\n\")\n",
        "                f.write(note['content'] + \"\\n\\n\")\n",
        "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def save_notes_as_json(self, notes, chapter_title, output_dir):\n",
        "        \"\"\"Save notes in JSON format\"\"\"\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "        filename = os.path.join(output_dir, f\"{safe_chapter_title}_notes.json\")\n",
        "\n",
        "        # Add IDs to notes for reference\n",
        "        numbered_notes = []\n",
        "        for i, note in enumerate(notes, 1):\n",
        "            note_with_id = {\n",
        "                \"id\": f\"{safe_chapter_title}_note_{i}\",\n",
        "                \"title\": note['title'],\n",
        "                \"content\": note['content']\n",
        "            }\n",
        "            numbered_notes.append(note_with_id)\n",
        "\n",
        "        data = {\n",
        "            \"chapter\": chapter_title,\n",
        "            \"notes\": numbered_notes\n",
        "        }\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def process_book(self, pdf_path, output_dir, parallel=False):\n",
        "        \"\"\"Process the entire book to generate evergreen notes\"\"\"\n",
        "        print(f\"Extracting text from PDF: {pdf_path}\")\n",
        "        full_text = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        print(\"Identifying chapters...\")\n",
        "        chapters = self.extract_chapters(full_text)\n",
        "        print(f\"Found {len(chapters)} chapters.\")\n",
        "\n",
        "        if len(chapters) == 0:\n",
        "            print(\"No chapters identified. Please check the PDF format.\")\n",
        "            return None\n",
        "\n",
        "        # Process each chapter\n",
        "        all_notes = {}\n",
        "        failed_chapters = []\n",
        "\n",
        "        if parallel and len(chapters) > 1:\n",
        "            # Process chapters in parallel with fewer workers to avoid rate limits\n",
        "            with ThreadPoolExecutor(max_workers=min(2, len(chapters))) as executor:\n",
        "                future_to_chapter = {\n",
        "                    executor.submit(self.process_chapter, title, text): title\n",
        "                    for title, text in chapters.items()\n",
        "                }\n",
        "\n",
        "                for future in tqdm(future_to_chapter, desc=\"Processing chapters\"):\n",
        "                    chapter_title = future_to_chapter[future]\n",
        "                    try:\n",
        "                        chapter_notes = future.result()\n",
        "\n",
        "                        # Save chapter notes\n",
        "                        txt_file = self.save_notes_as_text(chapter_notes, chapter_title, output_dir)\n",
        "                        json_file = self.save_notes_as_json(chapter_notes, chapter_title, output_dir)\n",
        "\n",
        "                        print(f\"✓ Saved notes for '{chapter_title}' to {txt_file} and {json_file}\")\n",
        "                        all_notes[chapter_title] = chapter_notes\n",
        "\n",
        "                        # Add delay between chapters\n",
        "                        time.sleep(3)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing chapter '{chapter_title}': {e}\")\n",
        "                        failed_chapters.append((chapter_title, chapters[chapter_title]))\n",
        "        else:\n",
        "            # Process chapters sequentially\n",
        "            for chapter_title, chapter_text in tqdm(chapters.items(), desc=\"Processing chapters\"):\n",
        "                try:\n",
        "                    chapter_notes = self.process_chapter(chapter_title, chapter_text)\n",
        "\n",
        "                    # Save chapter notes\n",
        "                    txt_file = self.save_notes_as_text(chapter_notes, chapter_title, output_dir)\n",
        "                    json_file = self.save_notes_as_json(chapter_notes, chapter_title, output_dir)\n",
        "\n",
        "                    print(f\"✓ Saved notes for '{chapter_title}' to {txt_file} and {json_file}\")\n",
        "                    all_notes[chapter_title] = chapter_notes\n",
        "\n",
        "                    # Add delay between chapters to avoid rate limits\n",
        "                    time.sleep(3)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing chapter '{chapter_title}': {e}\")\n",
        "                    failed_chapters.append((chapter_title, chapter_text))\n",
        "\n",
        "\n",
        "        # Retry failed chapters after a longer delay\n",
        "            if failed_chapters:\n",
        "                print(f\"\\nRetrying {len(failed_chapters)} failed chapters after a delay...\")\n",
        "                time.sleep(60)  # Wait a full minute before retries\n",
        "\n",
        "                for chapter_title, chapter_text in tqdm(failed_chapters, desc=\"Retrying failed chapters\"):\n",
        "                    try:\n",
        "                        print(f\"Retrying: {chapter_title}\")\n",
        "                        chapter_notes = self.process_chapter(chapter_title, chapter_text)\n",
        "\n",
        "                        # Save chapter notes\n",
        "                        txt_file = self.save_notes_as_text(chapter_notes, chapter_title, output_dir)\n",
        "                        json_file = self.save_notes_as_json(chapter_notes, chapter_title, output_dir)\n",
        "\n",
        "                        print(f\"✓ Successfully saved notes for '{chapter_title}' on retry\")\n",
        "                        all_notes[chapter_title] = chapter_notes\n",
        "\n",
        "                        # Longer delay between retries\n",
        "                        time.sleep(10)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Still failed to process chapter '{chapter_title}': {e}\")\n",
        "\n",
        "            # Save combined JSON with all notes\n",
        "            combined_json = os.path.join(output_dir, \"all_notes.json\")\n",
        "\n",
        "            # Create a structured representation of all notes\n",
        "            book_notes = {\n",
        "                \"book\": os.path.basename(pdf_path),\n",
        "                \"chapters\": []\n",
        "            }\n",
        "\n",
        "            for chapter_title, chapter_notes in all_notes.items():\n",
        "                safe_chapter_title = re.sub(r'[^\\w\\s-]', '', chapter_title).strip().replace(' ', '_')\n",
        "                chapter_data = {\n",
        "                    \"title\": chapter_title,\n",
        "                    \"id\": safe_chapter_title,\n",
        "                    \"notes\": []\n",
        "                }\n",
        "\n",
        "                for i, note in enumerate(chapter_notes, 1):\n",
        "                    note_id = f\"{safe_chapter_title}_note_{i}\"\n",
        "                    note_with_id = {\n",
        "                        \"id\": note_id,\n",
        "                        \"title\": note['title'],\n",
        "                        \"content\": note['content']\n",
        "                    }\n",
        "                    chapter_data[\"notes\"].append(note_with_id)\n",
        "\n",
        "                book_notes[\"chapters\"].append(chapter_data)\n",
        "\n",
        "            with open(combined_json, 'w', encoding='utf-8') as f:\n",
        "                json.dump(book_notes, f, indent=2)\n",
        "\n",
        "            print(f\"✓ Combined notes saved to {combined_json}\")\n",
        "\n",
        "            return book_notes\n",
        "\n",
        "# Function to run in Google Colab\n",
        "def run_in_colab(pdf_file_path, api_key, notes_per_chapter=5, use_mixed_models=True):\n",
        "    \"\"\"\n",
        "    Run the evergreen notes generator in Google Colab\n",
        "\n",
        "    Args:\n",
        "        pdf_file_path (str): Path to the PDF file\n",
        "        api_key (str): OpenAI API key\n",
        "        notes_per_chapter (int): Number of notes to generate per chapter\n",
        "        use_mixed_models (bool): Whether to use mixed models (gpt-4o-mini for simpler tasks)\n",
        "    \"\"\"\n",
        "    output_dir = \"./evergreen_notes_output\"\n",
        "\n",
        "    # Model configuration\n",
        "    if use_mixed_models:\n",
        "        model_config = {\n",
        "            'title_generation': 'gpt-4o-mini',  # Titles can use the mini model\n",
        "            'title_selection': 'gpt-4o-mini',   # Selection can use the mini model\n",
        "            'content_generation': 'gpt-4o'      # Content needs the full model\n",
        "        }\n",
        "    else:\n",
        "        model_config = {\n",
        "            'title_generation': 'gpt-4o',\n",
        "            'title_selection': 'gpt-4o',\n",
        "            'content_generation': 'gpt-4o'\n",
        "        }\n",
        "\n",
        "    # Initialize the generator\n",
        "    generator = EvergreenNotesGenerator(\n",
        "        api_key=api_key,\n",
        "        model_config=model_config,\n",
        "        notes_per_chapter=notes_per_chapter\n",
        "    )\n",
        "\n",
        "    # Process the book\n",
        "    generator.process_book(pdf_file_path, output_dir, parallel=False)\n",
        "\n",
        "    # Zip the output directory for easy download\n",
        "    os.system(f\"zip -r evergreen_notes.zip {output_dir}\")\n",
        "\n",
        "    print(\"Processing complete! You can now download the evergreen_notes.zip file.\")\n",
        "\n",
        "    # For Google Colab, download the zip file\n",
        "    from google.colab import files\n",
        "    files.download(\"evergreen_notes.zip\")\n",
        "\n",
        "    # Add viewer functionality\n",
        "    def view_notes():\n",
        "        import json\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        if not os.path.exists(f\"{output_dir}/all_notes.json\"):\n",
        "            print(\"Notes haven't been generated yet or couldn't be found.\")\n",
        "            return\n",
        "\n",
        "        with open(f\"{output_dir}/all_notes.json\", 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Create a simple HTML viewer\n",
        "        html = f\"\"\"\n",
        "        <h1>Notes for {data['book']}</h1>\n",
        "        <div style=\"max-width:800px\">\n",
        "        \"\"\"\n",
        "\n",
        "        for chapter in data['chapters']:\n",
        "            html += f\"\"\"\n",
        "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
        "                <h2>{chapter['title']}</h2>\n",
        "            \"\"\"\n",
        "\n",
        "            for note in chapter['notes']:\n",
        "                html += f\"\"\"\n",
        "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
        "                    <h3>{note['title']}</h3>\n",
        "                    <div style=\"white-space:pre-wrap\">{note['content']}</div>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "\n",
        "            html += \"</div>\"\n",
        "\n",
        "        html += \"</div>\"\n",
        "        display(HTML(html))\n",
        "\n",
        "    # Ask if user wants to view the notes\n",
        "    print(\"\\nWould you like to view the generated notes in the notebook?\")\n",
        "    if input(\"View notes now? (y/n): \").lower() == 'y':\n",
        "        view_notes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TzzHMdB3r3C0",
        "outputId": "5571f15e-56a7-4be9-87c1-660f97114125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your PDF book:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f53c020a-40e0-480d-8424-097704478369\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f53c020a-40e0-480d-8424-097704478369\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008).pdf to Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008).pdf\n",
            "Enter your OpenAI API key: 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\n",
            "How many notes per chapter? (default: 3): 3\n",
            "Use GPT-4o-mini for title generation to save costs? (y/n, default: y): y\n",
            "Starting to process Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008).pdf with 3 notes per chapter...\n",
            "Extracting text from PDF: Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008).pdf\n",
            "Identifying chapters...\n",
            "Identifying table of contents...\n",
            "Found 5 potential chapter titles\n",
            "Locating chapters in the full text...\n",
            "Found 'Mental Prayer is Not a Technique But a Grace' with pattern: \\n\\s*(?:\\d+\\.|\\d+|[IVX]+\\.|\\[.+?\\])\\s+Mental\\ Prayer\\ is\\ Not\\ a\\ Technique\\ But\\ a\\ Grace\\s*\\n\n",
            "Found 'How to Use the Time of Mental Prayer' with pattern: \\n\\s*(?:\\d+\\.|\\d+|[IVX]+\\.|\\[.+?\\])\\s+How\\ to\\ Use\\ the\\ Time\\ of\\ Mental\\ Prayer\\s*\\n\n",
            "Found 'The Development of the Life of Prayer' with pattern: \\n\\s*(?:\\d+\\.|\\d+|[IVX]+\\.|\\[.+?\\])\\s+The\\ Development\\ of\\ the\\ Life\\ of\\ Prayer\\s*\\n\n",
            "Found 'Material Conditions for Mental Prayer' with pattern: \\n\\s*(?:\\d+\\.|\\d+|[IVX]+\\.|\\[.+?\\])\\s+Material\\ Conditions\\ for\\ Mental\\ Prayer\\s*\\n\n",
            "Found 'Some Methods of Mental Prayer' with pattern: \\n\\s*(?:\\d+\\.|\\d+|[IVX]+\\.|\\[.+?\\])\\s+Some\\ Methods\\ of\\ Mental\\ Prayer\\s*\\n\n",
            "Successfully extracted 1 chapters via TOC method\n",
            "Using fallback chapter extraction method...\n",
            "Created 9 equal-length sections\n",
            "Found 9 chapters.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:   0%|          | 0/9 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:07<00:14,  7.25s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:14<00:07,  7.44s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:22<00:00,  7.40s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 1' to ./evergreen_notes_output/Section_1_notes.txt and ./evergreen_notes_output/Section_1_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:  11%|█         | 1/9 [00:30<04:02, 30.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:07<00:14,  7.20s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:14<00:07,  7.50s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:22<00:00,  7.56s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 2' to ./evergreen_notes_output/Section_2_notes.txt and ./evergreen_notes_output/Section_2_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:  22%|██▏       | 2/9 [01:00<03:30, 30.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:09<00:18,  9.42s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:17<00:08,  8.34s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:25<00:00,  8.37s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 3' to ./evergreen_notes_output/Section_3_notes.txt and ./evergreen_notes_output/Section_3_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:  33%|███▎      | 3/9 [01:33<03:09, 31.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:08<00:16,  8.03s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:14<00:07,  7.38s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:22<00:00,  7.39s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 4' to ./evergreen_notes_output/Section_4_notes.txt and ./evergreen_notes_output/Section_4_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:  44%|████▍     | 4/9 [02:04<02:36, 31.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:08<00:17,  8.54s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:16<00:08,  8.36s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:23<00:00,  7.59s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 5' to ./evergreen_notes_output/Section_5_notes.txt and ./evergreen_notes_output/Section_5_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:  56%|█████▌    | 5/9 [02:35<02:05, 31.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:06<00:13,  6.57s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:13<00:06,  6.85s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:21<00:00,  7.55s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 6' to ./evergreen_notes_output/Section_6_notes.txt and ./evergreen_notes_output/Section_6_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:  67%|██████▋   | 6/9 [03:05<01:31, 30.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:08<00:17,  8.94s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:18<00:09,  9.24s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:27<00:00,  9.33s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 7' to ./evergreen_notes_output/Section_7_notes.txt and ./evergreen_notes_output/Section_7_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:  78%|███████▊  | 7/9 [03:40<01:04, 32.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:07<00:14,  7.44s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:18<00:09,  9.72s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:29<00:00, 10.00s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 8' to ./evergreen_notes_output/Section_8_notes.txt and ./evergreen_notes_output/Section_8_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing chapters:  89%|████████▉ | 8/9 [04:20<00:34, 34.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Section 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating notes:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Generating notes:  33%|███▎      | 1/3 [00:08<00:16,  8.17s/it]\u001b[A\n",
            "Generating notes:  67%|██████▋   | 2/3 [00:18<00:09,  9.28s/it]\u001b[A\n",
            "Generating notes: 100%|██████████| 3/3 [00:24<00:00,  8.03s/it]\u001b[A\n",
            "                                                               \u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Saved notes for 'Section 9' to ./evergreen_notes_output/Section_9_notes.txt and ./evergreen_notes_output/Section_9_notes.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing chapters: 100%|██████████| 9/9 [04:52<00:00, 32.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Combined notes saved to ./evergreen_notes_output/all_notes.json\n",
            "Processing complete! You can now download the evergreen_notes.zip file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_6206ca46-97fd-458a-b7f0-51d6415a436a\", \"evergreen_notes.zip\", 77400)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Would you like to view the generated notes in the notebook?\n",
            "View notes now? (y/n): y\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <h1>Notes for Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008).pdf</h1>\n",
              "        <div style=\"max-width:800px\">\n",
              "        \n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 1</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Mental prayer is a gift from God, not a technique to be mastered.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">### Mental Prayer as a Divine Gift: Beyond Techniques\n",
              "\n",
              "Jacques Philippe, in his exploration of mental prayer, emphasizes its nature as a divine gift rather than a technique to be mastered. Mental prayer, or contemplative prayer, is a spiritual engagement deeply rooted in grace—a departure from activities determined by human capability and technical mastery. Unlike Eastern practices such as yoga or Zen, which rely on systematic methodologies and human effort to reach higher states of consciousness, Christian mental prayer is fundamentally about receiving God's freely given grace. This differentiates it significantly from self-reliant spiritual techniques, as it centers on God’s initiative rather than human endeavor.\n",
              "\n",
              "The notion that mental prayer is a technique predisposes one to believe that mastery of specific methods can lead to spiritual advancement. This misconception shifts focus inwardly, emphasizing personal effort and technique rather than the transformative power of divine grace. A key implication of recognizing mental prayer as a grace is the understanding that anyone, regardless of their personal strengths or spiritual maturity, is capable of engaging with God in meaningful prayer. This universality underscores that the call to prayer, much like the call to holiness, is accessible to everyone.\n",
              "\n",
              "Furthermore, Philippe suggests that instead of perfecting methods, emphasis should be placed on cultivating inner dispositions conducive to receiving this spiritual gift. These dispositions include humility, faith, trust, and openness to God’s presence. By enhancing these attitudes, the individual opens themselves more fully to the encounter with the divine that mental prayer facilitates.\n",
              "\n",
              "Overall, mental prayer as a gift rather than a technique redefines spiritual practice by aligning it closely with divine generosity and invitation. It liberates practitioners from the pressures of self-performance and opens pathways for authentic communion with God, emphasizing trust and reliance on divine grace as the foundation of contemplation. This perspective aligns closely with the Christian understanding that true spiritual progress is rooted in God’s grace and our heartfelt response to it.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Faith and trust are essential foundations of a fruitful mental prayer life.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">**Title: Faith and Trust as the Foundations of a Fruitful Mental Prayer Life**\n",
              "\n",
              "Faith and trust form the bedrock of a meaningful mental prayer life, serving as the essential attitudes that facilitate a deeper communion with God. In Jacques Philippe's conception of mental prayer, faith is not merely a passive belief but an active engagement with God's presence. This trust must be persistent, transcending our feelings of inadequacy or the lack of immediate experience of God’s presence. As described by Philippe, the initiatory act of prayer begins with the unwavering belief that God is present and attentively listening, irrespective of our emotional or spiritual state. This divine encounter is promised through Scripture, like in Matthew 6:6, affirming that we are indeed heard and loved by God in our solitude and silence.\n",
              "\n",
              "Moreover, faith entails recognizing that mental prayer is a universal call, not limited to a religious elite. Anyone, from the sinner to the saint, is invited to experience divine intimacy through prayer. This understanding democratizes spiritual access, echoing the Gospel's inclusivity, and it shifts the focus from human ability to divine grace. The fruitfulness of mental prayer is guaranteed not by human effort or technique but by the trust in God's gracious promise to transform and sanctify those who pray.\n",
              "\n",
              "The implication of this faith-centered approach is transformative. It counters the temptation to abandon prayer due to perceived sterility or lack of tangible results. Trust in prayer's ultimate fruitfulness requires patience, akin to a farmer awaiting the harvest, as stated in James 5:7-8. This patient trust ensures that the practitioner remains committed to the path of mental prayer, allowing God’s grace to work beyond immediate perceptible outcomes.\n",
              "\n",
              "Thus, the integration of faith and trust into mental prayer aligns with the Christian understanding of grace. It liberates the pray-er from the pressure to produce results through technique and invites a humble receptivity to God's transformative presence. This foundational disposition not only anchors one’s prayer life but also enriches the broader journey towards spiritual maturity and holiness.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>God's presence and promise sustain us even when we don't see immediate results in prayer.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">### Title: God's Presence and Promise Sustain Us Even When We Don't See Immediate Results in Prayer\n",
              "\n",
              "The notion that \"God's presence and promise sustain us even when we don't see immediate results in prayer\" encapsulates a profound theological and spiritual truth. At its core, this idea emphasizes trust in God's timing and the sustaining grace of His presence, irrespective of immediate tangible outcomes from our prayers. This aligns with Jacques Philippe’s teachings in \"Time For God,\" where he underscores the experiential nature of mental prayer as a divine gift rather than a human-driven technique. Mental prayer invites an intimate encounter with God, cherished not for the outcomes it yields, but for the foundational relationship it cultivates with the divine.\n",
              "\n",
              "This concept of divine presence is rooted in faith, independent of feelings or visible efficacy. When believers engage in prayer, they do so with the conviction that God is present with them, even when their emotional or mental state challenges this belief. This aligns with scripture, particularly passages like John 6:37, which assures us of God's unwavering presence. Such faith demands embracing the certainty that prayer is transformative, albeit in ways that may not be immediately perceivable.\n",
              "\n",
              "The promise of sustained divine presence in prayer also speaks to the virtue of perseverance (James 5:7-8). It is a call to fidelity in spiritual practice, prioritizing regular prayer engagement over aesthetically or emotionally pleasing experiences. This reflects a deeper spiritual maturation, whereby one learns to lean into the unseen work of God—a process likened to the farmer's patience for crops to mature, reminding us that spiritual fruits often require time.\n",
              "\n",
              "Moreover, this perspective challenges contemporary tendencies that prioritize rapid results and technique-driven spirituality, contrasting them with a Christian approach that values grace over human effort. By focusing on our inner disposition—faith and trust—rather than methods, we open ourselves to receive God's abundant grace, fostering a prayer life that thrives on divine generosity. Thus, this evergreen insight encourages believers to persist in prayer, confident that God's presence and promises offer profound sustenance beyond observable immediacy.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 2</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Regular and faithful mental prayer holds greater value than sporadic, sublime experiences of prayer.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">The idea that \"Regular and faithful mental prayer holds greater value than sporadic, sublime experiences of prayer\" underscores the importance of consistency over intensity in spiritual practice. This notion is rooted in the understanding that mental prayer, as an exercise of loving God, requires faithfulness, which is a cornerstone of true love. Just like in human relationships, consistency and fidelity in prayer foster a deeper, more meaningful connection with God. Sublime experiences, while profound in the moment, lack the sustained engagement needed for long-term spiritual growth.\n",
              "\n",
              "Mental prayer demands purity of intention, aligning with the biblical teaching that \"Blessed are the pure in heart, for they shall see God\" (Matthew 5:8). The pure of heart seek not their own fulfillment but God’s pleasure, steering the practitioner away from self-seeking motives. This mindset helps individuals withstand periods of dryness in prayer, where the act may not be gratifying or fulfilling. Such perseverance echoes the virtue of humility, acknowledging our spiritual poverty and allowing God's grace to act, as noted by St. Teresa of Avila, who emphasized that prayer's foundation is humility.\n",
              "\n",
              "The broader implications for spiritual life are striking. Regular mental prayer cultivates spiritual maturity, fostering virtues like patience, humility, and trust. It becomes a fertile ground for receiving sacramental grace effectively, ensuring that such grace bears fruit in one’s life. This practice not only transforms the individual but also enhances their capacity for genuine charity and presence to others, offering a model of living that prioritizes spiritual richness over external achievements.\n",
              "\n",
              "In the rhythm of everyday life, regularity in prayer embeds spiritual discipline, ensuring a steady progression in holiness. It counters the temptation of superficial busyness, offering a profound, quiet strength. This idea resonates across spiritual traditions, emphasizing that the journey's worth is not in sporadic peaks but in faithful, daily steps.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Genuine love for God requires both fidelity and purity of intention in mental prayer.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Title: Genuine Love for God Requires Both Fidelity and Purity of Intention in Mental Prayer\n",
              "\n",
              "In exploring the intricate dynamics of genuine love for God through mental prayer, two core attributes emerge as indispensable: fidelity and purity of intention. These elements transcend mere religious practices and offer profound insights into spiritual devotion and personal transformation. \n",
              "\n",
              "Fidelity in mental prayer emphasizes the critical importance of consistent and committed engagement with God. It is not enough to experience sporadic spiritual highs or moments of profound insight; rather, it is through unwavering dedication that one cultivates a fertile environment for spiritual growth. The analogy of a steadfast relationship serves well here: just as a loving partnership relies on continuous presence and support, mental prayer requires regularity to bear fruit. Thus, this fidelity signifies a resilient relationship with God, wherein one persistently honors their spiritual \"appointments\" despite life's inconsistencies and distractions.\n",
              "\n",
              "Purity of intention, on the other hand, is about orienting one's actions and desires wholly towards pleasing God, rather than seeking personal gratification or spiritual accolades. This aligns with Jesus’ teaching, \"Blessed are the pure in heart, for they shall see God\" (Matthew 5:8). Purity in prayer demands a selfless motive, a deliberate choice to prioritize divine satisfaction over human reward. The challenges of maintaining such purity are acknowledged as part of the spiritual journey. However, sincere aspiration towards this purity, even when imperfect, is crucial and liberating. This self-forgetfulness allows God’s grace to work more deeply, transforming our intentions and aligning them with divine will.\n",
              "\n",
              "Moreover, both fidelity and purity of intention contribute to a holistic, transformative prayer practice that enhances one's spiritual journey. By maintaining regularity and selfless motives, one can access a deeper communion with God that inspires genuine love—one that transcends the superficial and enters into the realm of divine intimacy. This comprehensive approach connects related theological concepts, like humility and trust, reinforcing the essential dynamics of devotion and spiritual resilience. Such an integrated perspective on mental prayer enriches the understanding of how genuine love for God is cultivated and sustained.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>The transformative power of mental prayer necessitates determination and a resolute commitment to God.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Title: The Transformative Power of Mental Prayer Necessitates Determination and a Resolute Commitment to God\n",
              "\n",
              "Mental prayer is a profound spiritual exercise that transcends the act of simply communicating with the Divine and becomes a transformative journey dependent on unwavering determination and commitment. At its core, mental prayer is less about grandeur or infrequent moments of transcendental insight and more about the faithful and consistent pursuit of a loving relationship with God. It's an expression of fidelity similar to the commitment seen in deep, longstanding human relationships, where sustained, everyday acts speak louder than sporadic, grand gestures. This perspective emphasizes that the quality of prayer is less about its immediate effects and more about its role as a consistent, grounding practice that plays a vital role in the maturation of one's faith.\n",
              "\n",
              "Purity of intention—a critical component of effective mental prayer—requires that the practitioner sets aside personal satisfaction or spiritual achievement, focusing instead on pleasing God. This selfless disposition aligns with the beatitude, “Blessed are the pure in heart, for they shall see God” (Matthew 5:8), highlighting the transformative potential of approaching God with genuine love rather than utilitarian aims. As such, mental prayer demands an attitude where the believer offers their prayer time not seeking personal gratification, but rather offering it freely to God as an act of love and devotion.\n",
              "\n",
              "Humility and the acceptance of one’s own spiritual poverty form the bedrock of a perseverant prayer life. This understanding—that spiritual transformation relies on God's grace rather than personal merit—acknowledges human limitations while fostering reliance on divine mercy. Through humble persistence and relinquishing self-reliance, one emulates the prayerful posture of the saints who combined contemplative dedication with active service.\n",
              "\n",
              "Moreover, the concerns of daily obligations should not overshadow the necessity of prayer. The transformative nature of mental prayer is often hampered by perceived time constraints, yet the issue is often one of priority rather than availability. A recalibration of personal values, supported by faith in God's promise of mutual blessings, reinforces the understanding that time dedicated to prayer enriches personal and communal relationships, leading to a more fulfilling spiritual and practical life.\n",
              "\n",
              "In conclusion, mental prayer is central to spiritual growth, requiring a resolute commitment to making it an integral part of daily life. It is this enduring resolve that ensures the prayer's transformative power, deepening one's faith and aligning one's intentions with God's will, thus fostering holistic spiritual enrichment.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 3</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>To nurture a fruitful life of prayer, we must first embrace total self-giving to God.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">To nurture a fruitful life of prayer, one must first embrace total self-giving to God, a principle rooted in the understanding that genuine prayer is an expression of a life steeped in God-centered love. This approach requires more than the mere act of prayer; it asks for a deliberate alignment of one's entire life with God's will. Total self-giving involves relinquishing personal desires, including attachments and worldly distractions, in favor of making God the central focus.\n",
              "\n",
              "A contemplative life emerges from a sincere commitment to this self-renunciation. It is not enough to thread thoughts of God through daily routines, confining spiritual communion to transient moments. True spiritual communion with God demands a consistent and disciplined pause—an exercise in mental prayer where one stops entirely and dedicates time solely to the divine presence. This deliberate detachment from the busyness of life reorients one's focus from the self to the divine, enhancing spiritual intimacy and maturation.\n",
              "\n",
              "St. John of the Cross eloquently captures this process with his concept of ecstasy, defined as moving beyond personal will to be engulfed by God's will. Embracing this ecstasy in prayer necessitates a divestment from self-centric desires, signaling that true spiritual enrichment arises from losing oneself to find fulfillment in God.\n",
              "\n",
              "The broader implication of this commitment is profound: it instills a form of freedom grounded in faithfulness, unshackled from transient emotional impulses. As one's natural tendencies become aligned with divine intention, spontaneous love and virtue emerge effortlessly. Ultimately, this journey toward total self-giving transforms the practitioner of mental prayer into a conduit of divine love, whose life and relationships are enriched by faithfulness, authenticity, and deeper connections, both with God and others.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Achieving genuine freedom in prayer requires us to transcend our fickle moods and focus on faithfulness.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Achieving genuine freedom in prayer requires transcending our fickle moods and focusing on faithfulness, a topic that transcends mere theoretical discussion and delves into the essence of human spirituality. At its core, this idea reveals the tension between our natural inclination to be guided by fluctuating emotions and the higher calling of consistent and deliberate spiritual engagement. True freedom, as espoused in prayer, is not the liberty to follow every passing impulse but is found in the steadfast commitment to one's foundational beliefs and practices, even when they contradict immediate feelings or desires.\n",
              "\n",
              "This commitment to faithfulness over fickleness aligns with broader themes of authenticity and integrity in relationships, both divine and earthly. Just as relationships require intentional investment and presence beyond transient feelings, so does one's relationship with God. The stability and depth of any meaningful connection are built on a consistent and deliberate choice to prioritize that relationship, a lesson that extends to familial, social, and romantic engagements.\n",
              "\n",
              "The implication here is a call to cultivate an internal discipline over one’s momentary whims. In prayer, this entails setting aside dedicated time and mental space to foster a connection with the divine, beyond the busyness of daily life and the unpredictability of personal emotions. The text further suggests that prayer is analogous to a process of re-education, wherein one learns to subordinate transient moods to enduring truths and commitments.\n",
              "\n",
              "The broader spiritual lesson here is one of liberating the human spirit from the enslavement of both mundane distractions and internal inconsistencies. This echoes spiritual teachings across traditions—such as the Stoic discipline of focusing on what can be controlled and relinquishing the rest, or Buddhist mindfulness emphasizing presence and intention over fleeting thoughts. Thus, authentic freedom in prayer—and life—comes through a disciplined alignment with truth, facilitated by divine grace and sustained through continuous, faithful practice.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Cultivating an authentic relationship with God necessitates creating unhurried space for His presence.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Cultivating an authentic relationship with God requires intentionally creating unhurried spaces for His presence, akin to nurturing any meaningful relationship. The notion challenges the modern predisposition towards constant busyness and multitasking. While integrating prayer into daily activities is beneficial, it often falls short of fostering the deep, contemplative connection needed to truly experience God’s presence. This is comparable to familial relationships; love cannot thrive if we do not allocate dedicated time free from distractions, where we are fully present with the other person. Similarly, our relationship with God demands moments free from the chaos of everyday life, allowing us to focus solely on Him.\n",
              "\n",
              "This premise also speaks to the state of authenticity, an ideal highly regarded in contemporary society. True authenticity in spirituality is not guided by fluctuating emotions or impulses but rather by consistent and deliberate actions reflecting our faith. In this context, mental prayer emerges as a vital practice. It serves as a disciplined space where one can realign with God’s will, beyond the superficial inclinations dominated by daily fluctuations in our emotional state. Just as St. John of the Cross highlights the ecstasy in self-abandonment to God’s will, mental prayer can transform spontaneous desires to naturally align with divine will.\n",
              "\n",
              "Moreover, the journey toward maintaining this unhurried space informs our freedom. Freedom is not merely acting upon immediate desires but involves submitting these desires to a divine framework that offers true liberation and peace. Thus, cultivating this relationship involves more than sporadic engagement; it necessitates a disciplined, ongoing practice of mental prayer, fostering a profound union that transcends everyday distractions, aligning our deepest yearnings with God’s eternal faithfulness, as emphasized by figures like St. Teresa of Avila. This practice ultimately enriches both our spiritual undertakings and our interactions with others, grounding them in stability and faithfulness.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 4</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Balancing Acceptance of Our Wretchedness and Aspiration for Holiness Cultivates Fruitful Mental Prayer.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">### Balancing Acceptance of Our Wretchedness and Aspiration for Holiness Cultivates Fruitful Mental Prayer\n",
              "\n",
              "The concept of harmonizing the acknowledgment of human frailty with the pursuit of holiness is pivotal in nurturing effective mental prayer. This balance fosters a prayer life that transcends superficial engagements, grounding it instead in a profound and fruitful spiritual journey. Mental prayer, when guided by this equilibrium, becomes less about reaching an unattainable sanctity through personal effort and more about aligning oneself with divine grace.\n",
              "\n",
              "Acknowledging our wretchedness, or human shortcomings, is not a call to despair but an invitation to humility. It encourages an honest appraisal of our limitations, fostering genuine vulnerability before God. Engaging in mental prayer without pretense allows for a more authentic communication with the divine. This acceptance strips away the barriers of false pride and self-sufficiency, opening the heart to the workings of grace that can lead to transformation and growth in holiness.\n",
              "\n",
              "Simultaneously, the aspiration for holiness propels the believer beyond complacency. It is rooted in a strong, continuous desire for divine intimacy—a yearning that imbues mental prayer with passion and purpose. Such aspiration, however, recognizes that holiness is a gift bestowed by God rather than a status achieved solely through human endeavor. This understanding invites a partnership with the divine, where one's spiritual growth is seen as a cooperative venture between human effort and divine action.\n",
              "\n",
              "Moreover, the broader implications of this balanced approach extend into daily living. By cultivating a lifestyle that supports mental prayer—eschewing distractions and embracing the essential—a perpetual awareness of God's presence is fostered. Practicing this perpetual presence, akin to Brother Lawrence's approach, integrates prayer with daily life, encouraging a continual dialogue with God.\n",
              "\n",
              "Ultimately, this balance nurtures inner freedom. Living under God's merciful gaze liberates individuals from the judgments of self and others, fostering an inner landscape ripe for spiritual fruitfulness. This understanding transforms mental prayer from rote exercise to a dynamic encounter with God, where love and grace flourish. Through this dance between humility and aspiration, mental prayer becomes a school of love, deeply imprinting virtues that ripple throughout one's entire being and existence.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Living Under God's Loving Gaze Fosters Inner Freedom and Mental Prayer Growth.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">**Living Under God's Loving Gaze Fosters Inner Freedom and Mental Prayer Growth**\n",
              "\n",
              "Understanding the concept of living under God's loving gaze entails recognizing the transformative power of divine presence in cultivating inner freedom and enhancing the practice of mental prayer. This spiritual principle is rooted in the balance between recognizing our human frailties and aspiring towards holiness. On a personal level, it implies acknowledging our imperfections while also embracing a journey towards spiritual maturity through God’s grace. Instead of being driven by guilt or self-condemnation, one learns to operate from a place of acceptance and divine love, which fosters authentic freedom of the soul.\n",
              "\n",
              "This inner freedom is crucial for mental prayer, which is not merely a routine activity but a profound encounter with God. Mental prayer requires a state of recollection and presence, difficult to achieve if one's lifestyle is muddled with distractions or superficial engagements. The text echoes the notion of \"practicing the presence of God,\" as advocated by figures like Brother Lawrence, highlighting how even mundane activities can become an opportunity for ongoing dialogue with God. This continuous awareness transforms mental prayer from a mere exercise into a heartfelt expression of love, aligning with the intrinsic human inclination towards absolute self-giving.\n",
              "\n",
              "Living under God’s gaze removes the burdens of societal judgments and self-critical evaluations, replacing them with the liberating belief in divine mercy and acceptance. It invites a shift from self-centered introspection to God-focused tranquility. This dynamic fosters a life that is in constant communion with the divine, making every moment, from the ordinary to the profound, an act of prayer. In essence, by cultivating a life lived under God’s loving watch, one achieves not only growth in mental prayer but also a comprehensive shift in outlook—a pattern of peace and love that permeates all aspects of existence. This perspective aligns well with broader theological and spiritual principles, underscoring the integral link between prayer, personal growth, and a broader sense of spiritual freedom.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Recognizing the Primacy of God's Action in Mental Prayer Liberates Us from Performance Pressure.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Recognizing the primacy of God's action in mental prayer fundamentally shifts the focus from our performance to divine presence, thus liberating us from the pressure to achieve a subjective notion of success. Mental prayer can often be misconceived as an active endeavor where the value is measured by the depth of our reflections, the insights gained, or even the intensity of our emotional engagement. However, this perspective can inadvertently place undue pressure on the practitioner, while missing the essential quality of prayer: its receptivity to God's transformative presence.\n",
              "\n",
              "The true nature of mental prayer is anchored in surrender and openness to divine influence. Understanding that God is the principal actor in this sacred dialogue redirects our approach from one of anxiety about our contributions to one of peaceful receptivity. This aligns with theological teachings, such as those from St. Thérèse of Lisieux, who emphasized that even a state of restfulness (or even involuntary sleep during prayer) can be a profound form of trust and abandonment to God’s will. It's not about our conscious activity, but our silent, trusting presence before God that counts.\n",
              "\n",
              "Furthermore, this insight extends beyond the formal boundaries of mental prayer into our everyday lives. By cultivating a constant awareness of God's presence—a practice eloquently developed by Brother Lawrence of the Resurrection—every moment becomes an opportunity for grace, thus eliminating the dichotomy between 'prayer time' and 'action'. This integrated approach to spirituality not only enriches the prayer life but also permeates the entirety of a person’s experience, fostering a tranquil confidence that replaces anxiety with the comfort of being under God’s watchful care.\n",
              "\n",
              "Ultimately, by embracing the primacy of God’s action, mental prayer transforms from a task of spiritual merit to an ongoing, liberating communion with the divine, where one finds solace not in performance, but in presence. This understanding bridges mental prayer with the broader ideals of spiritual life, highlighting its role as a cornerstone of humility, trust, and genuine freedom from worldly judgments and self-imposed expectations.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 5</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Recognizing Prayer as an Act of Surrender to God’s Action Leads to Deeper Communion.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">### Recognizing Prayer as an Act of Surrender to God’s Action Leads to Deeper Communion\n",
              "\n",
              "Prayer, fundamentally, is not an exercise of exerting human effort but a surrender of self to divine action. This concept is rooted in the understanding that prayer is more about aligning oneself with God’s will than engaging in personal introspection or activity. Recognizing prayer as an act of surrender involves allowing God’s influence to permeate one’s heart and mind without the obstruction of ego or self-directed intent. This surrender can be viewed as an opportunity for spiritual anesthesia, reminiscent of the state St. John of the Cross describes—where one’s capacities may feel inert, akin to a patient allowing a surgeon to operate freely. This metaphoric 'night' enables a deeper penetration of divine grace, leading to transformation.\n",
              "\n",
              "This surrender is intrinsically linked to the primacy of love, which is not contingent on activity but rather on being loved by God. St. Teresa of Avila’s insight—that the essence of prayer is to love a lot rather than think a lot—reinforces the notion that love, not intellectual effort, fortifies communion with God. The relinquishment built into surrender becomes an act of profound love: trusting in the perpetual presence and action of God within us, as opposed to relying on personal theological or mental feats. This concept dovetails with the understanding that divine love predominantly acts upon us, merely awaiting our consent.\n",
              "\n",
              "The broader implication of this surrender in prayer is its potential to reorder the spiritual life, prioritizing divine agency over human effort. This shift demands humility and deep faith, aligning with the scriptural mandate that \"God loved us first\" (1 John 4:10). It dismantles human-centered spirituality, liberating practitioners from self-imposed spiritual exertions and opening pathways to a more profound communion centered on God's initiative. Understanding prayer in this light invites believers to experience God’s presence as a living, transformative force, beyond cognitive boundaries or self-reliant spirituality.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Letting Ourselves Be Loved by God is the Foundation of Authentic Prayer Life.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">**Letting Ourselves Be Loved by God: The Foundation of Authentic Prayer Life**\n",
              "\n",
              "At the heart of authentic prayer life lies a profound simplicity: the act of letting oneself be loved by God. This concept turns the traditional notion of prayer as active petition or contemplation on its head. Instead, it emphasizes surrender over effort, reception over action. Integral to this understanding is the recognition of God's primacy—His love and action precede ours. This is not merely theological abstraction but a practical guide for deepening one's spiritual engagement.\n",
              "\n",
              "Saints like John of the Cross and Thérèse of Lisieux have expressed this through concepts like \"the primacy of love\" and the \"dark night of the soul.\" These moments of spiritual dryness, where cognitive and emotional engagement fails, are invitations to let God's love work unimpeded within us. This reframing of spiritual 'aridity' as opportunity rather than obstacle invites believers to confront their existential 'poverty' and accept divine love without adornment or achievement—much like an anesthetized patient in the care of a surgeon.\n",
              "\n",
              "The challenge, however, is profound. Letting oneself be loved requires a steadfast belief in God's love and an acceptance of one's own limitations. This means overcoming the ego-driven impetus to act, do, or achieve something in prayer and instead trust that God's love is not contingent on our actions or virtues. By anchoring prayer in love, we ground it on an unshakeable foundation, beyond the grasp of personal inadequacy. Love thus becomes not just a component of prayer, but its animating principle.\n",
              "\n",
              "This framework for prayer suggests that the real work of spiritual life happens internally, through consent to God’s work. It shifts the locus of spiritual action from the external to the internal, from the self to God. By understanding prayer in this way, we enable a form of spiritual growth that is deeply rooted in humility and openness to divine transformation, setting the stage for a more profound experience of God’s presence in our lives. This understanding aligns seamlessly with the teachings of various mystics and supports a broader theology of divine grace and human receptivity.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Acknowledging God's Indwelling Presence in Our Hearts Transforms Our Approach to Prayer.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Acknowledging God's indwelling presence in our hearts fundamentally transforms our approach to prayer by shifting the focus from external actions to internal surrender. This concept emphasizes that prayer is not merely about articulating desires or performing rituals, but about consenting to divine action within us. As the chapter illustrates, sometimes God guides us into states of spiritual aridity—not as abandonment, but as an opportunity to cease our striving and allow God to work unhindered, akin to a surgeon anesthetizing a patient to perform intricate operations.\n",
              "\n",
              "This transition in understanding prayer invites us to embrace our spiritual poverty, offering our limitations to God rather than lamenting them. When we endow our prayer with genuine love, as noted by St. Teresa of Avila, it transcends our cognitive efforts, rooting itself in the pure act of loving God, even amidst distractions or emotional dryness. In this context, spiritual growth is fostered not through achieving eloquent prayers or profound theological insights, but through the simplicity of being loved by God, paralleling the childlike trust exemplified by the saints.\n",
              "\n",
              "The broader implication is a reorientation of our spiritual lives towards a model that prioritizes divine love over human action. Just as St. Therese of Lisieux illustrates through prayer devoid of words yet filled with love, the act of letting oneself be loved becomes powerful. This approach challenges the conventional notion of prayer as active performance, highlighting instead an inner transformation facilitated by God's presence within us. Such an understanding aligns well with interconnected themes of humility, receptivity, and divine grace, illustrating a coherent framework for experiencing God in deeper, more personal dimensions. This offers a path to spiritual liberation, where personal growth and divine communion are realized not through human merit, but through divine grace received in simple, profound love.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 6</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Christians Discover God’s Intimacy Within Their Hearts Through Silent Contemplation.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">The concept of discovering God’s intimacy within the heart through silent contemplation presents a profound journey into the depths of Christian spirituality. Central to this exploration is the notion that by engaging in silent contemplation, Christians can transcend their weaknesses and wretchedness to encounter God as an intimate presence. Drawing from classical theological insights, notably those of St. Augustine and St. John of the Cross, this practice emphasizes that God is more intimate to us than we are to ourselves. This metaphorical inward pilgrimage involves uniting oneself with God’s indwelling presence through the grace of the Holy Spirit. \n",
              "\n",
              "Silent contemplation facilitates a spiritual encounter where the believer meets God not as an external 'Other,' but as an internal communion within the 'deepest center of the soul.' This internalization of divine intimacy aligns with the biblical notion found in Deuteronomy 30:12-14 that God’s word and presence are profoundly near, accessible within one's heart. By seeking solitude and embracing silence, individuals cultivate an environment that allows them to speak with God as intimately as with a paternal figure, expressing needs and sufferings with humility and faith.\n",
              "\n",
              "This practice holds significant transformative potential; it shifts the source of human thoughts and actions from external anxieties to this deep, loving center. By persistently engaging in mental prayer and contemplation, practitioners navigate through the \"mud\" of spiritual wretchedness to draw upon the \"purest water\" of divine love, echoing St. Teresa of Avila’s metaphor. In the broader scope of prayer life, this movement from meditation to contemplation signifies a shift from an active to a passive state, where God’s grace predominates. Consequently, silent contemplation becomes a gateway to an enriched spiritual life where actions and decisions are increasingly motivated by God's love. Hence, silent contemplation is not merely an inward exercise but a transformative encounter that anchors the Christian life in divine love and intimacy.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>True Prayer Transforms from Mental Effort to a Loving Communion with God.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">True prayer, as conceptualized by mystics like St. John of the Cross and St. Teresa of Avila, transcends the initial stages of mental effort to become a profound state of loving communion with God. This transformation reflects a shift from perceiving God as an external entity to experiencing His presence within the depths of one's soul. This inner discovery—a journey from \"the mind to the heart\"—enables believers to realize that the Divine is closer and more intimate than themselves, as articulated by St. Augustine and echoed in various spiritual traditions.\n",
              "\n",
              "This inner communion is facilitated by an attitude of silence and attentive self-forgetfulness, where prayer evolves from an active mental engagement into a more passive, receptive state. St. John of the Cross emphasized that this stage allows for a deeper influx of Divine love, famously described as \"a sweet respiration of love.\" This communion happens when believers are no longer bound by structured thought and imagery, but instead invite God’s essence through acts of faith and love. The transition to this contemplative state might initially present as an impoverishment of traditional prayer, but it is, paradoxically, a rich, grace-filled space that fosters true unity with God.\n",
              "\n",
              "The broader implication of this paradigm is profound. It suggests that true prayer and spiritual maturation involve a journey towards interior simplicity and an understanding that God is not reached by mental exertion alone. Instead, it is through a receptive heart that believers tap into the divine wellspring within. This idea of internal transformation aligns with concepts from the Eastern Christian practice of the Jesus Prayer, highlighting an ecumenical affirmation of the heart's centrality in spirituality. Thus, true prayer, according to these spiritual principles, becomes a loving communion that transcends mental barriers, allowing for a deeper relationship with the Divine through silent, loving presence.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>The Wound of Love Leads to True Communion with God Through Contemplative Prayer.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">The concept of \"The Wound of Love Leads to True Communion with God Through Contemplative Prayer\" relates to a profound spiritual journey where the soul's deep thirst for divine connection transforms prayer from mere dialogue into an intimate, transformative experience. This idea, grounded in Christian mysticism, suggests that through contemplative prayer, individuals can transcend superficial interactions with God to reach a deeper communion, facilitated by a metaphorical 'wound of love.' This wound, as described by Christian mystics like St. John of the Cross, represents a longing or an insatiable thirst for divine presence, which compels the soul towards God, fostering a relationship that is both intimate and transformative.\n",
              "\n",
              "The idea of this 'wound' can be understood as both a metaphorical and paradoxical catalyst for spiritual growth. It implies that only by experiencing a deep yearning and sometimes even spiritual pain, can one truly encounter the divine within themselves. This journey within is often marked by stages of purification and enlightenment, as described in the contemplative traditions. The wound signifies vulnerability and an openness to divine love. This state is crucial for transcending the ego and the superficial layers of personal wretchedness, allowing an individual to engage with what is described as the 'deep center of the soul,’ where God resides, as articulated by St. John of the Cross and St. Augustine.\n",
              "\n",
              "In practical terms, this introspective journey involves entering a state of silent prayer and meditative contemplation, where one seeks solitude and self-recollection to encounter God's presence in their heart. It requires perseverance and faith in the process of mental prayer, gradually allowing God's intimate presence to prevail over one's personal inclinations and distractions. The ultimate goal of this journey is to allow one's actions, thoughts, and existence to emanate from this divine love. This not only enriches personal spirituality but also embodies a holistic transformation where one's life becomes an outpouring of divine love. This beautiful fusion of wounding and healing represents the spiritual paradox, where divine love purifies and sanctifies, leading to an enduring, intimate communion with the divine.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 7</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Jesus’ Response to Paul’s Thorn Illustrates the Transformative Power of Weakness in Prayer.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">**Jesus’ Response to Paul’s Thorn Illustrates the Transformative Power of Weakness in Prayer**\n",
              "\n",
              "The transformative power of weakness in prayer is exemplified beautifully in Jesus’ response to Paul's plea for relief from his \"thorn in the flesh\" as recounted in 2 Corinthians 12:9. Rather than removing the difficulty, Jesus offers a profound insight: \"My grace is sufficient for you, for my power is made perfect in weakness.\" This statement suggests that human frailty is not an obstacle to divine power; rather, it is the very canvas upon which divine grace and strength can be most vividly displayed. Weakness, therefore, becomes a critical space for divine intervention and spiritual growth.\n",
              "\n",
              "In the dynamics of prayer, this transformation of weakness into strength is pivotal. It aligns with the experiences of many who, in suffering or adversity, find an unexpected intimacy with God. The \"poor and little ones\" are often highlighted in religious contexts as having profound prayer lives precisely because their societal and personal vulnerabilities incline them toward reliance on divine grace. This is reminiscent of St. Teresa of Avila's concept of keeping the \"wound of love\" open. It is through acknowledging and inviting God into these vulnerable spaces that prayer becomes a journey toward love and union with the divine.\n",
              "\n",
              "Moreover, the transformative aspect of weakness in prayer underscores a broader spiritual principle: redemption and grace often emerge from the depths of struggle, not from worldly strength or self-sufficiency. St. Thérèse of Lisieux, in her contemplation of the Church, echoes this sentiment by emphasizing that love, like prayer founded in weakness, is what fuels the mission of the Church. Just as St. Paul, despite his 'thorn,' carried the message of Christ with great fervor, a similar divine energy can emerge from our own frailties when met with openness and faith.\n",
              "\n",
              "This idea interlinks with the Carmelite tradition, which sees intimate prayer as deeply tied to the Church's mystery and mission. The transformative power of weakness is not merely a personal spiritual truth but a communal one, integrally connecting our individual sufferings and prayers to the Church's life and purpose. Through this lens, moments of personal weakness become not just sites of potential divine power but are also contributions to the spiritual vitality of the broader faith community.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Woundedness Opens Channels for Deeper Prayer and Gifts Unavailable to the Privileged.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">### Woundedness Opens Channels for Deeper Prayer and Gifts Unavailable to the Privileged\n",
              "\n",
              "The paradoxical nature of woundedness offers profound insights into the spiritual journey, particularly in the context of prayer and divine intimacy. The concept of \"woundedness,\" as seen in theological discourses, is not merely a sign of weakness but rather a conduit for divine grace and deeper spiritual experiences. This perspective is vividly illustrated by Paul's experience of the \"thorn in his flesh,\" which, according to Jesus, serves to highlight how divine power is perfected in human weakness (2 Corinthians 12:9).\n",
              "\n",
              "For individuals who have experienced the hardships of life, often described as the \"poor and the little ones,\" there exists a unique openness to prayer and spiritual gifts. These gifts are frequently less accessible to those whose lives are cushioned by privilege and material comfort. The poor are often more attuned to the nuances of divine love, as their life's struggles and vulnerabilities create an inherent receptivity to grace.\n",
              "\n",
              "Critically, this notion challenges conventional perceptions of spiritual progress, suggesting that the traditional stages of spiritual development can be transcended or reordered by profound divine encounters. Spiritual poverty becomes a fertile ground for mental prayer, emphasizing the maintenance of an \"open wound of love\" that is never allowed to heal, fostering a continual longing for divine communion. This is aligned with St. Teresa of Avila's call to actively \"strain to draw up the water we need,\" which symbolizes nurturing a responsive heart in love (St. Teresa of Avila).\n",
              "\n",
              "The theological and ecclesiastical implications are significant, underscoring the deep relationship between personal woundedness, prayer, and Church community life. The ecclesial dimension, particularly in the Carmelite tradition, illustrates how personal contemplative practices integrate with the Church's larger mission. St. Thérèse of Lisieux epitomizes this unity; her prayerful commitment is an act of love that serves the Church's broader calling. Thus, the wounded heart, through its very vulnerability, becomes a powerful agent of divine love and service, capable of lifting the spiritual world as articulated through the teachings of saints and mystics. The profound truth here is that in embracing our woundedness, we open ourselves to a deeper channel with the divine, a channel that echoes throughout personal and communal spiritual journeys.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>The Experience of Love in Prayer Connects Personal Intimacy with the Mission of the Church.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">**The Experience of Love in Prayer Connects Personal Intimacy with the Mission of the Church**\n",
              "\n",
              "The intertwining of personal prayer and ecclesiastical mission reveals a profound dimension of Christian spiritual life. Prayer, as a personal, intimate dialogue with God, becomes not merely an individual pursuit of divine presence, but a communal act that nurtures the vibrancy and mission of the Church. This connection is encapsulated beautifully in the Carmelite tradition, particularly in the teachings of St. Thérèse of Lisieux, who exemplified the transformative power of love in prayer as it radiates throughout the Church's mission.\n",
              "\n",
              "Prayer, especially in its most contemplative form, transcends personal spirituality to invigorate the Church's mission—a concept significantly evolved through St. Thérèse of Lisieux's life. Her dedication to prayerful love portrayed the inseparability of personal intimacy with God and the universal call to Church mission. St. Thérèse’s realization that her ultimate vocation was \"in the heart of the Church, I will be love,\" underscores how intimate love for God in prayer does not isolate the individual from the world but weaves them into the very fabric of the Church’s evangelical mission. In essence, her contemplative spirit led to a dynamic participation in the Church's apostolic efforts, despite her physical seclusion.\n",
              "\n",
              "The idea that deep, personal prayer contributes significantly to ecclesiastical endeavors aligns with the notion of contemplative prayer as a fulcrum for spiritual strength. This perspective reflects St. John of the Cross's assertion that a single act of pure love in prayer brings more to the Church than manifold deeds of service. Here, love becomes both the method and the goal; it is the binding force that unites the soul with God and, by extension, integrates the individual into the Church's mission.\n",
              "\n",
              "This concept also invites broader theological reflections on the nature of the Church as the Mystical Body of Christ—wherein each member's spiritual growth through prayer fortifies the whole. The Carmelite charism emphasizes this connection, teaching that prayer is not an escape from reality but a powerful engagement with it through love. This love, poured forth in personal prayer, finds its ultimate expression in service to the Church and humanity, echoing the vibrant interplay of contemplation and action in Christian life. Thus, the experience of love in prayer profoundly connects the personal with the ecclesiastical, nurturing a holistic spirituality rooted in communion and mission.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 8</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Mental prayer requires stability and stillness for a deep exchange of love with God.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Mental prayer, a form of internal dialogue with the divine, necessitates an environment of stability and stillness to facilitate a deep exchange of love with God. This practice, deeply rooted in both Eastern and Western Christian traditions, emphasizes the importance of a deliberate, calm, and stable posture to engage fully in a spiritual conversation. Mental prayer is not merely a sequence of thoughts or words but a dynamic participation that involves the entire being—mind, heart, soul, and spirit—in both receiving God and offering oneself in return. The essence of mental prayer is captured in the idea that the movements of love are inherently slow and peaceful, necessitating a foundation of stability.\n",
              "\n",
              "This stillness is a countermeasure to the modern propensity for distraction and the rapid switching between different modes of engagement. Flitting from one prayer form to another dilutes the depth and quality of the experience, undermining the potential for an authentic encounter with the divine. Like meditation, which has been central to mental prayer practices, stability in mental prayer fosters a contemplative space where the intellect and emotions converge in a unified act of devotion. The concern here is not intellectual achievement but rather the cultivation of a profound love for God—a slow, intentional 'rumination' that transforms scripture and spiritual insights into heartfelt conversation, thanksgiving, and adoration.\n",
              "\n",
              "Moreover, the broader implications of achieving such stability in mental prayer extend to improved spiritual awareness and presence in daily life. As one's capacity for inner stillness grows, so does their ability to carry that tranquility into external interactions, fostering a life more attuned to the sacred. This practice encourages a gradual shedding of the need for constant stimulation, aligning the practitioner's life with a more contemplative, meaningful rhythm—a basis for spiritual growth and an enriched daily existence. Thus, mental prayer not only enriches one's personal spirituality but also harmonizes one's life with the divine essence.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>The Jesus Prayer offers a simple and humble approach to deepen one’s mystical union with God.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">The Jesus Prayer, an ancient spiritual practice within Eastern Christianity, offers a profound yet simple approach to achieving a deep mystical union with God. Its essence lies in the repetition of a short, humble formula, typically \"Lord Jesus, Son of the Living God, have mercy on me, a sinner!\" This prayer encapsulates the duality of acknowledging human fallibility and seeking divine mercy, forging a relationship grounded in humility and devotion.\n",
              "\n",
              "The beauty of the Jesus Prayer is its accessibility and adaptability. Unlike structured meditation or elaborate ritualistic practices, it can be practiced anywhere, at any time, becoming a seamless part of daily life. Over time, the prayer can become deeply ingrained, moving from a verbal exercise to an internal heartbeat, embodying the ideal of \"ceaseless prayer\" as encouraged by the Apostle Paul.\n",
              "\n",
              "A critical component of the Jesus Prayer is the \"spirituality of the Name,\" where invoking the name of Jesus is an act of profound significance. This is rooted in the belief that names hold power and presence, thus repeating Jesus’ name is a way of inviting his essence into one's heart and life. It aligns with the concept of prayers as dialogic, emphasizing presence over eloquence.\n",
              "\n",
              "The practice is not without its challenges; it requires moderation and sincerity to avoid becoming a mechanical ritual devoid of life. The transformation from mere repetition to a \"prayer of the heart\" is gradual and often described as a gift of grace rather than an achievement of technique.\n",
              "\n",
              "The broader implications of this prayer reach into other domains of spiritual practice. It serves as a bridge between structured meditation and spontaneous prayer, echoing the meditative aspects of practices like lectio divina and the simplicity of the Rosary. Ultimately, the Jesus Prayer exemplifies a pathway to contemplative intimacy with the divine, characterized by simplicity, humility, and constant remembrance. Through persistent, heartfelt devotion, practitioners can nurture a vibrant, personal relationship with God, inviting divine presence into every aspect of life.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Encountering difficulties in prayer, such as dryness and distraction, can be seen as opportunities for growth and deeper connection with God.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Encountering dryness and distraction in prayer may initially seem like obstacles to one's spiritual journey; however, they can be transformative opportunities for deeper growth and a more profound connection with God. These challenges, though often greeted with frustration, can act as crucibles through which one’s faith is tested and refined. The experience of dryness in prayer, for example, often signifies an evolving spiritual maturity. It indicates a transition from basing one’s relationship with God on emotional or sensory fulfillment, to a more resilient faith grounded in trust and love, independent of perceptible affirmation.\n",
              "\n",
              "Distractions, on the other hand, are a testament to the unpredictability and restlessness of the human mind. While they can disrupt the flow of prayer, they can also serve as reminders of the need for humility and perseverance. The task isn’t to eradicate distractions but to learn to lovingly return focus to God without self-reproach, recognizing that genuine prayer is more about the heart’s intention than the mind’s steadfastness. This symbiotic relationship between distraction and focus can cultivate a form of prayer that is robust and patient, eventually enriching the practitioner's spiritual life.\n",
              "\n",
              "These difficulties also highlight an important principle in spiritual growth: the move from active to passive forms of prayer, where one allows God to lead the soul in stillness and depth. As exemplified by St. John of the Cross, letting go of the need to control or perfect the prayer experience can open doors to an intimate encounter with God that transcends words and concepts. Thus, these trials in prayer are fundamental to nurturing a faith that is both humble and liberated, fostering a union with God that is based on divine love and presence rather than conditioned experiences.\n",
              "\n",
              "In this process, integrating practices like the Jesus Prayer or the Rosary can also aid in maintaining a rhythm of prayer throughout daily life. By engaging consistently with these difficulties, one learns to approach prayer with the heart, embracing its transformative potential rather than seeking comfort in its transient manifestations. This paradigm shift underscores the journey of faith as a pilgrimage toward greater communion with the divine, with each obstacle a stepping stone toward authentic spiritual maturity.</div>\n",
              "                </div>\n",
              "                </div>\n",
              "            <div style=\"margin-top:20px; border:1px solid #ccc; padding:10px; border-radius:5px\">\n",
              "                <h2>Section 9</h2>\n",
              "            \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Meditative contemplation on virtues enhances the desire to imitate Christ's humility.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">The practice of meditative contemplation on virtues—particularly humility—can profoundly enhance the desire to emulate Christ's humility. This spiritual exercise invites individuals to engage deeply with both cognitive and affective dimensions of their faith, fostering a transformative longing to embody Christ-like qualities. By contemplating virtues as exemplified by Christ, believers are prompted to reflect not merely on the ethical implications of these virtues but also on their spiritual significance.\n",
              "\n",
              "This meditation process involves several steps. Initially, one evokes vivid mental images of Christ's humility, such as His birth in a stable, His obedience to Mary and Joseph, or the act of washing His apostles' feet. These narratives cultivate admiration and appreciation for the depth of Christ's humility, stirring one's heart towards love and imitation. This emotional engagement serves as a catalyst for aspiring to greater humility personally.\n",
              "\n",
              "From a broader perspective, this practice aligns with the Christian concept of \"kenosis,\" the self-emptying exemplified by Christ. As believers meditate on humility, they are encouraged to recognize their own pride and self-sufficiency, which stand in stark contrast to Christ's self-giving nature. The understanding that humility is foundational to all virtues, as it counters pride—the root of sin—offers a compelling reason for internalizing this virtue.\n",
              "\n",
              "Furthermore, contemplation can extend beyond Christ to include other figures such as Mary or Saints, whose lives also model humility. The Saints’ practices provide contemporary and relatable contexts for application. Engaging with the lives of these figures urges a practical evaluation of personal behavior, inspiring resolutions tailored to personal contexts. This could be as specific as maintaining a posture of humility in scholarly discussions or self-awareness in moments of potential pride.\n",
              "\n",
              "Thus, the contemplative meditation on humility serves as an integrative practice that not only enhances spiritual understanding but also fosters actionable resolutions towards personal growth, interweaving cognitive reflection, emotional resonance, and behavioral transformation in the journey toward holiness. This practice, by its very nature, invites ongoing reflection, reinforcing one's commitment to transformation through the imitation of Christ and Saints, thereby embedding virtue deeply within one's spiritual framework.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Effective mental prayer requires specific resolutions based on self-examination and accountability.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">### Effective Mental Prayer Requires Specific Resolutions Based on Self-Examination and Accountability\n",
              "\n",
              "Effective mental prayer, as examined in spiritual disciplines, is greatly enhanced through the formulation of specific resolutions grounded in self-examination and accountability. This approach not only supports spiritual growth but also ensures that the practice of prayer transcends mere ritual and becomes a transformative experience. \n",
              "\n",
              "The process begins with meditation on divine attributes or virtues, allowing the practitioner to connect deeply with their inherent meanings. For instance, meditating on humility invites reflection on Christ's life—from His birth in humble circumstances to His service and sacrifices. This sets the stage for expressing love, gratitude, and a desire to emulate these virtues. Such meditative exercises are further enriched by considering similar virtues in the lives of saints or Biblical figures, which provides concrete examples of virtue in action.\n",
              "\n",
              "Following meditation, self-examination serves as a critical introspective phase, prompting individuals to assess their personal alignment with the virtues contemplated. This involves identifying past behaviors and thought patterns that diverge from these ideals, fostering awareness of specific instances where one's practices fall short. For example, recognizing moments of pride amidst a meditation on humility could uncover patterns of behavior that need to be addressed.\n",
              "\n",
              "Transformative prayer then demands specific and actionable resolutions. General intentions, such as \"to be more humble,\" are insufficient. Instead, resolutions require identifying contextual applications, like cultivating humility in specific interpersonal interactions where pride often emerges. This strategized approach integrates the abstract virtues into tangible daily actions.\n",
              "\n",
              "Accountability mechanisms further solidify these resolutions. By regularly revisiting the resolutions throughout the day, practitioners can gauge their progress and realign intentions, ensuring that the prayerful intentions are manifested in their actions. Moreover, invoking divine assistance affirms the individual's dependence on grace, intertwining spiritual dedication with active self-improvement.\n",
              "\n",
              "The broader implication of integrating resolution-based prayer with self-examination underscores its role in continuous spiritual development. It shifts prayer from being a passive recital to an active engagement with divine principles, fostering a more profound connection with God and promoting personal transformation consistent with one's spiritual commitments. Such practices resonate with related spiritual concepts, such as the Ignatian Examen and the practice of mindfulness in contemporary contexts, both emphasizing intentionality and reflective awareness as paths to spiritual and personal growth.</div>\n",
              "                </div>\n",
              "                \n",
              "                <div style=\"margin:10px 0; background:#f8f8f8; padding:10px; border-radius:3px\">\n",
              "                    <h3>Integrating brief, conscious moments of prayer throughout the day cultivates a continuous connection with God.</h3>\n",
              "                    <div style=\"white-space:pre-wrap\">Integrating brief, conscious moments of prayer throughout the day cultivates a continuous connection with God, offering a powerful way to maintain spiritual mindfulness and presence. This practice, as described within spiritual contexts such as those by Brother Lawrence and traditional meditation, emphasizes the importance of periodic, intentional pauses amidst daily activities. Such moments transform mundane tasks into opportunities for divine communion and reflection.\n",
              "\n",
              "Unlike formal prayer settings, these brief moments of prayer don't require extensive preparation or specific physical settings. They are accessible anywhere at any time. For instance, a momentary prayer can occur while walking, during work, or even amidst routine tasks, exemplifying how daily life interweaves with spirituality seamlessly. Imagining oneself amidst significant mysteries or virtues—whether the humility of Christ or celestial mysteries—fuels this practice, encouraging active participation in and reflection on divine perfections.\n",
              "\n",
              "Critically, these moments foster an internal oratory, a sacred inner space accessible at will. By cultivating this awareness, practitioners engage in an ongoing dialogue with God, which enhances spiritual resilience and focus. This ongoing spiritual connection mitigates distractions, grounding believers in divine presence and stabilizing their moral and ethical compass amidst societal pressures.\n",
              "\n",
              "The implications of such a practice are profound. It suggests a recalibration of spiritual life where spiritual expression isn't confined to the chapel but extends into every facet of existence. This echoes themes found in broader spiritual discussions that emphasize the perpetual nature of the divine-human relationship, contrasting with approaches that compartmentalize spirituality into distinct segments of life.\n",
              "\n",
              "Therefore, integrating brief prayers aligns with practices like mindfulness and meditation, revealing that spiritual consciousness is an ever-present divine encounter. This understanding not only enhances individual spiritual journeys but also contributes to a collective sense of sacred mindfulness, potentially transforming communities as each individual's spiritual awareness heightens.</div>\n",
              "                </div>\n",
              "                </div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Copy the entire EvergreenNotesGenerator class and run_in_colab function here\n",
        "\n",
        "# Now add this code at the end to execute:\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Upload PDF file\n",
        "print(\"Please upload your PDF book:\")\n",
        "uploaded = files.upload()\n",
        "pdf_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Get OpenAI API key\n",
        "api_key = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "# Set number of notes per chapter\n",
        "notes_per_chapter = int(input(\"How many notes per chapter? (default: 3): \") or \"3\")\n",
        "\n",
        "# Ask if user wants to use mixed models\n",
        "use_mixed = input(\"Use GPT-4o-mini for title generation to save costs? (y/n, default: y): \").lower() != 'n'\n",
        "\n",
        "# Run the generator\n",
        "print(f\"Starting to process {pdf_filename} with {notes_per_chapter} notes per chapter...\")\n",
        "run_in_colab(pdf_filename, api_key, notes_per_chapter=notes_per_chapter, use_mixed_models=use_mixed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iTZzyaQJwFWh",
        "outputId": "669896c5-135e-4509-e508-1852da28bb18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.66.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Please upload your PDF file:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b05deb0-9493-4a7d-9cee-e87c883b2f15\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b05deb0-9493-4a7d-9cee-e87c883b2f15\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008).pdf to Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008) (2).pdf\n",
            "Processing Jacques Philippe_ Helena Scott - Time for God_ A Guide to Mental Prayer (2008) (2).pdf...\n",
            "Enter your OpenAI API key: ··········\n",
            "Extracting chapters using OpenAI...\n",
            "\n",
            "Found 33 chapters:\n",
            "1. Mental Prayer is Not a Technique But a Grace\n",
            "2. 1. Mental Prayer is Not a Kind of Christian Yoga\n",
            "3. 2. Some Immediate Consequences\n",
            "4. 3. Faith and Trust as the Basis for Mental Prayer\n",
            "5. 4. Fidelity and Perseverance\n",
            "6. 5. Purity of Intention\n",
            "7. 6. Humility and Poverty of Heart\n",
            "8. 7. Determination to Persevere\n",
            "9. 8. Total Self-Giving to God\n",
            "10. How to Use the Time of Mental Prayer\n",
            "11. 1. Introductory Ideas\n",
            "12. 2. When the Question Does Not Arise\n",
            "13. 3. Primacy of God’s Action\n",
            "14. 4. Primacy of Love\n",
            "15. 5. God Gives Himself Through the Humanity of Jesus\n",
            "16. 6. God Dwells in Our Hearts\n",
            "17. The Development of the Life of Prayer\n",
            "18. 1. From the Mind to the Heart\n",
            "19. 2. The Wound in the Heart\n",
            "20. 3. Our Hearts and the Church’s Heart\n",
            "21. Material Conditions for Mental Prayer\n",
            "22. 1. Time\n",
            "23. 2. Place\n",
            "24. 3. Physical Attitudes\n",
            "25. Some Methods of Mental Prayer\n",
            "26. 1. Preliminary Ideas\n",
            "27. 2. Meditation\n",
            "28. 3. The “Jesus Prayer”\n",
            "29. 4. The Rosary\n",
            "30. 5. How to Tackle Certain Difficulties\n",
            "31. Appendices\n",
            "32. 1. Method of Meditation Proposed by Father Liebermann\n",
            "33. 2. The Practice of the Presence of God According to Brother Lawrence of the Resurrection\n",
            "\n",
            "Extracting chapter contents...\n",
            "Estimating page numbers...\n",
            "Saved: 01_Mental_Prayer_is_Not_a_Technique_But_a_Grace.txt\n",
            "Saved: 02_1_Mental_Prayer_is_Not_a_Kind_of_Christian_Yoga.txt\n",
            "Saved: 03_2_Some_Immediate_Consequences.txt\n",
            "Saved: 04_3_Faith_and_Trust_as_the_Basis_for_Mental_Prayer.txt\n",
            "Saved: 05_4_Fidelity_and_Perseverance.txt\n",
            "Saved: 06_5_Purity_of_Intention.txt\n",
            "Saved: 07_6_Humility_and_Poverty_of_Heart.txt\n",
            "Saved: 08_7_Determination_to_Persevere.txt\n",
            "Saved: 09_8_Total_Self-Giving_to_God.txt\n",
            "Saved: 10_How_to_Use_the_Time_of_Mental_Prayer.txt\n",
            "Saved: 11_1_Introductory_Ideas.txt\n",
            "Saved: 12_2_When_the_Question_Does_Not_Arise.txt\n",
            "Saved: 13_3_Primacy_of_Gods_Action.txt\n",
            "Saved: 14_4_Primacy_of_Love.txt\n",
            "Saved: 15_5_God_Gives_Himself_Through_the_Humanity_of_Jesus.txt\n",
            "Saved: 16_6_God_Dwells_in_Our_Hearts.txt\n",
            "Saved: 17_The_Development_of_the_Life_of_Prayer.txt\n",
            "Saved: 18_1_From_the_Mind_to_the_Heart.txt\n",
            "Saved: 19_2_The_Wound_in_the_Heart.txt\n",
            "Saved: 20_3_Our_Hearts_and_the_Churchs_Heart.txt\n",
            "Saved: 21_Material_Conditions_for_Mental_Prayer.txt\n",
            "Saved: 22_1_Time.txt\n",
            "Saved: 23_2_Place.txt\n",
            "Saved: 24_3_Physical_Attitudes.txt\n",
            "Saved: 25_Some_Methods_of_Mental_Prayer.txt\n",
            "Saved: 26_1_Preliminary_Ideas.txt\n",
            "Saved: 27_2_Meditation.txt\n",
            "Saved: 28_3_The_Jesus_Prayer.txt\n",
            "Saved: 29_4_The_Rosary.txt\n",
            "Saved: 30_5_How_to_Tackle_Certain_Difficulties.txt\n",
            "Saved: 31_Appendices.txt\n",
            "Saved: 32_1_Method_of_Meditation_Proposed_by_Father_Lieberma.txt\n",
            "\n",
            "Creating zip archive for download...\n",
            "updating: extracted_chapters/07_6_Humility_and_Poverty_of_Heart.txt (deflated 56%)\n",
            "updating: extracted_chapters/22_1_Time.txt (deflated 56%)\n",
            "updating: extracted_chapters/27_2_Meditation.txt (deflated 58%)\n",
            "updating: extracted_chapters/20_3_Our_Hearts_and_the_Churchs_Heart.txt (deflated 57%)\n",
            "updating: extracted_chapters/04_3_Faith_and_Trust_as_the_Basis_for_Mental_Prayer.txt (deflated 54%)\n",
            "updating: extracted_chapters/10_How_to_Use_the_Time_of_Mental_Prayer.txt (deflated 55%)\n",
            "updating: extracted_chapters/25_Some_Methods_of_Mental_Prayer.txt (deflated 57%)\n",
            "updating: extracted_chapters/31_Appendices.txt (deflated 58%)\n",
            "updating: extracted_chapters/15_5_God_Gives_Himself_Through_the_Humanity_of_Jesus.txt (deflated 53%)\n",
            "updating: extracted_chapters/03_2_Some_Immediate_Consequences.txt (deflated 56%)\n",
            "updating: extracted_chapters/23_2_Place.txt (deflated 59%)\n",
            "updating: extracted_chapters/14_4_Primacy_of_Love.txt (deflated 57%)\n",
            "updating: extracted_chapters/11_1_Introductory_Ideas.txt (deflated 54%)\n",
            "updating: extracted_chapters/19_2_The_Wound_in_the_Heart.txt (deflated 57%)\n",
            "updating: extracted_chapters/21_Material_Conditions_for_Mental_Prayer.txt (deflated 56%)\n",
            "updating: extracted_chapters/32_1_Method_of_Meditation_Proposed_by_Father_Lieberma.txt (deflated 63%)\n",
            "updating: extracted_chapters/05_4_Fidelity_and_Perseverance.txt (deflated 56%)\n",
            "updating: extracted_chapters/02_1_Mental_Prayer_is_Not_a_Kind_of_Christian_Yoga.txt (deflated 53%)\n",
            "updating: extracted_chapters/28_3_The_Jesus_Prayer.txt (deflated 56%)\n",
            "updating: extracted_chapters/18_1_From_the_Mind_to_the_Heart.txt (deflated 56%)\n",
            "updating: extracted_chapters/06_5_Purity_of_Intention.txt (deflated 57%)\n",
            "updating: extracted_chapters/29_4_The_Rosary.txt (deflated 58%)\n",
            "updating: extracted_chapters/24_3_Physical_Attitudes.txt (deflated 56%)\n",
            "updating: extracted_chapters/16_6_God_Dwells_in_Our_Hearts.txt (deflated 55%)\n",
            "updating: extracted_chapters/30_5_How_to_Tackle_Certain_Difficulties.txt (deflated 55%)\n",
            "updating: extracted_chapters/00_chapter_summary.txt (deflated 51%)\n",
            "updating: extracted_chapters/08_7_Determination_to_Persevere.txt (deflated 56%)\n",
            "updating: extracted_chapters/12_2_When_the_Question_Does_Not_Arise.txt (deflated 55%)\n",
            "updating: extracted_chapters/13_3_Primacy_of_Gods_Action.txt (deflated 57%)\n",
            "updating: extracted_chapters/17_The_Development_of_the_Life_of_Prayer.txt (deflated 57%)\n",
            "updating: extracted_chapters/26_1_Preliminary_Ideas.txt (deflated 54%)\n",
            "updating: extracted_chapters/01_Mental_Prayer_is_Not_a_Technique_But_a_Grace.txt (deflated 56%)\n",
            "updating: extracted_chapters/09_8_Total_Self-Giving_to_God.txt (deflated 55%)\n",
            "  adding: extracted_chapters/ (stored 0%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_76b69058-0696-4095-848e-497893a61fc1\", \"extracted_chapters.zip\", 64578)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download started for extracted_chapters.zip\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_c26fc1e9-3575-4da3-b7d8-3d6b6e9aa7c9\", \"00_chapter_summary.txt\", 1462)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download started for extracted_chapters/00_chapter_summary.txt\n",
            "\n",
            "Processing complete! 32 chapters extracted.\n",
            "Look in the extracted_chapters folder for your extracted chapters.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install pypdf openai\n",
        "\n",
        "# Import libraries\n",
        "import pypdf\n",
        "import openai\n",
        "import re\n",
        "import os\n",
        "from google.colab import files\n",
        "import getpass\n",
        "\n",
        "# Step 1: Upload PDF\n",
        "def upload_pdf():\n",
        "    print(\"Please upload your PDF file:\")\n",
        "    uploaded = files.upload()\n",
        "    pdf_filename = next(iter(uploaded))\n",
        "    return pdf_filename\n",
        "\n",
        "# Step 2: Extract first 10 pages (likely contains TOC)\n",
        "def extract_initial_text(pdf_path, pages=10):\n",
        "    reader = pypdf.PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for i in range(min(pages, len(reader.pages))):\n",
        "        text += reader.pages[i].extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Step 3: Use OpenAI model to extract TOC\n",
        "def get_toc_from_llm(initial_text, api_key, model=\"gpt-4o-mini\"):\n",
        "    client = openai.OpenAI(api_key=api_key)\n",
        "    prompt = f\"\"\"You're given the initial pages of a book. Clearly list ONLY the chapters from the table of contents exactly as they appear. Number them clearly.\n",
        "\n",
        "TEXT:\n",
        "{initial_text}\n",
        "\n",
        "Return ONLY the numbered list of chapters.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"Extract chapters accurately from tables of contents.\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    toc_raw = response.choices[0].message.content.strip()\n",
        "    chapters = [re.sub(r'^\\d+[.)]\\s*', '', line).strip() for line in toc_raw.splitlines() if line.strip()]\n",
        "    return chapters\n",
        "\n",
        "# Step 4: Extract full text and split by chapters\n",
        "def split_chapters(pdf_path, chapters):\n",
        "    reader = pypdf.PdfReader(pdf_path)\n",
        "\n",
        "    # First extract full text\n",
        "    full_text = \"\"\n",
        "    for page in reader.pages:\n",
        "        full_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "    # Map chapters to their positions in the text\n",
        "    splits = {}\n",
        "    for idx, chapter in enumerate(chapters):\n",
        "        pattern = re.escape(chapter)\n",
        "        match = re.search(pattern, full_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            splits[match.start()] = chapter\n",
        "\n",
        "    # Sort by position and extract chapter contents\n",
        "    sorted_splits = sorted(splits.items())\n",
        "    chapter_contents = {}\n",
        "\n",
        "    for i, (start_pos, title) in enumerate(sorted_splits):\n",
        "        end_pos = sorted_splits[i+1][0] if i+1 < len(sorted_splits) else len(full_text)\n",
        "        chapter_contents[title] = full_text[start_pos:end_pos].strip()\n",
        "\n",
        "    return chapter_contents\n",
        "\n",
        "# Step 5: Estimate page numbers for chapters\n",
        "def estimate_page_numbers(pdf_path, chapter_titles):\n",
        "    \"\"\"Estimate page numbers where each chapter begins\"\"\"\n",
        "    reader = pypdf.PdfReader(pdf_path)\n",
        "    page_numbers = {}\n",
        "\n",
        "    # For each page, check if it contains any chapter title\n",
        "    for page_num in range(len(reader.pages)):\n",
        "        page_text = reader.pages[page_num].extract_text()\n",
        "\n",
        "        for title in chapter_titles:\n",
        "            if title in page_text:\n",
        "                page_numbers[title] = page_num + 1  # 1-based page numbers\n",
        "                break\n",
        "\n",
        "    return page_numbers\n",
        "\n",
        "# Step 6: Save chapters to files with additional metadata\n",
        "def save_chapters(chapter_contents, page_numbers, output_dir=None):\n",
        "    # Create output directory\n",
        "    if output_dir is None:\n",
        "        output_dir = \"extracted_chapters\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create summary file\n",
        "    with open(os.path.join(output_dir, \"00_chapter_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"CHAPTER SUMMARY\\n\")\n",
        "        f.write(\"==============\\n\\n\")\n",
        "\n",
        "        for i, title in enumerate(chapter_contents.keys(), 1):\n",
        "            page = page_numbers.get(title, \"Unknown\")\n",
        "            f.write(f\"{i}. {title} (Page {page})\\n\")\n",
        "\n",
        "    # Save individual chapter files\n",
        "    for i, (title, content) in enumerate(chapter_contents.items(), 1):\n",
        "        safe_title = re.sub(r'[^\\w\\s-]', '', title).replace(' ', '_')[:50]\n",
        "        filename = f\"{i:02d}_{safe_title}.txt\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            page = page_numbers.get(title, \"Unknown\")\n",
        "            f.write(f\"CHAPTER: {title}\\n\")\n",
        "            f.write(f\"PAGE: {page}\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "            f.write(content)\n",
        "\n",
        "        print(f\"Saved: {filename}\")\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "# Step 7: Create zip file and download it\n",
        "def create_downloadable_zip(output_dir):\n",
        "    # Create a zip file\n",
        "    zip_filename = f\"{output_dir}.zip\"\n",
        "\n",
        "    # Use shell command to create zip (safer in Colab)\n",
        "    !zip -r {zip_filename} {output_dir}\n",
        "\n",
        "    # Make files available for download\n",
        "    from google.colab import files\n",
        "\n",
        "    try:\n",
        "        # Download the zip file\n",
        "        files.download(zip_filename)\n",
        "        print(f\"Download started for {zip_filename}\")\n",
        "\n",
        "        # Also provide the summary file separately\n",
        "        summary_file = os.path.join(output_dir, \"00_chapter_summary.txt\")\n",
        "        files.download(summary_file)\n",
        "        print(f\"Download started for {summary_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during download: {e}\")\n",
        "        print(\"If download doesn't start automatically, use the file browser to download the files.\")\n",
        "\n",
        "# Main workflow function\n",
        "def extract_book_chapters():\n",
        "    # 1. Upload PDF\n",
        "    pdf_file = upload_pdf()\n",
        "    print(f\"Processing {pdf_file}...\")\n",
        "\n",
        "    # 2. Get API key\n",
        "    api_key = getpass.getpass('Enter your OpenAI API key: ')\n",
        "\n",
        "    # 3. Extract initial text for TOC analysis\n",
        "    initial_text = extract_initial_text(pdf_file)\n",
        "\n",
        "    # 4. Use LLM to extract chapter list\n",
        "    print(\"Extracting chapters using OpenAI...\")\n",
        "    chapters = get_toc_from_llm(initial_text, api_key)\n",
        "\n",
        "    print(f\"\\nFound {len(chapters)} chapters:\")\n",
        "    for idx, chap in enumerate(chapters, 1):\n",
        "        print(f\"{idx}. {chap}\")\n",
        "\n",
        "    # 5. Split content by chapters\n",
        "    print(\"\\nExtracting chapter contents...\")\n",
        "    chapter_contents = split_chapters(pdf_file, chapters)\n",
        "\n",
        "    # 6. Estimate page numbers\n",
        "    print(\"Estimating page numbers...\")\n",
        "    page_numbers = estimate_page_numbers(pdf_file, chapters)\n",
        "\n",
        "    # 7. Save chapters to files\n",
        "    output_dir = save_chapters(chapter_contents, page_numbers)\n",
        "\n",
        "    # 8. Create zip file for download\n",
        "    print(\"\\nCreating zip archive for download...\")\n",
        "    create_downloadable_zip(output_dir)\n",
        "\n",
        "    print(f\"\\nProcessing complete! {len(chapter_contents)} chapters extracted.\")\n",
        "    print(f\"Look in the {output_dir} folder for your extracted chapters.\")\n",
        "\n",
        "# Run the extraction process\n",
        "if __name__ == \"__main__\":\n",
        "    extract_book_chapters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ftERWVgbNXb",
        "outputId": "7079587a-e854-4721-c420-8da37e4e2f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Den0rXbzbJ12",
        "outputId": "39381cf0-f1c0-4de3-f12f-b113fe7aec52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your PDF book:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-57e4f9bb-c7cb-40a2-9565-a9015885cee5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-57e4f9bb-c7cb-40a2-9565-a9015885cee5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Jacques Philippe - Interior Freedom-Scepter Publishers (2010).pdf to Jacques Philippe - Interior Freedom-Scepter Publishers (2010) (5).pdf\n",
            "Enter your OpenAI API key: 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\n",
            "How many notes per chapter? (default: 3): 1\n",
            "Use GPT-4o-mini for title generation to save costs? (y/n, default: y): y\n",
            "Starting to process Jacques Philippe - Interior Freedom-Scepter Publishers (2010) (5).pdf with 1 notes per chapter...\n",
            "Processing Jacques Philippe - Interior Freedom-Scepter Publishers (2010) (5).pdf...\n",
            "Extracting chapters using OpenAI...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 33 chapters:\n",
            "1. The Search for Freedom\n",
            "2. Accepting Ourselves\n",
            "3. Accepting Suffering\n",
            "4. Accepting Other People\n",
            "5. Freedom and the Present Moment\n",
            "6. “To Love” Has a Present Tense Only\n",
            "7. We Can Suffer for Only One Moment\n",
            "8. “Let the Day’s Own Trouble Be Sufficient for the Day”\n",
            "9. Tomorrow Can Take Care of Itself\n",
            "10. Live, Instead of Waiting to Live\n",
            "11. Availability to Other People\n",
            "12. Psychological Time and Interior Time\n",
            "13. The Theological Virtues\n",
            "14. The Three Outpourings of the Holy Spirit\n",
            "15. Vocation and the Gift of Faith\n",
            "16. St. Peter’s Tears, and the Gift of Hope\n",
            "17. Pentecost and the Gift of Charity\n",
            "18. The Fire That Lights Up, Burns, and Transfigures\n",
            "19. The Dynamism of the Theological Virtues\n",
            "20. Love Needs Hope; Hope is Based on Faith\n",
            "21. The Key Role of Hope\n",
            "22. Dynamism of Sin, Dynamism of Grace\n",
            "23. Hope and Purity of Heart\n",
            "24. Law and Grace\n",
            "25. “Where the Spirit Rules, There is Freedom.” The Difference Between Freedom and Licentiousness\n",
            "26. The Trap of the Law\n",
            "27. Learning to Love: Giving and Receiving Freely\n",
            "28. The Need to Be\n",
            "29. Pride and Spiritual Poverty\n",
            "30. Spiritual Trials\n",
            "31. Relying on Mercy Alone\n",
            "32. The Truly Free Person is the One Who Has Nothing Left to Lose\n",
            "33. Happy Are the Poor\n",
            "\n",
            "Extracting chapter contents...\n",
            "Estimating page numbers...\n",
            "\n",
            "Generating 1 notes per chapter...\n",
            "\n",
            "Processing Chapter 1: The Search for Freedom\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 2: Accepting Ourselves\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 3: Accepting Suffering\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 4: Accepting Other People\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 5: Freedom and the Present Moment\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 6: “To Love” Has a Present Tense Only\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 7: We Can Suffer for Only One Moment\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 8: “Let the Day’s Own Trouble Be Sufficient for the Day”\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 9: Tomorrow Can Take Care of Itself\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 10: Live, Instead of Waiting to Live\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 11: Availability to Other People\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 12: Psychological Time and Interior Time\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 13: The Theological Virtues\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 14: The Three Outpourings of the Holy Spirit\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 15: Vocation and the Gift of Faith\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 16: St. Peter’s Tears, and the Gift of Hope\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 17: Pentecost and the Gift of Charity\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 18: The Fire That Lights Up, Burns, and Transfigures\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 19: The Dynamism of the Theological Virtues\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 20: Love Needs Hope; Hope is Based on Faith\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 21: The Key Role of Hope\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 22: Hope and Purity of Heart\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 23: Law and Grace\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 24: The Trap of the Law\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 25: Learning to Love: Giving and Receiving Freely\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 26: The Need to Be\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 27: Pride and Spiritual Poverty\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 28: Spiritual Trials\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 29: Relying on Mercy Alone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 30: The Truly Free Person is the One Who Has Nothing Left to Lose\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 31: Happy Are the Poor\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 32: Dynamism of Sin, Dynamism of Grace\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Creating downloadable ZIP archive...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e7178e50-25da-4681-937b-d56765983e0e\", \"evergreen_notes.zip\", 93364)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download started for evergreen_notes.zip\n",
            "\n",
            "Processing complete! Generated 32 notes across 32 chapters.\n",
            "Notes saved in the 'evergreen_notes' folder.\n"
          ]
        }
      ],
      "source": [
        "import PyPDF2 as pdf_library\n",
        "import openai\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from google.colab import files\n",
        "import getpass\n",
        "\n",
        "class EvergreenNotesGenerator:\n",
        "    def __init__(self, pdf_path, api_key, notes_per_chapter=3, use_mixed_models=True):\n",
        "        \"\"\"\n",
        "        Initialize the Evergreen Notes Generator\n",
        "\n",
        "        Args:\n",
        "            pdf_path (str): Path to the PDF file\n",
        "            api_key (str): OpenAI API key\n",
        "            notes_per_chapter (int): Number of notes to generate per chapter\n",
        "            use_mixed_models (bool): Whether to use mixed models for cost efficiency\n",
        "        \"\"\"\n",
        "        self.pdf_path = pdf_path\n",
        "        self.api_key = api_key\n",
        "        self.notes_per_chapter = notes_per_chapter\n",
        "        self.use_mixed_models = use_mixed_models\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "        # Models configuration\n",
        "        self.primary_model = \"gpt-4o\"\n",
        "        self.lighter_model = \"gpt-4o-mini\"  # For less intensive tasks\n",
        "\n",
        "        # Initialize output directory\n",
        "        self.output_dir = \"evergreen_notes\"\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def extract_initial_text(self, pages=10):\n",
        "        \"\"\"Extract text from initial pages (for TOC detection)\"\"\"\n",
        "        reader = pdf_library.PdfReader(self.pdf_path)\n",
        "        text = \"\"\n",
        "        for i in range(min(pages, len(reader.pages))):\n",
        "            text += reader.pages[i].extract_text() + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def get_toc_from_llm(self, initial_text):\n",
        "        \"\"\"Use LLM to extract table of contents\"\"\"\n",
        "        model = self.lighter_model  # Use lighter model for TOC extraction\n",
        "\n",
        "        prompt = f\"\"\"You're given the initial pages of a book. Clearly list ONLY the chapters from the table of contents exactly as they appear. Number them clearly.\n",
        "\n",
        "TEXT:\n",
        "{initial_text}\n",
        "\n",
        "Return ONLY the numbered list of chapters.\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"Extract chapters accurately from tables of contents.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        toc_raw = response.choices[0].message.content.strip()\n",
        "        chapters = [re.sub(r'^\\d+[.)]\\s*', '', line).strip() for line in toc_raw.splitlines() if line.strip()]\n",
        "        return chapters\n",
        "\n",
        "    def split_chapters(self, chapters):\n",
        "        \"\"\"Split PDF text by chapters using exact chapter titles\"\"\"\n",
        "        reader = pdf_library.PdfReader(self.pdf_path)\n",
        "\n",
        "        # First extract full text\n",
        "        full_text = \"\"\n",
        "        for page in reader.pages:\n",
        "            full_text += page.extract_text() + \"\\n\"\n",
        "\n",
        "        # Map chapters to their positions in the text\n",
        "        splits = {}\n",
        "        for idx, chapter in enumerate(chapters):\n",
        "            pattern = re.escape(chapter)\n",
        "            match = re.search(pattern, full_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                splits[match.start()] = chapter\n",
        "\n",
        "        # Sort by position and extract chapter contents\n",
        "        sorted_splits = sorted(splits.items())\n",
        "        chapter_contents = {}\n",
        "\n",
        "        for i, (start_pos, title) in enumerate(sorted_splits):\n",
        "            end_pos = sorted_splits[i+1][0] if i+1 < len(sorted_splits) else len(full_text)\n",
        "            chapter_contents[title] = full_text[start_pos:end_pos].strip()\n",
        "\n",
        "        return chapter_contents\n",
        "\n",
        "    def estimate_page_numbers(self, chapter_titles):\n",
        "        \"\"\"Estimate page numbers where each chapter begins\"\"\"\n",
        "        reader = pdf_library.PdfReader(self.pdf_path)\n",
        "        page_numbers = {}\n",
        "\n",
        "        # For each page, check if it contains any chapter title\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page_text = reader.pages[page_num].extract_text()\n",
        "\n",
        "            for title in chapter_titles:\n",
        "                if title in page_text:\n",
        "                    page_numbers[title] = page_num + 1  # 1-based page numbers\n",
        "                    break\n",
        "\n",
        "        return page_numbers\n",
        "\n",
        "    def generate_evergreen_notes(self, chapter_title, chapter_content):\n",
        "        \"\"\"Generate evergreen notes for a single chapter using structured outputs\"\"\"\n",
        "        if self.use_mixed_models:\n",
        "            note_model = self.primary_model\n",
        "            title_model = self.lighter_model\n",
        "        else:\n",
        "            note_model = title_model = self.primary_model\n",
        "\n",
        "        # Make sure we don't try to generate more notes than requested\n",
        "        notes_to_generate = min(self.notes_per_chapter, 1)\n",
        "\n",
        "        # Comprehensive prompt for generating high-quality evergreen notes\n",
        "        prompt = f\"\"\"From the following chapter content, generate exactly {notes_to_generate} insightful evergreen notes that follow the principles of \"evergreen notes\" by Andy Matuschak.\n",
        "\n",
        "CHAPTER: {chapter_title}\n",
        "\n",
        "CONTENT:\n",
        "{chapter_content[:50000]}\n",
        "\n",
        "EVERGREEN NOTES PRINCIPLES:\n",
        "1. Evergreen notes should be atomic (about one thing, and one thing only)\n",
        "2. Evergreen notes should be concept-oriented (focused on ideas rather than facts)\n",
        "3. Evergreen notes should be densely linked (though this is handled elsewhere in our system)\n",
        "4. Evergreen notes should make assertions (they should have a clear opinion/stance)\n",
        "\n",
        "DETAILED INSTRUCTIONS:\n",
        "- Create notes that could be valuable to revisit years from now\n",
        "- Focus on timeless principles, mental models, and insights rather than specific examples or temporary facts\n",
        "- Each note should capture one complete idea in sufficient detail to make sense on its own\n",
        "- Phrase notes as complete statements, not as topics or questions\n",
        "- Make them precise, clear, and with a strong \"insight density\"\n",
        "- Avoid superficial observations or simple summaries - dig deeper for insights\n",
        "- Use clear, accessible language that will be understandable without the original context\n",
        "- Include enough context so the note can stand alone\n",
        "- Prioritize unique, counter-intuitive or profound insights over obvious ones\n",
        "\n",
        "You MUST generate EXACTLY {notes_to_generate} notes, no more and no less.\n",
        "\n",
        "Return your response in JSON format with an array of notes. For example if generating one note:\n",
        "{{\"notes\": [\n",
        "  {{\"content\": \"Your note content here\"}}\n",
        "]}}\n",
        "\n",
        "Or if generating multiple notes:\n",
        "{{\"notes\": [\n",
        "  {{\"content\": \"First note content here\"}},\n",
        "  {{\"content\": \"Second note content here\"}},\n",
        "  {{\"content\": \"Third note content here\"}}\n",
        "]}}\"\"\"\n",
        "\n",
        "        # Generate notes\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=note_model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at knowledge synthesis and creating high-quality evergreen notes in the style of Andy Matuschak. You extract deep, nuanced insights from content and express them as standalone, atomic conceptual units that will remain valuable over time.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            # Parse the JSON response\n",
        "            try:\n",
        "                response_content = response.choices[0].message.content\n",
        "                notes_data = json.loads(response_content)\n",
        "\n",
        "                # Handle different possible response formats\n",
        "                if isinstance(notes_data, list):\n",
        "                    notes = [note.get(\"content\", \"\") for note in notes_data]\n",
        "                elif \"notes\" in notes_data and isinstance(notes_data[\"notes\"], list):\n",
        "                    notes = [note.get(\"content\", \"\") for note in notes_data[\"notes\"]]\n",
        "                else:\n",
        "                    # If we can't find notes in the expected format, look for any content field\n",
        "                    notes = []\n",
        "                    for key, value in notes_data.items():\n",
        "                        if isinstance(value, dict) and \"content\" in value:\n",
        "                            notes.append(value[\"content\"])\n",
        "                        elif isinstance(value, list):\n",
        "                            for item in value:\n",
        "                                if isinstance(item, dict) and \"content\" in item:\n",
        "                                    notes.append(item[\"content\"])\n",
        "\n",
        "                # If we still have no notes, create a fallback\n",
        "                if not notes:\n",
        "                    print(f\"Warning: Could not parse notes from response: {response_content[:100]}...\")\n",
        "                    notes = [f\"Note for chapter: {chapter_title}\" for _ in range(notes_to_generate)]\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing JSON: {e}\")\n",
        "                print(f\"Raw response: {response_content[:100]}...\")\n",
        "                notes = [f\"Note for chapter: {chapter_title}\" for _ in range(self.notes_per_chapter)]\n",
        "\n",
        "            # Handle edge case where fewer notes were returned than requested\n",
        "            if len(notes) < self.notes_per_chapter:\n",
        "                print(f\"Warning: Only received {len(notes)} notes instead of {self.notes_per_chapter}.\")\n",
        "                # Pad with empty notes if necessary\n",
        "                while len(notes) < self.notes_per_chapter:\n",
        "                    notes.append(\"Note content unavailable.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating notes: {e}\")\n",
        "            # Try to directly extract content if possible\n",
        "            try:\n",
        "                content = response.choices[0].message.content\n",
        "                if content and len(content) > 50:  # If we got some substantial content\n",
        "                    # Just use the raw content as a single note\n",
        "                    notes = [content]\n",
        "                    # Pad with empty notes if necessary\n",
        "                    while len(notes) < self.notes_per_chapter:\n",
        "                        notes.append(f\"Additional note for chapter: {chapter_title}\")\n",
        "                else:\n",
        "                    # Fallback: generate generic placeholder notes\n",
        "                    notes = [f\"Note for chapter: {chapter_title}\" for _ in range(notes_to_generate)]\n",
        "            except:\n",
        "                # Fallback if all else fails\n",
        "                notes = [f\"Note for chapter: {chapter_title}\" for _ in range(notes_to_generate)]\n",
        "\n",
        "        # Generate titles for each note\n",
        "        titled_notes = []\n",
        "        for note in notes:\n",
        "            # Generate a title using the lighter model to save costs\n",
        "            title_prompt = f\"\"\"Create a concise, descriptive title (3-7 words) for this evergreen note:\n",
        "\n",
        "NOTE CONTENT: {note}\n",
        "\n",
        "TITLE GUIDELINES:\n",
        "- Make it specific enough to distinguish it from other notes\n",
        "- Use language that resonates with the note's key insight\n",
        "- Phrase it as a noun phrase or complete thought, not a question\n",
        "- Avoid generic words like \"Introduction\" or \"Overview\"\n",
        "- Include the most distinctive concept from the note\n",
        "- Make it memorable and easy to reference\n",
        "- Aim for clarity over cleverness\n",
        "\n",
        "Respond with a JSON object containing only a 'title' field, like this:\n",
        "{{\"title\": \"Your Title Here\"}}\"\"\"\n",
        "\n",
        "            try:\n",
        "                title_response = self.client.chat.completions.create(\n",
        "                    model=title_model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an expert at creating precise, distinctive titles for evergreen notes in the Zettelkasten tradition. Your titles are specific, conceptual, and optimized for future discoverability.\"},\n",
        "                        {\"role\": \"user\", \"content\": title_prompt}\n",
        "                    ],\n",
        "                    response_format={\"type\": \"json_object\"},\n",
        "                    temperature=0.5\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    title_data = json.loads(title_response.choices[0].message.content)\n",
        "                    title = title_data.get(\"title\", f\"Note on {chapter_title}\")\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON parsing fails, try to extract title with regex\n",
        "                    content = title_response.choices[0].message.content\n",
        "                    match = re.search(r'\"title\"\\s*:\\s*\"([^\"]+)\"', content)\n",
        "                    if match:\n",
        "                        title = match.group(1)\n",
        "                    else:\n",
        "                        title = f\"Note on {chapter_title}\"\n",
        "                    print(f\"Extracted title: {title}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating title: {e}\")\n",
        "                title = f\"Note on {chapter_title}\"\n",
        "\n",
        "            titled_notes.append({\"title\": title, \"content\": note})\n",
        "\n",
        "        # Return exactly the requested number of notes\n",
        "        return titled_notes[:self.notes_per_chapter]\n",
        "\n",
        "    def save_notes(self, chapter_contents, page_numbers):\n",
        "        \"\"\"Process all chapters and save notes\"\"\"\n",
        "        all_notes = []\n",
        "        chapter_metadata = []\n",
        "\n",
        "        # Create index file\n",
        "        index_path = os.path.join(self.output_dir, \"00_notes_index.md\")\n",
        "        with open(index_path, \"w\", encoding=\"utf-8\") as index_file:\n",
        "            index_file.write(\"# Evergreen Notes Index\\n\\n\")\n",
        "\n",
        "            # Process each chapter\n",
        "            for i, (chapter_title, content) in enumerate(chapter_contents.items(), 1):\n",
        "                print(f\"\\nProcessing Chapter {i}: {chapter_title}\")\n",
        "                page = page_numbers.get(chapter_title, \"Unknown\")\n",
        "                chapter_metadata.append({\"number\": i, \"title\": chapter_title, \"page\": page})\n",
        "\n",
        "                # Generate notes for this chapter\n",
        "                try:\n",
        "                    chapter_notes = self.generate_evergreen_notes(chapter_title, content)\n",
        "\n",
        "                    # Add chapter info to notes\n",
        "                    for note in chapter_notes:\n",
        "                        note[\"chapter\"] = chapter_title\n",
        "                        note[\"chapter_number\"] = i\n",
        "                        note[\"page\"] = page\n",
        "                        all_notes.append(note)\n",
        "\n",
        "                    # Add chapter header to the index file\n",
        "                    index_file.write(f\"### Chapter {i}: {chapter_title}\\n\\n\")\n",
        "\n",
        "                    for j, note in enumerate(chapter_notes, 1):\n",
        "                        # Combine chapter number, chapter title, and note title for the filename\n",
        "                        combined_title = f\"Chapter {i}: {chapter_title} - {note['title']}\"\n",
        "                        note_filename = f\"{i:02d}_{j:02d}_{self.safe_filename(combined_title)}.md\"\n",
        "\n",
        "                        # Only write exactly the number of notes per chapter requested\n",
        "                        if j <= self.notes_per_chapter:\n",
        "                            index_file.write(f\"{i}.{j} [{combined_title}](./{note_filename})\\n\\n\")\n",
        "\n",
        "                            # Save individual note file\n",
        "                            note_path = os.path.join(self.output_dir, note_filename)\n",
        "                            with open(note_path, \"w\", encoding=\"utf-8\") as note_file:\n",
        "                                note_file.write(f\"# {combined_title}\\n\\n\")\n",
        "                                note_file.write(f\"*Page {page}*\\n\\n\")\n",
        "                                note_file.write(note['content'])\n",
        "                                note_file.write(\"\\n\")\n",
        "\n",
        "                    print(f\"  Generated {len(chapter_notes)} notes\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error generating notes for chapter: {e}\")\n",
        "\n",
        "        # Create metadata file\n",
        "        meta_path = os.path.join(self.output_dir, \"book_metadata.md\")\n",
        "        with open(meta_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
        "            meta_file.write(f\"# Book: {os.path.basename(self.pdf_path)}\\n\\n\")\n",
        "            meta_file.write(f\"* Total Chapters: {len(chapter_contents)}\\n\")\n",
        "            meta_file.write(f\"* Total Notes: {len(all_notes)}\\n\\n\")\n",
        "\n",
        "            meta_file.write(\"## Chapter List\\n\\n\")\n",
        "            for chapter in chapter_metadata:\n",
        "                meta_file.write(f\"{chapter['number']}. {chapter['title']} (Page {chapter['page']})\\n\")\n",
        "\n",
        "        return len(all_notes)\n",
        "\n",
        "    def safe_filename(self, text):\n",
        "        \"\"\"Convert text to a safe filename\"\"\"\n",
        "        # Remove unsafe chars and replace spaces with underscores\n",
        "        safe = re.sub(r'[^\\w\\s-]', '', text).strip().replace(' ', '_')\n",
        "        # Limit length to avoid overly long filenames\n",
        "        return safe[:50]\n",
        "\n",
        "    def create_zip_archive(self):\n",
        "        \"\"\"Create a downloadable ZIP archive of notes\"\"\"\n",
        "        # Create a zip file\n",
        "        zip_filename = f\"{self.output_dir}.zip\"\n",
        "\n",
        "        # Use shell command to create zip (safer in Colab)\n",
        "        os.system(f\"zip -r {zip_filename} {self.output_dir}\")\n",
        "\n",
        "        # Make files available for download in Colab\n",
        "        try:\n",
        "            files.download(zip_filename)\n",
        "            print(f\"Download started for {zip_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during download: {e}\")\n",
        "            print(\"If download doesn't start automatically, use the file browser to download the files.\")\n",
        "\n",
        "    def process_book(self):\n",
        "        \"\"\"Main processing function\"\"\"\n",
        "        print(f\"Processing {self.pdf_path}...\")\n",
        "\n",
        "        # Step 1: Extract initial text for TOC analysis\n",
        "        initial_text = self.extract_initial_text()\n",
        "\n",
        "        # Step 2: Use LLM to extract chapter list\n",
        "        print(\"Extracting chapters using OpenAI...\")\n",
        "        chapters = self.get_toc_from_llm(initial_text)\n",
        "\n",
        "        print(f\"\\nFound {len(chapters)} chapters:\")\n",
        "        for idx, chap in enumerate(chapters, 1):\n",
        "            print(f\"{idx}. {chap}\")\n",
        "\n",
        "        # Step 3: Split content by chapters\n",
        "        print(\"\\nExtracting chapter contents...\")\n",
        "        chapter_contents = self.split_chapters(chapters)\n",
        "\n",
        "        # Step 4: Estimate page numbers\n",
        "        print(\"Estimating page numbers...\")\n",
        "        page_numbers = self.estimate_page_numbers(chapters)\n",
        "\n",
        "        # Step 5: Generate and save notes\n",
        "        print(f\"\\nGenerating {self.notes_per_chapter} notes per chapter...\")\n",
        "        total_notes = self.save_notes(chapter_contents, page_numbers)\n",
        "\n",
        "        # Step 6: Create zip archive\n",
        "        print(\"\\nCreating downloadable ZIP archive...\")\n",
        "        self.create_zip_archive()\n",
        "\n",
        "        print(f\"\\nProcessing complete! Generated {total_notes} notes across {len(chapter_contents)} chapters.\")\n",
        "        print(f\"Notes saved in the '{self.output_dir}' folder.\")\n",
        "\n",
        "\n",
        "def run_in_colab(pdf_path, api_key, notes_per_chapter=3, use_mixed_models=True):\n",
        "    \"\"\"Run the Evergreen Notes Generator in Google Colab\"\"\"\n",
        "    generator = EvergreenNotesGenerator(\n",
        "        pdf_path=pdf_path,\n",
        "        api_key=api_key,\n",
        "        notes_per_chapter=notes_per_chapter,\n",
        "        use_mixed_models=use_mixed_models\n",
        "    )\n",
        "    generator.process_book()\n",
        "\n",
        "\n",
        "# Main execution code\n",
        "if __name__ == \"__main__\":\n",
        "    from google.colab import files\n",
        "    import os\n",
        "\n",
        "    try:\n",
        "        # Try to import PyPDF2, and if that fails, try pypdf\n",
        "        import PyPDF2 as pdf_library\n",
        "    except ImportError:\n",
        "        try:\n",
        "            import pypdf as pdf_library\n",
        "        except ImportError:\n",
        "            # If neither is available, we'll need to install one\n",
        "            import pip\n",
        "            pip.main(['install', 'PyPDF2'])\n",
        "            import PyPDF2 as pdf_library\n",
        "\n",
        "    # Upload PDF file\n",
        "    print(\"Please upload your PDF book:\")\n",
        "    uploaded = files.upload()\n",
        "    pdf_filename = list(uploaded.keys())[0]\n",
        "\n",
        "    # Get OpenAI API key\n",
        "    api_key = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "    # Set number of notes per chapter\n",
        "    notes_per_chapter = int(input(\"How many notes per chapter? (default: 3): \") or \"3\")\n",
        "\n",
        "    # Ask if user wants to use mixed models\n",
        "    use_mixed = input(\"Use GPT-4o-mini for title generation to save costs? (y/n, default: y): \").lower() != 'n'\n",
        "\n",
        "    # Run the generator\n",
        "    print(f\"Starting to process {pdf_filename} with {notes_per_chapter} notes per chapter...\")\n",
        "    run_in_colab(pdf_filename, api_key, notes_per_chapter=notes_per_chapter, use_mixed_models=use_mixed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2 as pdf_library\n",
        "import openai\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from google.colab import files\n",
        "import getpass\n",
        "\n",
        "class EvergreenNotesGenerator:\n",
        "    def __init__(self, pdf_path, api_key, notes_per_chapter=3, use_mixed_models=True):\n",
        "        \"\"\"\n",
        "        Initialize the Evergreen Notes Generator\n",
        "\n",
        "        Args:\n",
        "            pdf_path (str): Path to the PDF file\n",
        "            api_key (str): OpenAI API key\n",
        "            notes_per_chapter (int): Number of notes to generate per chapter\n",
        "            use_mixed_models (bool): Whether to use mixed models for cost efficiency\n",
        "        \"\"\"\n",
        "        self.pdf_path = pdf_path\n",
        "        self.api_key = api_key\n",
        "        self.notes_per_chapter = notes_per_chapter\n",
        "        self.use_mixed_models = use_mixed_models\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "        # Models configuration\n",
        "        self.primary_model = \"gpt-4o\"\n",
        "        self.lighter_model = \"gpt-4o-mini\"  # For less intensive tasks\n",
        "\n",
        "        # Get book title from filename\n",
        "        book_title = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "        # Clean up book title for folder name\n",
        "        self.book_folder = re.sub(r'[^\\w\\s-]', '', book_title).strip().replace(' ', '_')\n",
        "\n",
        "        # Create a timestamp suffix to ensure unique folder names\n",
        "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Initialize output directory with book name and timestamp\n",
        "        self.output_dir = f\"evergreen_notes_{self.book_folder}_{timestamp}\"\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "    def extract_initial_text(self, pages=10):\n",
        "        \"\"\"Extract text from initial pages (for TOC detection)\"\"\"\n",
        "        reader = pdf_library.PdfReader(self.pdf_path)\n",
        "        text = \"\"\n",
        "        for i in range(min(pages, len(reader.pages))):\n",
        "            text += reader.pages[i].extract_text() + \"\\n\"\n",
        "        return text\n",
        "\n",
        "    def get_toc_from_llm(self, initial_text):\n",
        "        \"\"\"Use LLM to extract table of contents\"\"\"\n",
        "        model = self.lighter_model  # Use lighter model for TOC extraction\n",
        "\n",
        "        prompt = f\"\"\"You're given the initial pages of a book. Clearly list ONLY the chapters from the table of contents exactly as they appear. Number them clearly.\n",
        "\n",
        "TEXT:\n",
        "{initial_text}\n",
        "\n",
        "Return ONLY the numbered list of chapters.\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"system\", \"content\": \"Extract chapters accurately from tables of contents.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        toc_raw = response.choices[0].message.content.strip()\n",
        "        chapters = [re.sub(r'^\\d+[.)]\\s*', '', line).strip() for line in toc_raw.splitlines() if line.strip()]\n",
        "        return chapters\n",
        "\n",
        "    def split_chapters(self, chapters):\n",
        "\n",
        "      reader = pdf_library.PdfReader(self.pdf_path)\n",
        "      total_pages = len(reader.pages)\n",
        "      print(f\"PDF has {total_pages} pages\")\n",
        "\n",
        "      # First, find the approximate page numbers for each chapter\n",
        "      chapter_pages = {}\n",
        "      for page_num in range(total_pages):\n",
        "          page_text = reader.pages[page_num].extract_text()\n",
        "\n",
        "          for chapter in chapters:\n",
        "              if chapter in page_text and chapter not in chapter_pages:\n",
        "                  chapter_pages[chapter] = page_num\n",
        "                  print(f\"Found chapter '{chapter}' on page {page_num+1}\")\n",
        "\n",
        "      # Sort chapters by their order in the TOC (not by page number)\n",
        "      sorted_chapters = []\n",
        "      for chapter in chapters:\n",
        "          if chapter in chapter_pages:\n",
        "              sorted_chapters.append(chapter)\n",
        "\n",
        "      missing_chapters = [c for c in chapters if c not in chapter_pages]\n",
        "      if missing_chapters:\n",
        "          print(f\"Warning: Could not find these chapters: {', '.join(missing_chapters)}\")\n",
        "\n",
        "      # Extract content for each chapter\n",
        "      chapter_contents = {}\n",
        "      for i, chapter in enumerate(sorted_chapters):\n",
        "          start_page = chapter_pages[chapter]\n",
        "\n",
        "          # For the end page, find the next chapter or use the end of the document\n",
        "          if i < len(sorted_chapters) - 1:\n",
        "              next_chapter = sorted_chapters[i+1]\n",
        "              end_page = chapter_pages[next_chapter]\n",
        "          else:\n",
        "              end_page = total_pages\n",
        "\n",
        "          # Extract text from the chapter's pages\n",
        "          chapter_text = \"\"\n",
        "          for page_num in range(start_page, end_page):\n",
        "              chapter_text += reader.pages[page_num].extract_text() + \"\\n\"\n",
        "\n",
        "          chapter_contents[chapter] = chapter_text\n",
        "          print(f\"Extracted content for '{chapter}' from pages {start_page+1}-{end_page}\")\n",
        "\n",
        "      print(f\"Successfully processed {len(chapter_contents)}/{len(chapters)} chapters\")\n",
        "      return chapter_contents\n",
        "\n",
        "    def generate_evergreen_notes(self, chapter_title, chapter_content):\n",
        "        \"\"\"Generate evergreen notes for a single chapter using structured outputs\"\"\"\n",
        "        if self.use_mixed_models:\n",
        "            note_model = self.primary_model\n",
        "            title_model = self.lighter_model\n",
        "        else:\n",
        "            note_model = title_model = self.primary_model\n",
        "\n",
        "        # Make sure we don't try to generate more notes than requested\n",
        "        notes_to_generate = min(self.notes_per_chapter, 1)\n",
        "\n",
        "        # Comprehensive prompt for generating high-quality evergreen notes\n",
        "        prompt = f\"\"\"Generate exactly {notes_to_generate} evergreen notes following Andy Matuschak's approach from the following chapter content.\n",
        "\n",
        "CHAPTER: {chapter_title}\n",
        "\n",
        "CONTENT:\n",
        "{chapter_content[:50000]}\n",
        "\n",
        "EVERGREEN NOTE PRINCIPLES:\n",
        "1. Each note should be atomic - about one thing and one thing only, capturing that concept completely\n",
        "2. Each note should be concept-oriented rather than fact-oriented\n",
        "3. Each note should make a clear, declarative assertion\n",
        "4. Notes should be written for yourself, not for an audience\n",
        "5. Notes should be densely linked (though links will be added elsewhere)\n",
        "\n",
        "DETAILED INSTRUCTIONS:\n",
        "- Create notes that contain a single, clear, complete idea that will remain valuable for years\n",
        "- Focus on timeless principles and mental models, not temporary facts\n",
        "- Each note should represent a unit of knowledge that can stand on its own\n",
        "- Aim for insight density - avoid obvious observations or simple summaries\n",
        "- Write from a position of understanding, not just collecting information\n",
        "- Make the note complete enough that your future self would understand it without the book\n",
        "\n",
        "You MUST generate EXACTLY {notes_to_generate} notes, no more and no less.\n",
        "\n",
        "Return your response in JSON format with an array of notes. For example:\n",
        "{{\"notes\": [\n",
        "  {{\"content\": \"Your note content here - several paragraphs that fully explore the idea\"}}\n",
        "]}}\"\"\"\n",
        "\n",
        "        # Generate notes\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=note_model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at knowledge synthesis and creating high-quality evergreen notes in the style of Andy Matuschak. You extract deep, nuanced insights from content and express them as standalone, atomic conceptual units that will remain valuable over time.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.7\n",
        "            )\n",
        "\n",
        "            # Parse the JSON response\n",
        "            try:\n",
        "                response_content = response.choices[0].message.content\n",
        "                notes_data = json.loads(response_content)\n",
        "\n",
        "                # Handle different possible response formats\n",
        "                if isinstance(notes_data, list):\n",
        "                    notes = [note.get(\"content\", \"\") for note in notes_data]\n",
        "                elif \"notes\" in notes_data and isinstance(notes_data[\"notes\"], list):\n",
        "                    notes = [note.get(\"content\", \"\") for note in notes_data[\"notes\"]]\n",
        "                else:\n",
        "                    # If we can't find notes in the expected format, look for any content field\n",
        "                    notes = []\n",
        "                    for key, value in notes_data.items():\n",
        "                        if isinstance(value, dict) and \"content\" in value:\n",
        "                            notes.append(value[\"content\"])\n",
        "                        elif isinstance(value, list):\n",
        "                            for item in value:\n",
        "                                if isinstance(item, dict) and \"content\" in item:\n",
        "                                    notes.append(item[\"content\"])\n",
        "\n",
        "                # If we still have no notes, create a fallback\n",
        "                if not notes:\n",
        "                    print(f\"Warning: Could not parse notes from response: {response_content[:100]}...\")\n",
        "                    notes = [f\"Note for chapter: {chapter_title}\" for _ in range(notes_to_generate)]\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing JSON: {e}\")\n",
        "                print(f\"Raw response: {response_content[:100]}...\")\n",
        "                notes = [f\"Note for chapter: {chapter_title}\" for _ in range(self.notes_per_chapter)]\n",
        "\n",
        "            # Handle edge case where fewer notes were returned than requested\n",
        "            if len(notes) < self.notes_per_chapter:\n",
        "                print(f\"Warning: Only received {len(notes)} notes instead of {self.notes_per_chapter}.\")\n",
        "                # Pad with empty notes if necessary\n",
        "                while len(notes) < self.notes_per_chapter:\n",
        "                    notes.append(\"Note content unavailable.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating notes: {e}\")\n",
        "            # Try to directly extract content if possible\n",
        "            try:\n",
        "                content = response.choices[0].message.content\n",
        "                if content and len(content) > 50:  # If we got some substantial content\n",
        "                    # Just use the raw content as a single note\n",
        "                    notes = [content]\n",
        "                    # Pad with empty notes if necessary\n",
        "                    while len(notes) < self.notes_per_chapter:\n",
        "                        notes.append(f\"Additional note for chapter: {chapter_title}\")\n",
        "                else:\n",
        "                    # Fallback: generate generic placeholder notes\n",
        "                    notes = [f\"Note for chapter: {chapter_title}\" for _ in range(notes_to_generate)]\n",
        "            except:\n",
        "                # Fallback if all else fails\n",
        "                notes = [f\"Note for chapter: {chapter_title}\" for _ in range(notes_to_generate)]\n",
        "\n",
        "        # Generate titles for each note\n",
        "        titled_notes = []\n",
        "        for note in notes:\n",
        "            # Generate a title using the lighter model to save costs\n",
        "            title_prompt = f\"\"\"Create a title for an evergreen note following Andy Matuschak's approach.\n",
        "\n",
        "NOTE CONTENT: {note}\n",
        "\n",
        "TITLE REQUIREMENTS:\n",
        "1. Phrase as a complete declarative statement that makes a clear assertion\n",
        "2. The title should function as an \"API\" for the note's content\n",
        "3. Be specific and precise enough that someone could understand the core idea just from the title\n",
        "4. Use active phrasing that emphasizes the claim being made\n",
        "5. The title should be memorable and distinctive\n",
        "6. Prefer positive framing over negative framing\n",
        "7. Include the most distinctive concept from the note\n",
        "8. Avoid questions or vague phrasings like \"Introduction to X\" or \"Notes on Y\"\n",
        "\n",
        "Examples of excellent evergreen note titles:\n",
        "- \"Enabling environments focus on doing things rather than understanding things\"\n",
        "- \"Evergreen notes permit smooth incremental progress in writing\"\n",
        "- \"Spaced repetition systems can be used to program attention\"\n",
        "- \"People seem to have different intrinsic capacities for different kinds of thinking\"\n",
        "- \"Knowledge work should accrete\"\n",
        "\n",
        "Respond with a JSON object containing only a 'title' field, like this:\n",
        "{{\"title\": \"Your declarative title that serves as an API for the note\"}}\"\"\"\n",
        "\n",
        "            try:\n",
        "                title_response = self.client.chat.completions.create(\n",
        "                    model=title_model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an expert at creating precise, distinctive titles for evergreen notes in the Zettelkasten tradition. Your titles are specific, conceptual, and optimized for future discoverability.\"},\n",
        "                        {\"role\": \"user\", \"content\": title_prompt}\n",
        "                    ],\n",
        "                    response_format={\"type\": \"json_object\"},\n",
        "                    temperature=0.5\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    title_data = json.loads(title_response.choices[0].message.content)\n",
        "                    title = title_data.get(\"title\", f\"Note on {chapter_title}\")\n",
        "                except json.JSONDecodeError:\n",
        "                    # If JSON parsing fails, try to extract title with regex\n",
        "                    content = title_response.choices[0].message.content\n",
        "                    match = re.search(r'\"title\"\\s*:\\s*\"([^\"]+)\"', content)\n",
        "                    if match:\n",
        "                        title = match.group(1)\n",
        "                    else:\n",
        "                        title = f\"Note on {chapter_title}\"\n",
        "                    print(f\"Extracted title: {title}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating title: {e}\")\n",
        "                title = f\"Note on {chapter_title}\"\n",
        "\n",
        "            titled_notes.append({\"title\": title, \"content\": note})\n",
        "\n",
        "        # Return exactly the requested number of notes\n",
        "        return titled_notes[:self.notes_per_chapter]\n",
        "\n",
        "    def save_notes(self, chapter_contents, page_numbers):\n",
        "        \"\"\"Process all chapters and save notes\"\"\"\n",
        "        all_notes = []\n",
        "        chapter_metadata = []\n",
        "\n",
        "        # Initialize consolidated content with header\n",
        "        consolidated_content = []\n",
        "        consolidated_content.append(f\"# Evergreen Notes from {os.path.basename(self.pdf_path)}\\n\")\n",
        "        consolidated_content.append(f\"Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "        # Create index file\n",
        "        index_path = os.path.join(self.output_dir, \"00_index.md\")\n",
        "        with open(index_path, \"w\", encoding=\"utf-8\") as index_file:\n",
        "            index_file.write(\"# Evergreen Notes\\n\\n\")\n",
        "            index_file.write(f\"Source: {os.path.basename(self.pdf_path)}\\n\\n\")\n",
        "            index_file.write(f\"Created: {time.strftime('%Y-%m-%d')}\\n\\n\")\n",
        "            index_file.write(\"## Notes\\n\\n\")\n",
        "\n",
        "            # Process each chapter\n",
        "            for i, (chapter_title, content) in enumerate(chapter_contents.items(), 1):\n",
        "                print(f\"\\nProcessing Chapter {i}: {chapter_title}\")\n",
        "                page = page_numbers.get(chapter_title, \"Unknown\")\n",
        "                chapter_metadata.append({\"number\": i, \"title\": chapter_title, \"page\": page})\n",
        "\n",
        "                # Add chapter header to index file once\n",
        "                index_file.write(f\"### Chapter {i}: {chapter_title}\\n\\n\")\n",
        "                consolidated_content.append(f\"## Chapter {i}: {chapter_title} (Page {page})\\n\\n\")\n",
        "\n",
        "                # Generate notes for this chapter\n",
        "                try:\n",
        "                    chapter_notes = self.generate_evergreen_notes(chapter_title, content)\n",
        "\n",
        "                    for j, note in enumerate(chapter_notes, 1):\n",
        "                        note[\"chapter\"] = chapter_title\n",
        "                        note[\"chapter_number\"] = i\n",
        "                        note[\"page\"] = page\n",
        "                        all_notes.append(note)\n",
        "\n",
        "                        combined_title = f\"Chapter {i}: {chapter_title} - {note['title']}\"\n",
        "                        note_filename = f\"{i:02d}_{j:02d}_{self.safe_filename(combined_title)}.md\"\n",
        "\n",
        "                        if j <= self.notes_per_chapter:\n",
        "                            # Write to index file\n",
        "                            index_file.write(f\"{i}.{j} [{combined_title}](./{note_filename})\\n\\n\")\n",
        "\n",
        "                            # Save individual note file\n",
        "                            note_path = os.path.join(self.output_dir, note_filename)\n",
        "                            with open(note_path, \"w\", encoding=\"utf-8\") as note_file:\n",
        "                                note_file.write(f\"# {combined_title}\\n\\n\")\n",
        "                                note_file.write(f\"*Page {page}*\\n\\n\")\n",
        "                                note_file.write(note['content'])\n",
        "                                note_file.write(\"\\n\")\n",
        "\n",
        "                            # Append note content to consolidated files\n",
        "                            consolidated_content.append(f\"### {combined_title}\\n\\n\")\n",
        "                            consolidated_content.append(note['content'] + \"\\n\\n\")\n",
        "\n",
        "                    print(f\"  Generated {len(chapter_notes)} notes\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error generating notes for chapter: {e}\")\n",
        "\n",
        "        # Create metadata file\n",
        "        meta_path = os.path.join(self.output_dir, \"book_metadata.md\")\n",
        "        with open(meta_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
        "            book_filename = os.path.basename(self.pdf_path)\n",
        "            meta_file.write(f\"# Book: {book_filename}\\n\\n\")\n",
        "            meta_file.write(f\"* Process Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            meta_file.write(f\"* Total Chapters: {len(chapter_contents)}\\n\")\n",
        "            meta_file.write(f\"* Total Notes: {len(all_notes)}\\n\\n\")\n",
        "\n",
        "            meta_file.write(\"## Chapter List\\n\\n\")\n",
        "            for chapter in chapter_metadata:\n",
        "                meta_file.write(f\"{chapter['number']}. {chapter['title']} (Page {chapter['page']})\\n\")\n",
        "\n",
        "        # Create consolidated notes files with actual content\n",
        "        consolidated_path = os.path.join(self.output_dir, \"all_notes.md\")\n",
        "        with open(consolidated_path, \"w\", encoding=\"utf-8\") as consolidated_file:\n",
        "            consolidated_file.write(\"\\n\".join(consolidated_content))\n",
        "\n",
        "        consolidated_txt_path = os.path.join(self.output_dir, \"all_notes.txt\")\n",
        "        with open(consolidated_txt_path, \"w\", encoding=\"utf-8\") as consolidated_txt_file:\n",
        "            consolidated_txt_file.write(\"\\n\".join(consolidated_content))\n",
        "\n",
        "        return len(all_notes)\n",
        "\n",
        "\n",
        "    def safe_filename(self, text):\n",
        "        \"\"\"Convert text to a safe filename\"\"\"\n",
        "        # Remove unsafe chars and replace spaces with underscores\n",
        "        # First truncate to avoid very long text processing\n",
        "        truncated = text[:100] if text else \"Untitled\"\n",
        "        safe = re.sub(r'[^\\w\\s-]', '', truncated).strip().replace(' ', '_')\n",
        "        # Further limit length to avoid overly long filenames\n",
        "        return safe[:80]\n",
        "\n",
        "    def create_zip_archive(self):\n",
        "        \"\"\"Create a downloadable ZIP archive of notes\"\"\"\n",
        "        # Create a zip file with the same name as the output directory\n",
        "        zip_filename = f\"{self.output_dir}.zip\"\n",
        "\n",
        "        # Use shell command to create zip (safer in Colab)\n",
        "        os.system(f\"zip -r {zip_filename} {self.output_dir}\")\n",
        "\n",
        "        # Make files available for download in Colab\n",
        "        try:\n",
        "            files.download(zip_filename)\n",
        "            print(f\"Download started for {zip_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during download: {e}\")\n",
        "            print(\"If download doesn't start automatically, use the file browser to download the files.\")\n",
        "\n",
        "\n",
        "    def generate_summary(self, max_words=1000):\n",
        "        \"\"\"\n",
        "        Generate a concise summary (~1000 words) from existing evergreen notes, structured by themes.\n",
        "\n",
        "        Returns:\n",
        "            str: Generated summary\n",
        "        \"\"\"\n",
        "        import re, os\n",
        "\n",
        "        # Helper method to load notes from markdown files in the output directory\n",
        "        def load_notes():\n",
        "            notes = []\n",
        "            note_files = [f for f in os.listdir(self.output_dir) if f.endswith(\".md\") and f not in [\"00_index.md\", \"all_notes.md\", \"book_metadata.md\"]]\n",
        "\n",
        "            for filename in note_files:\n",
        "                filepath = os.path.join(self.output_dir, filename)\n",
        "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                    content = f.read()\n",
        "\n",
        "                title_match = re.search(r\"# (.+)\", content)\n",
        "                title = title_match.group(1).strip() if title_match else \"Untitled\"\n",
        "\n",
        "                chapter_match = re.search(r\"\\*Page (\\d+)\\*\", content)\n",
        "                page = chapter_match.group(1).strip() if chapter_match else \"Unknown\"\n",
        "\n",
        "                chapter_number_match = re.match(r\"(\\d+):\", filename)\n",
        "                chapter_number = int(chapter_number_match.group(1)) if chapter_number_match else \"Unknown\"\n",
        "\n",
        "                chapter_title_match = re.match(r\"\\d+_\\d+_Chapter_\\d+_(.+?)_-_\", filename)\n",
        "                chapter_title = chapter_title_match.group(1).replace(\"_\", \" \") if chapter_title_match else \"Unknown Chapter\"\n",
        "\n",
        "                # Clean note content (remove markdown headers/metadata)\n",
        "                note_body = re.sub(r\"# .+\\n\\n\\*Page \\d+\\*\\n\\n\", \"\", content).strip()\n",
        "\n",
        "                notes.append({\n",
        "                    \"title\": title,\n",
        "                    \"content\": note_body,\n",
        "                    \"page\": page,\n",
        "                    \"chapter\": chapter_title,\n",
        "                    \"chapter_number\": chapter_number\n",
        "                })\n",
        "\n",
        "            return notes\n",
        "\n",
        "        # Load notes\n",
        "        notes = load_notes()\n",
        "\n",
        "        # Prepare aggregated notes content\n",
        "        notes_content = []\n",
        "        for note in notes:\n",
        "            notes_content.append(f\"{note['title']} (Chapter {note['chapter_number']}): {note['content']}\")\n",
        "\n",
        "        aggregated_notes = \"\\n\\n\".join(notes_content)\n",
        "\n",
        "        # Create prompt for generating summary\n",
        "        summary_prompt = f\"\"\"\n",
        "        You have evergreen notes extracted from a book. Your task is to create a coherent, insightful summary of approximately {max_words} words.\n",
        "\n",
        "        The summary must:\n",
        "\n",
        "        1. Highlight the 3-5 core themes emerging from these notes.\n",
        "        2. Reference explicitly the original evergreen note titles with corresponding chapter numbers.\n",
        "        3. Create logical bridges between ideas, expanding briefly with examples or analogies when helpful.\n",
        "        4. Be insightful, clear, and structured logically into sections based on themes.\n",
        "        5. Include a short introduction and integrative conclusion.\n",
        "\n",
        "        EVERGREEN NOTES CONTENT:\n",
        "        {aggregated_notes}\n",
        "\n",
        "        Return the summary in Markdown format clearly structured into an introduction, thematic sections, and conclusion.\n",
        "        \"\"\"\n",
        "\n",
        "        # Call OpenAI API\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.primary_model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You expertly synthesize knowledge into coherent, insightful, and concise summaries suitable for deep understanding.\"},\n",
        "                    {\"role\": \"user\", \"content\": summary_prompt}\n",
        "                ],\n",
        "                temperature=0.5,\n",
        "                max_tokens=2000\n",
        "            )\n",
        "\n",
        "            summary = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Save summary to markdown file\n",
        "            summary_path = os.path.join(self.output_dir, \"book_summary.md\")\n",
        "            with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(summary)\n",
        "\n",
        "            print(f\"Summary generated and saved to {summary_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating summary: {e}\")\n",
        "            summary = \"Summary generation failed.\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def process_book(self):\n",
        "        \"\"\"Main processing function\"\"\"\n",
        "        print(f\"Processing {self.pdf_path}...\")\n",
        "\n",
        "        # Step 1: Extract initial text for TOC analysis\n",
        "        initial_text = self.extract_initial_text()\n",
        "\n",
        "        # Step 2: Use LLM to extract chapter list\n",
        "        print(\"Extracting chapters using OpenAI...\")\n",
        "        chapters = self.get_toc_from_llm(initial_text)\n",
        "\n",
        "        print(f\"\\nFound {len(chapters)} chapters:\")\n",
        "        for idx, chap in enumerate(chapters, 1):\n",
        "            print(f\"{idx}. {chap}\")\n",
        "\n",
        "        # Step 3: Split content by chapters\n",
        "        print(\"\\nExtracting chapter contents...\")\n",
        "        chapter_contents = self.extract_chapters_from_pdf(chapters)\n",
        "\n",
        "        # Step 4: Estimate page numbers\n",
        "        print(\"Estimating page numbers...\")\n",
        "        page_numbers = self.estimate_page_numbers(chapters)\n",
        "\n",
        "        # Step 5: Generate and save notes\n",
        "        print(f\"\\nGenerating {self.notes_per_chapter} notes per chapter...\")\n",
        "        total_notes = self.save_notes(chapter_contents, page_numbers)\n",
        "\n",
        "        # Step 6: Generate book summary\n",
        "        print(\"\\nGenerating book summary...\")\n",
        "        summary = self.generate_summary()\n",
        "\n",
        "        # Save summary in text format as well\n",
        "        summary_txt_path = os.path.join(self.output_dir, \"book_summary.txt\")\n",
        "        with open(summary_txt_path, \"w\", encoding=\"utf-8\") as summary_txt_file:\n",
        "            summary_txt_file.write(summary)\n",
        "\n",
        "        print(f\"Summary saved to {os.path.basename(summary_txt_path)}\")\n",
        "\n",
        "        # Step 7: Create zip archive\n",
        "        print(\"\\nCreating downloadable ZIP archive...\")\n",
        "        self.create_zip_archive()\n",
        "\n",
        "        print(f\"\\nProcessing complete! Generated {total_notes} notes across {len(chapter_contents)} chapters.\")\n",
        "        print(f\"Notes and summary saved in the '{self.output_dir}' folder.\")\n",
        "\n",
        "    def run_in_colab(pdf_path, api_key, notes_per_chapter=3, use_mixed_models=True):\n",
        "        \"\"\"Run the Evergreen Notes Generator in Google Colab\"\"\"\n",
        "        generator = EvergreenNotesGenerator(\n",
        "            pdf_path=pdf_path,\n",
        "            api_key=api_key,\n",
        "            notes_per_chapter=notes_per_chapter,\n",
        "            use_mixed_models=use_mixed_models\n",
        "        )\n",
        "        generator.process_book()\n",
        "\n",
        "    # Attach robust LLM-driven PDF chapter extraction methods dynamically to EvergreenNotesGenerator\n",
        "\n",
        "    def extract_chapters_from_pdf(self, chapter_titles):\n",
        "        reader = pdf_library.PdfReader(self.pdf_path)\n",
        "        total_pages = len(reader.pages)\n",
        "        print(f\"PDF has {total_pages} pages\")\n",
        "\n",
        "        # Extract metadata from the first 30 pages\n",
        "        beginning_text = \"\"\n",
        "        sample_page_count = min(30, total_pages)\n",
        "        for i in range(sample_page_count):\n",
        "            beginning_text += f\"--- PAGE {i+1} ---\\n{reader.pages[i].extract_text()}\\n\\n\"\n",
        "\n",
        "        # Extract metadata from the last 5 pages\n",
        "        ending_text = \"\"\n",
        "        sample_end_count = min(5, total_pages)\n",
        "        for i in range(total_pages - sample_end_count, total_pages):\n",
        "            ending_text += f\"--- PAGE {i+1} ---\\n{reader.pages[i].extract_text()}\\n\\n\"\n",
        "\n",
        "        # Prepare the prompt for LLM\n",
        "        prompt = f\"\"\"\n",
        "    You are analyzing a PDF book. Provide EXACT start and end page numbers for each chapter title listed below.\n",
        "\n",
        "    Book Title: {os.path.basename(self.pdf_path)}\n",
        "    Total Pages: {total_pages}\n",
        "\n",
        "    Chapters from TOC:\n",
        "    {', '.join(f\"{i+1}. {title}\" for i, title in enumerate(chapter_titles))}\n",
        "\n",
        "    First {sample_page_count} pages:\n",
        "    {beginning_text[:10000]}\n",
        "\n",
        "    Last {sample_end_count} pages:\n",
        "    {ending_text[:5000]}\n",
        "\n",
        "    Return JSON ONLY in this format:\n",
        "    {{\n",
        "      \"chapters\": [\n",
        "        {{ \"title\": \"Chapter Title\", \"start_page\": 1, \"end_page\": 10 }},\n",
        "        ...\n",
        "      ]\n",
        "    }}\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"Using LLM to identify chapter page ranges...\")\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.lighter_model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Identify PDF chapter boundaries with high accuracy.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0\n",
        "            )\n",
        "\n",
        "            mappings = json.loads(response.choices[0].message.content)\n",
        "            chapter_mappings = mappings.get(\"chapters\", [])\n",
        "\n",
        "            if not chapter_mappings:\n",
        "                print(\"No chapters found, reverting to fallback.\")\n",
        "                return self.fallback_extract_chapters(chapter_titles)\n",
        "\n",
        "            print(f\"Identified page ranges for {len(chapter_mappings)} chapters.\")\n",
        "\n",
        "            chapter_contents = {}\n",
        "            for mapping in chapter_mappings:\n",
        "                title = mapping[\"title\"]\n",
        "                start_page = max(0, min(mapping[\"start_page\"] - 1, total_pages - 1))\n",
        "                end_page = max(start_page + 1, min(mapping[\"end_page\"], total_pages))\n",
        "\n",
        "                chapter_text = \"\"\n",
        "                for page_num in range(start_page, end_page):\n",
        "                    try:\n",
        "                        chapter_text += reader.pages[page_num].extract_text() + \"\\n\"\n",
        "                    except Exception as e:\n",
        "                        print(f\"  Text extraction error on page {page_num}: {e}\")\n",
        "\n",
        "                matched_title = next((t for t in chapter_titles if title.lower() in t.lower() or t.lower() in title.lower()), title)\n",
        "                chapter_contents[matched_title] = chapter_text\n",
        "\n",
        "            missing_chapters = set(chapter_titles) - set(chapter_contents.keys())\n",
        "            if missing_chapters:\n",
        "                print(f\"Chapters missing from extraction: {', '.join(missing_chapters)}\")\n",
        "\n",
        "            return chapter_contents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"LLM extraction failed: {e}\\nUsing fallback method.\")\n",
        "            return self.fallback_extract_chapters(chapter_titles)\n",
        "\n",
        "    def fallback_extract_chapters(self, chapter_titles):\n",
        "        print(\"Fallback extraction: dividing PDF into equal sections...\")\n",
        "        reader = pdf_library.PdfReader(self.pdf_path)\n",
        "        full_text = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "        total_length = len(full_text)\n",
        "        num_chapters = len(chapter_titles)\n",
        "        section_size = total_length // num_chapters\n",
        "\n",
        "        chapter_contents = {}\n",
        "        for i, title in enumerate(chapter_titles):\n",
        "            start = i * section_size\n",
        "            end = start + section_size if i < num_chapters - 1 else total_length\n",
        "            chapter_contents[title] = full_text[start:end].strip()\n",
        "\n",
        "        print(f\"Divided text into {num_chapters} sections.\")\n",
        "        return chapter_contents\n",
        "\n",
        "    # Dynamically attach both methods to the class\n",
        "    setattr(EvergreenNotesGenerator, \"extract_chapters_from_pdf\", extract_chapters_from_pdf)\n",
        "    setattr(EvergreenNotesGenerator, \"fallback_extract_chapters\", fallback_extract_chapters)\n",
        "\n",
        "    print(\"Methods 'extract_chapters_from_pdf' and 'fallback_extract_chapters' successfully attached.\")\n",
        "\n",
        "    def estimate_page_numbers(self, chapter_titles):\n",
        "        \"\"\"Estimate page numbers where each chapter begins using extracted chapter data\"\"\"\n",
        "        reader = pdf_library.PdfReader(self.pdf_path)\n",
        "        page_numbers = {}\n",
        "\n",
        "        # For each page, check if it contains any chapter title\n",
        "        for page_num in range(len(reader.pages)):\n",
        "            page_text = reader.pages[page_num].extract_text()\n",
        "\n",
        "            for title in chapter_titles:\n",
        "                if title in page_text and title not in page_numbers:\n",
        "                    page_numbers[title] = page_num + 1  # 1-based page numbers\n",
        "                    break\n",
        "\n",
        "        # If we're missing any chapters after scanning, assign an estimated page\n",
        "        for idx, title in enumerate(chapter_titles):\n",
        "            if title not in page_numbers:\n",
        "                # Estimate a page number based on position in the TOC\n",
        "                estimated_page = (idx + 1) * (len(reader.pages) // (len(chapter_titles) + 1))\n",
        "                page_numbers[title] = estimated_page\n",
        "                print(f\"Estimated page for '{title}': {estimated_page}\")\n",
        "\n",
        "        return page_numbers\n",
        "\n",
        "# Main execution code\n",
        "if __name__ == \"__main__\":\n",
        "    from google.colab import files\n",
        "    import os\n",
        "\n",
        "    try:\n",
        "        # Try to import PyPDF2, and if that fails, try pypdf\n",
        "        import PyPDF2 as pdf_library\n",
        "    except ImportError:\n",
        "        try:\n",
        "            import pypdf as pdf_library\n",
        "        except ImportError:\n",
        "            # If neither is available, we'll need to install one\n",
        "            import pip\n",
        "            pip.main(['install', 'PyPDF2'])\n",
        "            import PyPDF2 as pdf_library\n",
        "\n",
        "    # Upload PDF file\n",
        "    print(\"Please upload your PDF book:\")\n",
        "    uploaded = files.upload()\n",
        "    pdf_filename = list(uploaded.keys())[0]\n",
        "\n",
        "    # Get OpenAI API key\n",
        "    api_key = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "    # Set number of notes per chapter\n",
        "    notes_per_chapter = int(input(\"How many notes per chapter? (default: 3): \") or \"3\")\n",
        "\n",
        "    # Ask if user wants to use mixed models\n",
        "    use_mixed = input(\"Use GPT-4o-mini for title generation to save costs? (y/n, default: y): \").lower() != 'n'\n",
        "\n",
        "    # Run the generator\n",
        "    print(f\"Starting to process {pdf_filename} with {notes_per_chapter} notes per chapter...\")\n",
        "    run_in_colab(pdf_filename, api_key, notes_per_chapter=notes_per_chapter, use_mixed_models=use_mixed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jf8ZdQxWK5fv",
        "outputId": "13c105c8-4b3a-4b62-9521-38419647efdf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Methods 'extract_chapters_from_pdf' and 'fallback_extract_chapters' successfully attached.\n",
            "Please upload your PDF book:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-54734a1f-7e3a-49f5-9ef2-7345adffed8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-54734a1f-7e3a-49f5-9ef2-7345adffed8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Toju Duke_ Paolo Giudici - Responsible AI in Practice _ A Practical Guide to Safe and Human AI-Apress (2025).pdf to Toju Duke_ Paolo Giudici - Responsible AI in Practice _ A Practical Guide to Safe and Human AI-Apress (2025) (6).pdf\n",
            "Enter your OpenAI API key: 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\n",
            "How many notes per chapter? (default: 3): 1\n",
            "Use GPT-4o-mini for title generation to save costs? (y/n, default: y): y\n",
            "Starting to process Toju Duke_ Paolo Giudici - Responsible AI in Practice _ A Practical Guide to Safe and Human AI-Apress (2025) (6).pdf with 1 notes per chapter...\n",
            "Processing Toju Duke_ Paolo Giudici - Responsible AI in Practice _ A Practical Guide to Safe and Human AI-Apress (2025) (6).pdf...\n",
            "Extracting chapters using OpenAI...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 8 chapters:\n",
            "1. Responsible AI and AI Governance\n",
            "2. Accuracy\n",
            "3. Robustness\n",
            "4. Explainability\n",
            "5. Fairness and Human Rights\n",
            "6. Privacy\n",
            "7. Sustainability\n",
            "8. Human-Centered AI\n",
            "\n",
            "Extracting chapter contents...\n",
            "PDF has 194 pages\n",
            "Using LLM to identify chapter page ranges...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified page ranges for 8 chapters.\n",
            "Estimating page numbers...\n",
            "Estimated page for 'Responsible AI and AI Governance': 21\n",
            "Estimated page for 'Fairness and Human Rights': 105\n",
            "\n",
            "Generating 1 notes per chapter...\n",
            "\n",
            "Processing Chapter 1: Responsible AI and AI Governance\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 2: Accuracy\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 3: Robustness\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 4: Explainability\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Retrying request to /chat/completions in 4.310000 seconds\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying request to /chat/completions in 4.310000 seconds\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 5: Fairness and Human Rights\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Retrying request to /chat/completions in 11.598000 seconds\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying request to /chat/completions in 11.598000 seconds\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 6: Privacy\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Retrying request to /chat/completions in 16.418000 seconds\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying request to /chat/completions in 16.418000 seconds\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 7: Sustainability\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Retrying request to /chat/completions in 11.940000 seconds\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying request to /chat/completions in 11.940000 seconds\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Processing Chapter 8: Human-Centered AI\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Retrying request to /chat/completions in 18.556000 seconds\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying request to /chat/completions in 18.556000 seconds\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Generated 1 notes\n",
            "\n",
            "Generating book summary...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Retrying request to /chat/completions in 3.316000 seconds\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Retrying request to /chat/completions in 3.316000 seconds\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary generated and saved to evergreen_notes_Toju_Duke__Paolo_Giudici_-_Responsible_AI_in_Practice___A_Practical_Guide_to_Safe_and_Human_AI-Apress_2025_6_20250323_143800/book_summary.md\n",
            "Summary saved to book_summary.txt\n",
            "\n",
            "Creating downloadable ZIP archive...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_522fb979-ea32-4cad-bbc3-c67f382f63de\", \"evergreen_notes_Toju_Duke__Paolo_Giudici_-_Responsible_AI_in_Practice___A_Practical_Guide_to_Safe_and_Human_AI-Apress_2025_6_20250323_143800.zip\", 31081)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download started for evergreen_notes_Toju_Duke__Paolo_Giudici_-_Responsible_AI_in_Practice___A_Practical_Guide_to_Safe_and_Human_AI-Apress_2025_6_20250323_143800.zip\n",
            "\n",
            "Processing complete! Generated 8 notes across 8 chapters.\n",
            "Notes and summary saved in the 'evergreen_notes_Toju_Duke__Paolo_Giudici_-_Responsible_AI_in_Practice___A_Practical_Guide_to_Safe_and_Human_AI-Apress_2025_6_20250323_143800' folder.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8y1+cgrMGRpnIoAc+yzwt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}